{"categories":[{"title":"“刷题\"","uri":"https://biofrostyy.github.io/categories/%E5%88%B7%E9%A2%98/"},{"title":"知识网络","uri":"https://biofrostyy.github.io/categories/%E7%9F%A5%E8%AF%86%E7%BD%91%E7%BB%9C/"},{"title":"“组件沉淀\"","uri":"https://biofrostyy.github.io/categories/%E7%BB%84%E4%BB%B6%E6%B2%89%E6%B7%80/"},{"title":"“读书笔记”","uri":"https://biofrostyy.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"title":"“读后感”","uri":"https://biofrostyy.github.io/categories/%E8%AF%BB%E5%90%8E%E6%84%9F/"}],"posts":[{"content":"PDF文件见GitHub\n推荐算法 Embedding 向量检索 自监督学习 ","id":0,"section":"posts","summary":"PDF文件见GitHub 推荐算法 Embedding 向量检索 自监督学习","tags":[],"title":"知识网络","uri":"https://biofrostyy.github.io/2022/05/%E7%9F%A5%E8%AF%86%E7%BD%91%E7%BB%9C/","year":"2022"},{"content":"主要工作方向为用户研究及推荐算法，搜广推理论多相通都会看看，包含前沿理论及工程实践，当然也都偶尔偏题看看AI领域前沿，毕竟谁能不爱机器人呢。\n论文主要来源为美团技术团队博客，KDD，RecSys，及上述论文的references，知识点主要来源为个人博客，pytorch论坛，reddit AI/ML板块。“来源”字段为个人笔记文件夹存储路径，仅作参考。 阿里巴巴论文整理： 推荐：https://github.com/guyulongcs/Awesome-Deep-Learning-Papers-for-Search-Recommendation-Advertising\n🥱看不懂就歇一会，过段时间自然就开窍了，多读自然通\n   论文名 来源 阅读日期 关键词 创新点     TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding KDD 26th Applied Data Science 2021.3.5 multi-task learning, ideology detection, heterogeneous information network, social network analysis, graph convolutional networks (1)与传统概率模型不同，使用 GNN方法以利用高效的计算资源 (2) 专注于links和relations，而非文本   美团外卖特征平台的建设与实践 2021美团技术团队 2021.3.7 美团,外卖,特征平台    Transformer 在美团搜索排序中的实践 2020美团技术团队 2021.4.2 搜索，排序, Transformer Model 精排序的行为序列建模中中应用Transformer 建模行为序列内部之间的关系，引入Target-item与Attention机制优化   Attention is all you need  2021.4.9 对于一个大数据从业者来说，工程能力很重要，此算法小抄配leetcode，药到病除;)    BERT在美团搜索核心排序中的探索和实践 2020美团技术团队 2021.4.12 搜索，排序，BERT，Transformer    Pre-trained Models for Natural Language Processing: A Survey Transformer fold 2021.4.22 NLP, 深度学习，Transformer，预训练    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Transformer fold 2021.4.22 NLP, BERT，Transformer 学习基于上下文的词嵌入，这些学习到的编码器在下游任务中会用于词在上下文中的语义表示。   多业务建模在美团搜索排序中的实践 2021美团技术团队 2021.4.23 搜索, 排序，多任务 粗排\u0026ndash;\u0026gt;配额融合\u0026ndash;\u0026gt;精排\u0026ndash;\u0026gt;重排\u0026ndash;\u0026gt;异构排序   多任务学习-Multitask Learning概述 知乎文章/深度学习于NLP 2021.4.23 多任务 多任务学习框架概览：单⻔混合专家模型(OMOE)\u0026ndash;\u0026gt;多⻔多专家混合模型(MMOE)\u0026ndash;\u0026gt;Customized Gate Control(CGC:既有共享的专家，又有每个任务独有的，能够更好地处理不同任务之间地关系)\u0026ndash;\u0026gt;PLE(存在不同专家之间的多层交互Multi-Level Extraction Networks,因此PLE中不同任务的参数并没有像CGC那样在早期层完全分离，而是在多层中逐步分离)   美团搜索多业务商品排序探索与实践 2021美团技术团队 2021.4.27 搜索, 排序，多任务 粗排\u0026ndash;\u0026gt;配额融合\u0026ndash;\u0026gt;精排\u0026ndash;\u0026gt;重排\u0026ndash;\u0026gt;异构排序   Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising 2021美团技术团队 2021.4.27 搜索, 排序，多任务 粗排\u0026ndash;\u0026gt;配额融合\u0026ndash;\u0026gt;精排\u0026ndash;\u0026gt;重排\u0026ndash;\u0026gt;异构排序   MMOE:Recommending What Video to Watch Next: A Multi-task Ranking System Google 2021.4.27 搜索, 排序，多任务 粗排\u0026ndash;\u0026gt;配额融合\u0026ndash;\u0026gt;精排\u0026ndash;\u0026gt;重排\u0026ndash;\u0026gt;异构排序   Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations 腾讯PCG RecSys2020 2021.4.28 搜索, 排序，多业务 为了解决上述的“跷跷板”现象，文章针对多任务之间的共享机制和单任务的特定网络结构进行了重新的设计，提出了PLE模型   Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising KDD2021 美团 2021.4.30 信用卡，行为序列，多任务 针对序列依赖任务，提出自适应信息迁移多任务(Adaptive Information Transfer Multi-task，AITM)框架   深度学习在美团搜索广告排序的应用实践 2021美团技术团队 2021.4.30 深度学习，搜索广告，CVR/CTR预估，模型调优 模型调优，工程优化及线上预估体系   ESMM：Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate SIGIR 阿里妈妈 2021.4.30 信用卡，行为序列，多任务 pCTCVR=pCVR*pCTR，利用可使用全量数据建模的CTCVR和CTR，隐式学习CVR，不用除法是实验证实结果不稳定   LARGE-SCALE CAUSAL APPROACHES TO DEBIASING POST-CLICK CONVERSION RATE ESTIMATION WITH MULTI-TASK LEARNING  2021.4.30 信用卡，行为序列，多任务 ESMM有偏   Ranking中的pairwise、pointwise、listwise 知乎 2021.5.2     用户行为序列建模概览 知乎 2021.5.3     Self-Attentive Sequential Recommendation ICDM 2018/Transformer 2021.5.5 行为序列建模，推荐模型，Transformer 同时建模用户短期兴趣(由self-attention结构提取)和用户⻓期兴趣   Behavior Sequence Transformer for E-commerce Recommendation in Alibaba Transformer 2021.5.12 行为序列建模，推荐模型，Transformer 弥补现有推荐模型缺少对用户行为序列信息的捕捉，利用Transformer的Encoder部分充分挖掘用户的行为序列，实现对用户行为序列的建模，主要用于ranking阶段   阿里推荐算法（BST）: 将Transformer用于淘宝电商推荐 知乎 2021.5.12 行为序列建模，推荐模型，Transformer 主要输入特征有Item Feature、用户画像、上下文特征、交叉特征经过Embedding 层后concat 在一起。用户行为序列包含Item ID类特征及对应的position 信息，进行Embedding 处理后输入到Transformer 层(Transformer 的Encoder 部分)捕获用户历史行为与Target Item 之间的相互关系得到用户行为兴趣表达，与其他特征embedding 向量concat 在一起，经过三层MLP层计算得到预测的点击率   【经典精读】Transformer模型和Attention机制 知乎 2021.5.19 解读，工程，行为序列建模，推荐模型，Transformer Transformer解读的非常清晰，值得一看   一文纵览向量检索 华为云社区博客 2021.5.29 向量检索 大规模向量检索技术概览   13种高维向量检索算法全解析 segmentfault 2021.5.29 向量检索 大规模向量检索技术概览   Pruned Bi-directed K-nearest Neighbor Graph for Proximity Search Yahoo 2021.5.30 近邻图索引，向量检索 开源的向量检索库，核心算法基于近邻图索引。NGT 在构建近邻图时类似于 NSW，也是对 DG 的近似，后续有一些度调整优化，其中最有效的路径优化也是对 RNG 的近似   Approximate nearest neighbor algorithm based on navigable small world graphs Yahoo 2021.6.1 向量检索 NSW 是对 DG 的近似，NSW 具有小世界导航性质：在构建早期，形成的边距离较远，像是一条“高速公路”，这将提升搜索的效率；在构建后期，形成的边距离较近，这将确保搜索的精度   [DMR] Deep Match to Rank Model for Personalized Click-Through Rate Prediction-AAAI20 阿里 2021.6.15 排序，召回 1.采用了user-to-item子网络和item-to-item子网络来充分提取目标商品与历史商品的相关性2.为辅助训练user-to-item子网络单独设计了一个额外的match网络，可以用作召回阶段，所以可以认为是召回模型和CTR预估模型联合训练的3.考虑到用户行为的时间序列属性，利用attention机制和position encoding来挖掘不同时期用户行为的权重，进行加权sum-pooling   推荐模型之用户行为序列处理 知乎知识点 2021.6.25 推荐，用户行为序列 对于Multi-hot特征，一个特征里面多个特征值，将多个特征值的embedding融合到一起形成一个定长的embedding，这里怎么融合就是各种方法大显神通之处   推荐模型之用户行为序列处理 知乎知识点 2021.6.29 推荐，用户行为序列 对于Multi-hot特征，一个特征里面多个特征值，将多个特征值的embedding融合到一起形成一个定长的embedding，这里怎么融合就是各种方法大显神通之处   阿里飞猪推荐算法探索实践 阿里飞猪 2021.11.20 工程，推荐模型，旅行行业，CVR预估 介绍电商背景下主流推荐技术的发展，例如基于全空间的CVR预估技术的发展历程等 ( ESMM / ESM^2 / HM^3 )；接着会重点结合旅行行业的特色，进一步介绍飞猪推荐算法的现状及发展   Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation  2021.12.20 工程，行为序列建模，推荐模型，Transformer 一整套序列推荐建模的pipeline，引入了NVIDIA开源的Tabular库，能够显著降低从原始数据到模型输入之间的转换时间，提高训练和推理性能   PyTorch中的损失函数\u0026ndash;CrossEntropyLoss/NLLLoss/KLDivLoss 知乎工程细节 2022.1.3 工程,Loss Function 开始尝试使用深度学习训练推荐模型，pytorch工程实现   NVIDIA GTC 2022 黄仁勋 2022.3.26 H100, nvidia triton,Omniverse, digital twin NVIDIA，硬件算力，AI平台，虚拟现实Omniverse，robotic systems    运筹学论坛-EURO2018论坛 KDD/ICLR/IEEE论坛paper IEEE 推荐系统顶会RecSys2020大奖 ICDM ACM MM\nTopics: 知识图谱 Transformer Model Multi-Task 向量检索，图 \u0026ndash; LOOKALIKE 调优：神经网络最常用的超参设置有：隐层层数及节点数、学习率、正则化、Dropout Ratio、优化器、激活函数、Batch Normalization、Batch Size等 用户行为序列建模 \u0026ndash;美团transformer，应用在推荐中 auto learning transformer，MLP 自监督学习 智能营销uplift modeling 度学习模型可以看做是在搭“乐高”，底层使用的那些模块（Attention/RNN/CNN/Transformer等)就是积木。当面临一个具体的问题时，找到合适的积木，搭建好适合某个问题的乐高就显得十分重要。\n","id":1,"section":"posts","summary":"主要工作方向为用户研究及推荐算法，搜广推理论多相通都会看看，包含前沿理论及工程实践，当然也都偶尔偏题看看AI领域前沿，毕竟谁能不爱机器人呢。","tags":null,"title":"⭐♥Paper List♥⭐","uri":"https://biofrostyy.github.io/2022/05/%E8%AE%BA%E6%96%87%E5%8D%95/","year":"2022"},{"content":"记录读书历程，喜欢在书店畅销书货架及微信读书推荐栏目淘书，所以看的书大都比较大众 PS.有想过把专业工具书放在paper list中，但最后因为界限模糊还是一并放在了书单里。 PS.一些MasterClass的视频也算做“书”记录在此，Youtube铺天盖地的广告让我知道这个平台，机缘巧合从小威的网球大师课开始，沉醉于master们的分享，制作墨西哥菜、训狗技巧、科学思维和交流技巧、还有Bobbi Brown亲自教的化妆和美容，仿佛在感受另一种人生，特别奇妙。\n第一部分为Book List，记录每个领域我精心挑选的，推荐阅读的书单\n第二部分为Reading Notes，以日记形式，按时间顺序，记录读的所有书的读书感想\n工具书或科普类书籍都有“xx分钟读完”的文章，小说类书籍读后感不多，但也有几篇有感而发记录在读后感分类中，都可以在博客中查看。  Book List: Machine Learning 基础篇    书名 作者 简介     机器学习基石 台大林轩田 数学基石   机器学习/深度学习 周志华 西瓜书，花书   算法4 Robert Sedgewick/ Kevin Wayne 联系具体应用场景的例子让读者对算法认知更为内化，书很厚   Machine Learning Yearning 吴恩达 调参与算法优化   labuladong的算法小抄 labuladong leetcode刷题可以看看   matrix cookbook  矩阵计算，工具书    算法应用篇    书名 作者 简介     深度学习推荐系统 王喆 推荐系统的宏观知识结构，读了两遍，每次都有新收获   美团机器学习实践 美团技术团队 干货很多    人工智能知识宽度拓展篇    书名 作者 简介     数学之美 吴恩达 宽度（非深度）拓展，涉及领域很多   浪潮之巅 吴恩达     小说    书名 作者 简介     星星是冰冷的玩具 谢尔盖·卢基扬年科，肖楚舟[译] 软科幻，讨论社会与自由   诡秘之主 爱潜水的乌贼 不要被书名劝退，猪猪倾情推荐！起点哈利波特它值得！    人文\u0026amp;艺术\u0026amp;历史  Reading Notes:    书名 作者 读书日期 心得\u0026amp;读书笔记标签     上帝掷色子吗 曹天元 2021.3.5\u0026mdash;\u0026gt;2021.4.11 作者花费了许多笔墨做比喻和感慨，信息密度稍低，但一点都没有减少对我的震撼，对于量子实验所提供的信息，各位勇士们各显神通提出各种对于宇宙奥秘的猜想，从我的角度希望一个客观的绝对的简单而完美的定理可以解答世界吧。   强化学习精要 核心算法与TensorFlow实现  2021.3.20 \u0026mdash;\u0026gt; 暂停于2021.4.5 记录于2021.4.5晚 p.61/308因为关注的VRP问题最新很多paper都开始应用强化学习（强化学习天然适合于这种输出与loss没有直接关系（不可导）的情况），所以选取了这本书了解强化学习。在工业应用中，强化学习还是更多应用于游戏行业，在VRP问题中的尝试应用暂时没有提升太多时间上的优化（找到更优的算子往往会伴随更长的学习时间），所以我在总体了解了强化学习的思想后暂停了对更深入的变体、调优等部分的了解，后续如果有其他相关工作实践，再重新开启吧 :)   幽灵塔 江户川乱步 2021.3.30\u0026mdash;\u0026gt; 2022.4.1 凑单买的一本日系推理小说，没注意是20世纪的出品，读的时候有仿佛在看聊斋志异的轻微“幼稚感”，坏人居然是被打雷吓到心脏病死的（从来没有提到过这个人有心脏病），推理线索和进展也是突然遇到某个人口述出来的，铺垫很少，人物形象很平面，另外男主对女主感情真是令人疑惑   美团机器学习实践 美团技术团队 2021.3.22\u0026mdash;\u0026gt;2021.7.01 干货很多，配合美团技术团队的博客，各个场景都有系统而详细的解决方案，也发现一些设计和构架去应用到现在工作的模型中，个人认为美团技术算是各大厂中真的想要去分享点干货的团队了，很是敬佩   给未来人类的终极12问 洛朗·亚历山大、让-米歇尔·贝尼耶、张芳[译] 2021.4.11\u0026mdash;\u0026gt;2021.5.3 杜海涛同款😂记录于2021.5.3劳动节清晨早上起来把最后一个topic看完了，怎么说呢，简直就是抱着我就看看到底哪里好的想法看完了整本迷你书，整本书就是两个大佬的对话，有举例争辩也有相互支持，但是可能是因为按出版时间算此书已不算前沿，或者只有两个人主观的辩论信息量太少，总体来说体验并不是很好   labuladong的算法小抄 labuladong 2021.4.11\u0026mdash;\u0026gt; 对于一个大数据从业者来说，工程能力很重要，此算法小抄配leetcode，药到病除;)   数学之美 吴军 2021.5.9\u0026mdash;\u0026gt;2021.6.10 本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，工作领域为智能营销推荐算法，而非自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些相关应用。这是一本让你构建宏观体系的书，介绍人工智能运用在工业界的方方面面，让人们对这些问题的解决有一个“道”的框架理解，有趣的是，因为这种宏观的描绘，在读书中会有很多句子背后蕴藏的深刻理论会吸引你停下读书的脚步，对其进行更深的探索，这正是本书的乐趣所在。   消失的第13级台阶 高野和明、赵建勋[译] 2021.5.22\u0026mdash;-\u0026gt;2021.5.23 一本非常有人气的日系悬疑推理小说，本书更多聚焦在社会派的推理，直到中间部分才开始出现紧张而烧脑的案件推进情节，对于本格推理爱好者可能吸引不大，对死刑相关及当下社会的思考才是这本小说最注重且可贵的地方。   阿加莎·克里斯蒂 阿加莎·克里斯蒂 2021.5.29\u0026mdash;\u0026gt; 看过消失的第13级台阶之后，就很想看悬疑小说😐，我的青少年时期的悬疑推理小说主要来源是福尔摩斯，蔡骏和各种在我初中书架上已经忘记名字的小说。但是阿加莎大名鼎鼎的著作，除了拍成电影的，我一部都没看过，得益于现在电子书籍的普及，我终于可以足不出户，开启这一段注定漫长（八十部）但又精彩的旅程   时间的形状·相对论史话[听书] 汪洁 2021.5.25\u0026mdash;\u0026gt;2021.6.25 是《上帝掷色子吗》的完美补充，一起代表物理界的两朵乌云\u0026ndash;相对论与量子力学。在上下班的路上听作者的有声书讲解，还是很有趣的。   果壳中的宇宙[听书] 霍金，于浩[播] 2021.8.1\u0026mdash;\u0026gt;2021.8.7 上下班的路上的小快乐   给忙碌者的天体物理 尼尔·德格拉斯·泰森、孙正凡[译] 2021.6.3\u0026mdash;\u0026gt; 向⭐出发   深度学习推荐系统 王喆 2021.7.26\u0026mdash;\u0026gt; 王喆大佬是我在知乎上最喜欢的博主之一，他总能用最简单的语言精准的讲述，同时又带有自己丰富经验的智慧，让我受益匪浅。因为工作原因，需要系统使用推荐系统，所以购买了大佬的这本书，希望构建自己推荐系统方面完整的知识构架。   唐朝穿越指南 森林鹿 2021.7.27\u0026mdash;\u0026gt; 2021.8.1 当作科普读物读的，除了小时候读过《明朝那些事儿》之外是个纯纯的历史盲，上大学那会连朝代前后都搞不清楚。最近却莫名的对历史生出兴趣来   长安的荔枝 马伯庸 2021.8.20\u0026mdash;\u0026gt; 2021.8.20 一个很平常的周五，下班比较早，8点就洗了澡躺在床上，故事很短，共计5个小时就翻完了最后一页。一将功成万骨枯，一事功成，也是万头皆白啊，其中一些官场的处事心思甚是巧妙，本职场白痴也是默默记下了。有趣的是，同时在看一本叫《唐朝穿越手册》的科普读物，很多唐朝生活细节都能对应上，很是奇妙   Machine Learning Yearning 吴恩达 2021.9.2\u0026mdash;\u0026gt; 2021.10.1    吴承恩捉妖记 马伯庸 2021.9.10\u0026mdash;\u0026gt; 2021.12.1    喜鹊谋杀案 安东尼·霍洛维茨 2021.10.1\u0026mdash;\u0026gt; 2021.10.6    幽灵塔 江户川乱步 2022.3.30\u0026mdash;\u0026gt; 2022.4.1 凑单买的一本日系推理小说，没注意是20世纪的出品，读的时候有仿佛在看聊斋志异的轻微“幼稚感”，坏人居然是被打雷吓到心脏病死的（从来没有提到过这个人有心脏病），推理线索和进展也是突然遇到某个人口述出来的，铺垫很少，人物形象很平面，另外男主对女主感情真是令人疑惑   计算广告 刘鹏，王超 2022.4.6\u0026mdash;\u0026gt; 2022.5.6 搜广推不分家嘛   星星是冰冷的玩具 谢尔盖·卢基扬年科，肖楚舟[译] 2022.4.8\u0026mdash;\u0026gt; 2022.4.22 人类及其他几个弱小种族潜入外星文明，以求制约银河委员会中强大种族的方法，来摆脱被强大种族奴役的现状。迷雾渐渐揭开，几何学家、暗影族竟然都与人类同族。算是一篇软科幻，更着重社会哲学，更精确一点，是对社会结构、对自由的探讨。印象比较深刻的是几何学家文明中避免人工智能占领世界的方法是让它们觉得自己是唯一的智慧，几何学家的飞船还是个唯心主义者，真的脑洞大开。唯一遗憾的是其中一些谜团没有得到很好的解答，例如提到人类的超时空跳跃与门的原理相关，但具体如果相关，门的原理，或者说门是什么，门是作为一个智慧体在解析一切吗？这篇软科幻再“硬”一点就好了   艺术之美 朱良志 2022.4.22\u0026mdash;\u0026gt; 我有罪，我实在没坚持看完，我是个失去高尚审美的人   前男友的遗书 新川帆立 2022.5.4\u0026mdash;\u0026gt; 2022.5.4 中规中矩，日式社会特点很浓重   喜鹊谋杀案 安东尼·霍洛维茨     大数据之路：阿里巴巴大数据实践      阿里云天池大赛赛题解析机器学习      霍金三部曲      算法与数据中台      智能搜索和推荐系统      果壳中的宇宙      machine learning yearning 调参      博物 中国国家地理       百年孤独\n银河系搭车客指南\n万物解释者\n神的九十亿个名字 by 阿瑟·克拉克\n给好奇者的暗黑物理学 一想到还有95%的问题留给人类我就放心了 给仰望者的天文朝圣之旅 给未来人类的终极12问 数据仓库 谷歌分析宝典/谷歌数据分析方法 计算广告-互联网商业变现的市场与技术 数据结构 Vehicle Routing Problems, Methods and Applications second edition Metahuristics for Vehicle Routing Problems 运筹需论坛-EURO2018论坛 KDD/ICLR/IEEE论坛paper IEEE\nICDM\nACM MM\n","id":2,"section":"posts","summary":"记录读书历程，喜欢在书店畅销书货架及微信读书推荐栏目淘书，所以看的书大都比较大众 PS.有想过把专业工具书放在paper list中，但最后因为","tags":null,"title":"⭐♥Book List♥⭐","uri":"https://biofrostyy.github.io/2022/05/%E4%B9%A6%E5%8D%95/","year":"2022"},{"content":"一.居住环境 1.1长安城 长安城城郭被横竖三十八条街道分割成一百多个居住区（坊），坊内街角有各自的武侯铺（派出所），除上元节三天，宵禁后坊外禁止通行，坊内也禁止但是执行并不严格。\n东市、西市： 从皇城的正南门朱雀门沿东西向大街，东走三坊地就是东市，西走三坊地就是西市 平康坊： 著名红灯区，北里名花集中居住地区 崇仁坊： 旅店集中地，此处西面是皇城，东南角是东市，南面是平康坊。有了这些便利，崇仁坊就成了外地来长安选官考评和参加科举考试的文人们的居住集中地，是长安城的夜生活中心\n二.\n","id":3,"section":"posts","summary":"一.居住环境 1.1长安城 长安城城郭被横竖三十八条街道分割成一百多个居住区（坊），坊内街角有各自的武侯铺（派出所），除上元节三天，宵禁后坊外禁","tags":[],"title":"唐朝穿越指南","uri":"https://biofrostyy.github.io/2022/03/2%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3%E5%94%90%E6%9C%9D%E4%BA%BA%E6%B0%91%E7%94%9F%E6%B4%BB/","year":"2022"},{"content":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和谐统一的。例如YouTube的用户体验和公司利益（时长越长广告曝光越多）在“观看时长”这一点上达成了一致。\n下图是推荐系统的技术架构示意图。其中数据部分为融合了数据离线批处理、实时流处理的数据流框架；算法和模型部分则为集训练(training)、评估(evaluation)、部署(deployment)、线上推断(online inference)为一体的模型框架。\n推荐系统的进化之路 传统推荐模型 一. 协同过滤算法 UserCF基于用户相似度进行推荐，它符合人们直觉上的“兴趣相似的朋友喜欢的物品，我也喜欢”的思想，这使其具有更强的社交属性，这样的特点使其非常适合新闻推荐场景，因为新闻本身的兴趣点往往是分散的，相比用户对不同兴趣的偏好，新闻的及时性、热点性往往是其更重要的属性，而UserCF正适用于发现热点，以及跟踪热点的趋势。而ItemCF适用于兴趣变化较稳定的应用，例如Amazon、Youtube等。但从技术角度，它主要有两个缺点，首先是，互联网应用场景下，用户数往往远大于物品数，而UserCF需要维护用户相似度矩阵以便快速找出$Top n$相似用户，这使得存储开销非常大$O(n^2)$，第二点是用户的历史数据向量非常稀疏，对于只有几次购买或点击的用户来说，找到相似用户的准确度是非常低的，这导致UserCF不适用于哪些正反馈获取困难的场景（如酒店预订、大件商品购买等低频应用）。\nItemCF解决了上述存储开销大的问题，但是由于数据稀疏，它仍然有协同过滤的天然缺陷——推荐结果的头部效应较明显，处理稀疏向量的能力弱。为了增强模型的泛化能力，矩阵分解技术被提出。相比协同过滤，矩阵分解的①泛化能力强，可以在一定程度上解决数据稀疏问题，②空间复杂度低，只需存储用户和物品隐向量，空间复杂度由$O(n^2)$降低到$O((m+n)\\cdot k)$级别。③具有更好的扩展性和灵活性，这其实与Embedding思想不谋而合，因此矩阵分解的结果也非常方便与其他特征进行组合和拼接，并便于与深度学习网络进行无缝结合。\n与此同时，矩阵分解也有一定局限性。它不方便加入其他特征，丧失了利用很多有效信息的机会。为了解决这个问题，逻辑回归及其后续发展出的因子分解机等模型，凭借其天然的融合不同特征的能力，逐渐在推荐系统领域得到更广泛的应用。\n1.UserCF 共现矩阵中，和你评分行为相似的TopN用户对物品p的评分。其中值得注意的两部分为 ①用户相似度和②最终结果排序\n①理论上，任何合理的“向量相似度定义方式”都可以作为相似用户计算的标准。例如余弦相似度，皮尔森相关系数，相比余弦相似度减小了用户评分偏置的影响。\n②最常用的方式是利用用户相似度和相似用户的评价的加权平均获得目标用户的评价预测\n$R^(u,p)=\\cfrac{\\sum_{s\\in{S}}(W_{u,s}\\cdot{R_{s,p}})}{\\sum_{s\\in{S}}W_{u,s}}$\n其中，权重$W_{u,s}$是用户$u$和用户$s$的相似度，$R_{s,p}$是用户$s$对物品$p$的评分。\n2.ItemCF 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的$Topk$物品，对这些物品进行相似度分值排序，相似度分值为与已有正反馈物品相似度的累加。\n$R_{u,p} = \\sum_{h\\in{H}}(W_{p,h} \\cdot R_{u,h})$\n其中，$H$是目标用户的正反馈物品合集，$w_{p,h}$是物品$p$与物品$h$的物品相似度，$R_{u,h}$是用户$u$对物品$h$的已有评分。\n3.矩阵分解\n该方法在协同过滤共现矩阵的基础上，使用更稠密的隐向量表示用户和物品，挖掘用户和物品的隐含兴趣和隐含特征，在一定程度上弥补了协同过滤模型处理稀疏矩阵能力不足的问题。但仍无法引入用户画像信息、物品画像信息和实时上下文信息，这就需要机器学习模型来解决了。\n矩阵分解算法将$m \\times n$维的共现矩阵R分解为$m \\times k$维的用户矩阵$U$和$k \\times n$维的物品矩阵$V$相乘的形式。其中$k$是隐向量的维度，$k$的大小决定了隐向量表达能力的强弱。$k$的取值越小，隐向量包含的信息越少，模型的泛化程度越高；反之，$k$的取值越大，隐向量的表达能力越强。在具体应用中，$k$的取值要经过多次试验找到一个推荐效果和工程开销的平衡点。\n矩阵分解主要有三种方法：①特征值分解(Eigen Decomposition)、奇异值分解(Singular Value Decomposition, SVD)和梯度下降(Gradient Descent)。其中特征值分解只能作用于方阵，显然用户-物品矩阵不是。奇异值分解存在两点缺陷，使其不宜作为互联网场景下矩阵分解的主要方法：①奇异值分解要求共现矩阵是稠密的，如果要应用，就要对缺失元素进行填充。②传统奇异值分解的计算复杂度达到了$O(mn^2)$，这对于动辄上千万的互联网场景来说不可接受。\n由上，梯度下降成了进行矩阵分解的主要方法，加入正则化项的目标函数入下：\n$\\underset {q^,p^}{min}\\underset {(u,i)\\in K}{\\sum}(r_{ui}-q_i^Tp_u)^2+\\lambda(||q_i||+||p_u||)^2$\n隐向量的生成过程其实是对共现矩阵进行全局拟合的过程，因此隐向量其实是利用全局信息生成的，有更强的泛化能力；而协同过滤中只利用用户和物品自己的信息进行相似度计算，这就使协同过滤不具备泛化利用全局信息的能力。\n为了消除用户和物品打分的偏差(Bias)，常用的做法是在矩阵分解时加入用户和物品的偏差向量：\n$r_{ui} = \\mu + b_i + b_u +q_i^Tp_u$\n其中$\\mu$是全局偏差常数，$b_i$是物品偏差系数，可使用物品$i$收到的所有评分的均值，$b_u$是用户偏差系数，可使用用户$u$给出的所有评分的均值。与此同时，目标函数也要有相应改变：\n$\\underset {q^,p^,b_*}{min}\\underset {(u,i)\\in K}{\\sum}(r_{ui}- \\mu -b_u-b_i-q_i^Tp_u)^2+\\lambda(||q_i||+||p_u||+b_u^2+b_i^2)^2$\n二.逻辑回归 逻辑回归作为广义线性模型的一种，使用softmax(二分类退化为sigmoid)映射线性模型至0-1，符合点击率的物理性质。目标函数可以分别通过交叉熵和服从伯努利的最大似然推导（最大似然取log后与交叉熵损失函数等价），参数训练常采用的方法为梯度下降法、牛顿法、拟牛顿法等。\n逻辑回归的优点在于，①数学含以上的支撑，点击率这个行为服从伯努利分布的这个假设，采用逻辑回归作为CTR模型是符合“点击”这一事件的物理意义的。②权重可解释性强。③易于并行化、模型简单、训练开销小。\n但它也有局限性，它表达能力不强，无法进行特征交叉、特征筛选等一系列较为“高级”的操作，因此不可避免地造成信息的损失。为了解决这一问题，衍生出因子分解机等高维的复杂模型，在进入深度学习时代后，多层神经网络强大的表达能力可以完全替代逻辑回归模型。\n三.从FM到FMM-自动特征交叉的解决方案 算法工程师手动组合特征，再通过各种分析手段筛选特征的，这种方法无疑是低效的，并且人类的经验往往有局限性，程序员的精力和时间无法支撑找到最优的特征组合。于是，模型自动特征交叉的方案应运而生。\n1.POLY2模型——特征交叉的开始 $\\phi POLY2(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}w_h(j_1,j_2)x_{j_1}x_{j_2}$\n该模型对所有特征两两交叉(特征$x_{j_1}$和$x_{j_2}$)，并对所有的特征组合赋予权重$w_{h(j_1,j_2)}$。POLY2通过暴力组合特征的方式，在一定程度上解决了特征组合的问题。POLY2模型本质上仍是线性模型，训练方法与逻辑回归并无区别，因此便于工程上的兼容。但是PLOY2模型存在两大缺陷，①one-hot编码的稀疏特征，交叉后更加稀疏，导致大部分交叉特征的权重缺乏有效数据训练，无法收敛。②权重参数的数量由$n$直接上升到$n^2$，极大地增加了训练的复杂度。\n2.FM模型——隐向量特征交叉 与POLY2不同的是，FM用两个向量的内积$(w_{j_1} \\cdot w_{j_2})$取代了单一的权重系数$w_h(j_1,j_2)$。具体地说，FM为每个特征学习了一个隐权重向量(latend vector)。在特征交叉时，使用两个隐向量的内积作为权重，这和矩阵分解的隐向量有着异曲同工之妙：\n$\\phi FM(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}(w_{j_1} \\cdot w_{j_2})x_{j_1}x_{j_2}$\n此时参数数量为$nk$（$k$为隐向量维度，$n\u0026raquo;k$），使用梯度下降法进行训练时复杂度可被同样降低到$nk$级别，极大降低了训练开销。\n同时，隐向量更好地解决了数据稀疏性问题。POLY2中只有两种特征取值同时出现时，才能学习这个组合的权重，例如(‘male’,\u0026lsquo;earrings\u0026rsquo;)，当这两种特征出现次数非常少时，则此参数缺乏有效训练。而隐向量可以通过(\u0026lsquo;male\u0026rsquo;,xx)和(xx,\u0026lsquo;earrings\u0026rsquo;)分别训练隐向量。这样，甚至对于一个从未出现过的组合，由于模型之前已经学习过两个的分别隐向量，也具备了计算该特征组合权重的能力。所以，相比POLY2，FM虽然丢失了某些具体特征组合的精确记忆，但是泛化能力大大提高。\n在工程方面，FM同样可以使用地图下降法，使其不失实时性和灵活性。相比之后深度学习复杂的网络结构导致难以部署和线上服务。FM较容易实现的模型结构使其线上推断的过程相对简单，也更容易进行线上部署和服务。因此，FM在2021-2014年前后，成为业界主流的推荐模型之一。\n3.FFM模型——引入特征域的概念 相比FM模型，FMM模型引入了特征域感知(field-aware)这一概念，使模型的表达力更强。\n$\\phi FMM(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}(w_{j_1,f_2} \\cdot w_{j_2,f_1})x_{j_1}x_{j_2}$\n当$x_{j1}$特征与$x_{j2}$特征进行交叉时，$x_{j1}$特征会从$x_{j1}$的这一组隐向量中挑出与特征$x_{j2}$的域$f_2$对应的隐向量$w_{j1,f_2}$进行交叉。这里说的 域(field)是指某个分类特征one-hot形成的一段特征向量。\nFMM保留了域的概念增强了模型表达能力，这也导致计算复杂度上升到$kn^2$，在实际工程应用中，需要在模型效果和工程投入之间进行权衡。\n四.GBDT+LR——特征工程模型化的开端 无论是FM还是FMM都是在做二阶特征交叉，如果继续提高特征交叉维度，会不可避免地产生组合爆炸和计算复杂度过高的问题。2014年，Facebook提出了基于GBDT+LR的组合模型解决方案。利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型输入。GBDT和LR这两步是独立训练的，所以不存在如何将LR的梯度回传到GBDT这类复杂问题。\nGDBT是由多棵回归树组成的树林，后一棵树以前面树林的结果与真实结果的残差为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此书中每个节点的分裂是一个自然的特征选择过程，而多层节点的结构则对特征进行了有效的自动组合，GDBT中每一个树都是一个交叉特征，而树的深度决定了交叉的阶数。\n虽然GDBT有如此强大的特征组合能力，但GBDT容易产生过拟合，以及丢失了大量特征的数值信息，因此不能直接说GBDT的交叉能力强，效果就比FMM好，在模型的选择和调试上，永远都是多种因素综合作用的结果。\n五.LS-PLM——阿里巴巴曾经的主流推荐模型 LS-PLM(Large Scale Piece-wise Linear Model,大规模分段线性模型)虽然在2017年才被阿里巴巴公之于众，但其实早在2012年，它就是阿里巴巴主流的推荐模型，并在深度学习模型提出之前长时间应用于阿里巴巴的各类广告场景。LS-PLM的结构与三层神经网络极其相似，在深度学习来临的前夜，可以将它看作推荐系统领域连接两个时代的节点。\nLS-PLM，又被称为MLR(Mixed Logistic Regression，混合逻辑回归)，它在逻辑回归的基础上采用分而治之的思想，先对样本分片，再在样本分片中引用逻辑回归进行预估，其灵感来自对广告推荐领域样本特点的观察。为了让CTR模型对不同用户群体、不同使用场景更有针对性，其采用的方法是先对全量样本进行聚类，再对每个分类施以逻辑回归模型进行CTR预估。\n$f(x)= \\sum ^m _{i=1} \\pi_i(x) \\cdot \\eta_i(x) = \\sum ^m {i=1} \\cfrac {e^{\\mu_i \\cdot x}}{\\sum^m{j=1} e^{\\mu_j \\cdot x}} \\cdot \\cfrac {1}{1+e^{-w_i \\cdot x}} $\n先用聚类函数$\\pi$对样本进行分类(这里的$\\pi$采用了$softmax$函数对样本进行多分类)，这个样本对每个分类都有一个概率值，这些值的和为1($softmax$的性质)。再用LR模型计算每个切片（类）的CTR，然后求加权CTR和。其中的超参数“分片数”m可以较好地平衡模型，当m=1时，LS-PLM就退化为普通LR模型，m越大，模型的拟合能力越强。但与此同时，模型参数规模也随m的增长而线性增长，模型收敛所需的训练样本也随之增长。在实践中，阿里巴巴给出的m的经验值为12.\nLS-PLM模型适用于工业级的推荐、广告等大规模稀疏数据的场景，主要是有以下两个优势①端到端的非线性学习能力②模型的稀疏性强（L1范数比L2范数更容易产生稀疏解），部署更加轻量级。\n从深度学习角度重新审视LS-PLM模型，LS-PLM模型可以看作一个加入了注意力(Attention)机制的三层神经网络模型，其中输入层是样本的特征向量，中间层是由m个神经元组成的隐层，其中m是分片的个数，对于一个CTR预估模型，LS-PLM的最后一层自然是由单一神经元组成的输出层。那么，注意力机制又是哪里应用的呢？其实是在隐层和输出层之间，神经元之间的权重是由分片函数得出的注意力得分来确定的，也就是说，样本属于哪个分片的概率就是其注意力得分。\n传统推荐模型总结    模型名称 基本原理 特点 局限性     协同过滤 根据用户的行为历史生成用户-物品共现矩阵，利用用户相似性和物品相似性进行推荐 原理简单、直接，应用广泛 泛化能力差，处理稀疏矩阵的能力差，推荐结果的头部效应明显   矩阵分解 将协同过滤算法中的共现矩阵分解为用户矩阵和物品矩阵，利用用户隐向量和物品隐向量的内积进行排序并推荐 相较协同过滤，泛化能力有所增强，对稀疏矩阵的处理能力有所增强 除了用户历史行为数据，难以利用 其他用户、物品特征及上下文特征   逻辑回归 将推荐问题转换成类似CTR预估的二分类问题，将用户、物品、上下文等不同特征转换成特征向量，再按照预估CTR进行排序并推荐 能够融合多种类型的不同特征 模型不具备特征组合能力，表达能力较差   FM 再逻辑回归的基础上，再模型中假如二阶特征交叉部分，为每一维特征训练得到相应特征隐向量，通过隐向量的内积运算得到交叉特征权重 相比逻辑回归，具备了二阶特征交叉能力，模型的表达能力有所增强 由于组合爆炸问题的限制，模型不易扩展到三阶特征交叉阶段   FFM 在FM模型的基础上，加入“特征域”的概念，使每个特征在与不同域的特征交叉时采用不同的隐向量 相比FM，进一步加强了特征交叉能力 模型的训练开销达到了O(n2)的量级，训练开销较大   GBDT+LR 利用GBDT进行“自动化”的特征组合，将原始特征向量转换成离散型特征向量，并输入逻辑回归模型，进行最终的CTR预估 特征工程模型化，使模型具备了更高阶特征组合的能力 无法进行完全的并行训练，模型更新所需的训练时长较长   LS-PLM 首先对样本进行“分片”，在每个“分片”内部构建逻辑回归模型，将每个样本的各个“分片”概率与逻辑回归的得分进行加权平均，得到最终的预估值 模型结构类似三层神经网络，具备了较强的表达能力 模型结构相比深度学习模型仍比较简单，有进一步提高的空间    2006年，矩阵分解的技术成功应用在推荐系统领域，其隐向量的思想与深度学习中Embedding技术的思路一脉相承；2010年，FM被提出，特征交叉的概念被引入推荐模型，其核心思想——特征交叉的思路也将在深度学习模型中被发扬光大；2012年，LS-PLM在阿里巴巴大规模应用，其结构已经非常接近三层神经网络；2014年，Facebook用GBDT自动化处理特征，揭开了特征工程模型化的篇章。\n另外，Alex Krizhevsky站在Geoffrey Hinton、Yann LeCun、Yoshua Bengio等大师的肩膀上，于2012年提出了引爆整个深度学习浪潮的AlexNet，将深度学习的大幕正式拉开，其应用快速地从图像扩展到语音，再到自然语言处理领域，推荐系统领域也必然紧随其后，投入深度学习的大潮之中。\n从2016年开始，随着FNN、Wide\u0026amp;Deep、Deep Crossing等一大批优秀的推荐模型架构的提出，深度学习模型逐渐席卷推荐和广告领域，成为新一代推荐模型当之无愧的主流。\n深度学习在推荐系统中的应用 随着微软的Deep Crossing，谷歌的Wide\u0026amp;Deep，以及FNN、PNN等一大批优秀的深度学习推荐模型在2016年被提出，推荐系统和计算广告领域全面进入深度学习时代。深度学习时代主要在以下两方面取得重大进展：\n①深度学习模型的表达能力更强，能够挖掘出更多数据中潜藏的模式\n②深度学习的模型结构非常灵活，能够根据业务场景和数据特点，灵活调整模型结构，使模型与应用场景完美契合\n沿着特征工程自动化的思路，深度学习模型从PNN一路走来，经过了Wide\u0026amp;Deep、Deep\u0026amp;Cross、FNN、DeepFM、NFM等模型，进行了大量的、基于不同特征互操作思路的尝试。但特征工程的思路走到这里已经穷尽了可能的尝试，模型进一步提升的空间很小，这也是这类模型的局限性所在。从这之后，越来越多的深度学习推荐模型开始探索更多”结构“上的尝试，诸如注意力机制、序列模型、强化学习等在其他领域大放异彩的模型结构也逐渐进入推荐系统领域，并且在推荐模型的效果提升上成果显著。\n一.AutoRec——单隐层神经网络推荐模型 AutoRec在2015年由澳大利亚国立大学提出。它将自编码器（AutoEncoder）的思想和协同过滤结合，提出了一种单隐层神经网络推荐模型。\n自编码器的原理类似于协同过滤中的共现矩阵，主成分分析等，相当于在重建函数$h(r； \\theta)$中存储了所有数据向量的“精华”。\n如上图，AutoRec是一个非常标准的三层神经网络，紫色单隐层的数量k远小于输入/输出评分向量的维度m，所以可以达到“泛化”的效果。\n重建函数的具体形式：\n$h(r; \\theta) = f(W \\cdot g(Vr+\\mu) + b)$\n其中，$f(\\cdot)$，$g(\\cdot)$分别为输出层神经元和隐层神经元的激活函数。\n为防止重构函数的过拟合，在加入L2正则化项后，AutoRec目标函数的具体形式：\n$\\underset {\\theta}{min} \\sum^m_{j=1}||r^{(i)}-h(r^{(i)} ; \\theta)||^2_O + \\cfrac{\\lambda}{2} \\cdot(||W||^2_F+||V||^2_F)$\n由于AutoRec是一个非常标准的三层神经网络，模型的训练利用梯度反向传播即可完成。\nAutoRec与协同过滤一样，有基于Item的I-AutoRec（Item based AutoRec），当输入物品$i$的评分向量$r^{(i)}$时，模型的输出向量$h(r^{(i)}; \\theta)$就是所有用户对$i$的评分预测。通过遍历，就可以得到一个用户$u$对所有物品的评分预测，进而根据评分预测排序得到推荐列表。U-AutoRec（User based AutoRec）相比I-AutoRec的优势在于仅需输入一次目标用户的用户向量，就可以重建用户对所有物品的评分向量，劣势是用户向量的稀疏性可能会影响模型效果。\n总体来说，AutoRec使用一个单隐层的AutoEncoder泛化用户或物品评分，有泛化和表达能力但是并不足。在模型结构上，AutoRec模型和后来的词向量模型(Word2vec)完全一致，但优化目标和训练方法有所不同。\n二.Deep Crossing——经典的深度学习框架 Deep Crossing由微软在2016年提出，应用在搜索引擎Bing的搜索广告推荐场景。广告点击率则作为Deep Crossing模型的优化目标，即CTR模型。\nDeep Crossing模型特征可以分为三类：一类是可以被处理成one-hot或multi-hot的类别型特征，一类是数值型特征，一类是需要进一步处理的特征，包括广告计划（campaign）、曝光样例（impression）、点击样例（click）等。\n为了完成端到端的训练，Deep Crossing解决了以下三个问题：\n①稀疏特征稠密化——Embedding层以经典的全连接层（Fully Connected Layer）结构为主，另有衍生出的Word2vec、Graph Embedding等。一般来说，Embedding向量的维度应远小于原始的稀疏特征向量，大多几十到上百维。数值型特征不需要Embedding，直接进入Stacking层。\n②自动交叉组合——Multiple Residual Units，相比标准的以感知机为基本单元的神经网络，Deep Crossing采用了多层残差网络（Multi-Layer Residual Network）作为MLP的具体实现。\n③输出层达成CTR预测的目标——Scoring层采用sigmoid（图像分类等多分类问题多采用softmax）\nStacking层比较简单，是把不同的Embedding特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量，该层通常也成为连接层（concatenate）。\n残差神经网络\n最著名的残差网络是在ImageNet大赛中由微软研究员何凯明提出的152层残差网络。推荐模型中的应用也是残差网络首次在图像识别领域之外的成功推广。残差神经网络就是由残差单元（Residual Unit）组成的神经网络，\n上面的残差单元与传统感知机的区别主要有两个不同：\n①输入经过两层以ReLU为激活函数的全连接层后，生成输出向量。\n②输入可以通过一个短路（shortcut）通路直接与输出向量进行元素加（element-wise plus）操作，生成最终的输出向量。\n此时，残差单元中的两层ReLU网络其实拟合的是输出和输入之间的残差（$x^o-x^i$），这就是残差神经网络名称的由来。\n残差神经网络的诞生主要为了解决两个问题：\n①神经网络加深后，容易产生过拟合。残差网络中，由于有输入向量短路的存在，很多时候可以越过两层ReLU网络，减少过拟合的发生。\n②残差单元使用ReLU激活函数取代sigmoid，越靠近0梯度越大。并且输入向量短路相当于直接把梯度毫无变化地传递到下一层，这也使残差网络收敛速度更快。\n三.NeuralCF——CF与深度学习的结合 Embedding层的主要作用是将稀疏向量转换成稠密向量，那么矩阵分解层的用户隐向量和物品隐向量完全可以看作一种Embedding方法。而用户隐向量和物品隐向量的内积操作则可以看作Scoring层。在实际使用矩阵分解来训练和评估模型的过程中，往往会发现模型容易处于欠拟合状态。究其原因是因为矩阵分解的模型结构相对比较简单，特别是Scoring层，无法对优化目标进行有效的拟合。这就要求模型有更强的表达能力，在此动机的启发下，新加坡国立大学的研究人员提出了NeuralCF模型。\nNeuralCF用“多层神经网络+输出层”的结构替代了矩阵分解中简单的内积操作。这样做的收益是直观的，一是让用户向量和物品向量做更充分的交叉，得到更多有价值的特征组合信息；二是引入更多的非线性特征，让模型的表达能力更强。\n以此类推，事实上，用户和物品向量的互操作层可以被任意的互操作形式所代替，这就是所谓的“广义矩阵分解”模型（Generalized Matrix Factorization）。例如，Scoring元素积+输出层逻辑回归。再进一步，可以把不同互操作网络得到的特征向量拼接起来，交由输出层进行拟合。NeuralCF的论文中给出了整合两个网络的例子：\nNeuralCF模型实际上提出了一个模型框架，它基于 用户向量和物品向量这两个Embedding层，利用不同的互操作层进行特征的交叉组合，并且可以灵活地进行不同互操作层的拼接。从这里可以看出深度学习构建推荐模型的优势——利用神经网络理论上能够拟合任意函数的能力，灵活地组合不同的特征，按需增加或减少模型的复杂度。\n在实践中要注意：并不是模型越复杂、特征越多越好。一是要防止过拟合的风险，而是往往需要更多数据和更长的训练时间才能使复杂的模型收敛，这需要算法工程师在模型的实用性、实时性和效果之间进行权衡。\nNeuralCF模型也存在局限性。由于是基于协同过滤的思想进行构造的，所以NeuralCF模型并没有引入更多其他类型的特征，这在实际应用中无疑浪费了其他有价值的信息。此外，对于模型中互操作的种类并没有做进一步的探究和说明。这就需要后来者进行更深入的探索。\n四.PNN——加强特征交叉能力 NeuralCF只提到了用户向量和物品向量两组特征向量，如果加入多组特征向量又该如何设计特征交互的方法呢？2016年，上海交通大学提出的PNN模型，给出了特征交互方式的几种设计思路。\n相比Deep Crossing，PNN模型在输入、Embedding层、多层神经网络，以及最终的输出层部分并没有结构上的不同，唯一的区别在于PNN模型用乘积层（Product Layer）代替了Deep Crossing模型中的Stacking层。也就是说，不同特征的Embedding向量不再是简单的拼接，而是用Product操作进行两两相交，更有针对性地获取特征之间的交叉信息。\nPNN的Product层的多种特征交叉方式 PNN模型对于深度学习的创新主要在于乘积层的引入。具体地说，PNN模型的乘积层由线性操作部分（上图z部分，对各特征向量进行线性拼接）和乘积操作部分（上图p部分）。其中，乘积特征交叉部分又分为内积操作和外积操作，其中内积操作的PNN模型被称为IPNN（Inner Product-based Neural Network），使用外积操作的PNN模型被称为OPNN（Outer Product-based Neural Network）。\n其中外积操作，$g_{outer}(f_i,f_j) = f_if_j^T$，外积互操作生成的是特征向量$f_i,f_j$各维度两两交叉而成的一个$M \\times M$的方形矩阵（其中$M$是输入向量的维度）。这样的外积操作无疑会将问题的复杂度从$M$提升到$M^2$，为了一定程度上减少训练负担，PNN模型的论文中介绍了一种降维的方法，就是把所有两两特征Embedding向量外积互操作结果叠加（Superposition），形成一个叠加外积操作矩阵$p$：\n$p=\\sum^N_{i=1} \\sum^N_{j=1}g_{outer}(f_i,f_j) = \\sum^N_{i=1} \\sum^N_{j=1} f_if_j^T=f_ \\sum f_\\sum^T,f_\\sum=\\sum^N_{i=1}f_i$\n从公式看，叠加矩阵$p$的最终形式类似于让所有特征Embedding向量通过一个平均池化层（Average Pooling）后，再进行外积互操作。在实际应用中，还应对平均池化操作谨慎对待。因为把不同特征对应维度进行平均，实际上是假设不同特征的对应维度有类似含义。但显然，年龄和地域两个特征在经过各自的Embedding后，两者的Embedding向量不在一个向量空间中，显然不具备任何可比性。这是做平均池化，会模糊很多有价值的信息。平均池化的操作经常发生在同类Embedding上，例如，将用户浏览过的多个物品的Embedding进行平均。因此，PNN模型的外积池化操作也需要谨慎，在训练效率和模型效果上进行权衡。\n事实上，PNN模型在对特征的线性和乘积操作后，并没有把结果直接送入上层的$L_1$全连接层，而是在乘积层内部又进行了局部全连接的转换，分别将线性部分$z$，乘积部分$p$映射成了$D_1$维的输入向量$l_z$和$l_p$（$D_1$为$L_1$隐层的神经元数量），再将$l_z$和$l_p$叠加，输入$L_2$隐层。这部分操作不具备创新性，并且可以被其他转换操作完全替代，因此不再详细介绍。\nPNN的结构特点在于强调了特征Embedding向量之间的交叉方式是多样化的，相比于简单的交由全连接层进行无差别化的处理，PNN模型定义的内积和外积操作显然更有针对性地强调了不同特征之间的交互，从而让模型更容易捕获特征交叉信息。\n但PNN模型同样存在局限性，例如在外积操作时，为了优化$M \\times M$的训练效率，对所有特征进行无差别交叉（平均池化），这一定程度上忽略了原始特征向量中包含的有价值信息。如何综合原始特征及交叉特征，让特征交叉的方式更加高效，后续的Wide\u0026amp;Deep模型和基于FM的各类深度学习模型将给出他们的解决方案。\n五.Wide\u0026amp;Deep——记忆能力和泛化能力的综合 谷歌于2016年提出Wide\u0026amp;Deep模型，由单层的Wide部分和多层的Deep部分组成的混合模型。其中，Wide部分的作用是让模型具有较强的“记忆能力”（memorization）；Deep部分的主要作用是让模型具有“泛化能力”（generalization），使模型兼具了逻辑回归（简单模型的记忆能力强）和深度神经网络（深度学习网络不断进行的交叉处理，会减弱记忆能力，但会拥有泛化能力）的优点——能够快速处理并记忆大量历史特征，并且具有强大的表达能力，不仅在当时迅速成为业界争相应用的主流模型，而且衍生出了大量以Wide\u0026amp;Deep模型为基础结构的混合模型，影响力一直延续至今。\nWide\u0026amp;Deep模型把单输入层的Wide部分与由Embedding层和多隐层组成的Deep部分连接起来，一起输入最终的输出层（逻辑回归）。其中Wide部分善于处理大量稀疏特征，而Deep部分善于挖掘特征背后的数据模式。这种把不同特征使用不同处理方法的组合模型，就需要对业务场景的深刻理解。从下图可以详细地了解到Google Play的推荐团队到底将哪些特征作为Deep输入，哪些作为Wide部分输入。\nDeep部分输入全量的特征向量，拼接成1200维的Embedding向量，再经过3层ReLU全连接层，最终输入LogLoss输出层。\nWide部分输入仅仅是已安装应用和曝光应用两类特征，其中已安装应用代表用户的历史行为，而曝光应用代表当前的待推荐应用。选择这两类特征的原因是充分发挥Wide部分的记忆能力，使用简单模型善于记忆用户行为特征中的信息，并根据此类信息直接影响推荐结果。\nWide部分组合“已安装应用”和”曝光应用“两个特征的函数被称为交叉积变换（Cross Product Transformation）函数，其形式化定义如：\n$\\phi_k(X)= \\prod_{i=1}^{d}x_i^{c_ki}$ $c_{ki} \\in {0,1}$\n$c_{ki}$是一个布尔变量，当第$i$个特征属于第$k$个组合特征时，$c_{ki}$的值为1，否则为0；$x_i$是第$i$​个特征的值。例如，对于”AND(user_installed_app=netflix, impression_app=pandora)“这个组合特征来说，只有当\u0026quot;user_installed_app=netflix\u0026quot;和”impression_app=pandora“这两个特征同时为1时，其对应的交叉积变换层的结果才为1，否则为0。\n在通过交叉积变换层操作完成特征组合之后，Wide部分将组合特征输入最终的LogLoss输出层，与Deep部分的输出一同参与最后的目标拟合，完成Wide与Deep的融合部分。\nWide\u0026amp;Deep开启了不同网络融合的新思路，日后有比较经典的2017年由斯坦福大学和谷歌的研究人员提出的Deep\u0026amp;Crossing模型。其主要思路是使用Cross网络替代原来的Wide部分。\n使用Cross网络的目的是增加特征之间的交互力度，使用多交叉层（Cross layer）对输入向量进行特征交叉。假设第$l$层交叉层的输出向量为$x_l$，那么第$l+1$层的输出向量：\n$x_{l+1}=x_0x_l^TW_l + b_l + x_l$\n可以看到，交叉层操作的二阶部分类似于PNN模型中的外积操作，在此基础上增加了外积操作的权重向量$w_l$，以及原输入向量$x_l$和偏置向量$b_l$。\n可以看出，Cross层在参数方面是比较”克制“的，每一层仅增加了一个$n$维的权重向量$w_l$（n维输入向量维度），并且在每一层均保留了输入向量，因此输入与输出之间变化不会非常明显。由多层交叉层组成的Cross网络在Wide\u0026amp;Deep模型中的Wide部分的基础上进行特征的自动化交叉，避免了很多基于业务理解的人工特征组合。同Wide\u0026amp;Deep模型一样，Deep\u0026amp;Cross模型的Deep部分相比Cross部分表达能力更强，使模型具备更强的非线性学习能力。\nWide\u0026amp;Deep模型的影响力无疑是巨大的，不仅是其本身成功应用于多家一线互联网公司，而且其后续的改进创新工作也延续至今。事实上，DeepFM、NFM等模型都可以看成Wide\u0026amp;Deep模型的延伸：\nWide\u0026amp;Deep模型能够取得成功的关键在于：\n①抓住了业务问题的本质特点，能够融合传统模型记忆能力和深度学习模型泛化能力的优势\n②模型的结构并不复杂，易于工程实现、训练和上线，这加速了业界推广\n也正是从Wide\u0026amp;Deep模型之后，越来越多的模型结构被加入推荐模型中，深度学习模型的结构开始朝着多样化、复杂化的方向发展。\n六.FM与深度学习的结合 FNN——用FM的隐向量完成Embedding层初始化 FNN由伦敦大学学院的研究人员于2016年提出，以FM改进Embedding层的Deep crossing模型，用FM模型训练好的各特征向量初始化Embedding层的参数代替随机初始化，相当于在初始化神经网络参数时，已经引入了有价值的先验信息。也就是说，神经网络训练的起点更接近目标最优点，自然加速了整个神经网络的收敛过程。\n一般情况下，模型的收敛速度往往受限于Embedding层。主要有两个原因：①Embedding层的参数量巨大。假设输入层维度维100,000，Embedding层输出维度为32，上层再加5层32维的全连接层，最后输出层维度为10，那么输入层到Embedding层的参数数量是$32 \\times100,000=3,200,000$，其余所有层的参数总数是$(32 \\times 32) \\times4+32\\times 10 = 4416$​。此时Embedding层参数占比99.86%。这就导致大部分的训练时间和计算开销都被Embedding层占据。②由于输入向量过于稀疏，在随机梯度下降时，只有与非零特征相连的Embedding层权重会被更新，这进一步降低了Embedding层的收敛速度。\n需要说明的是，在训练FM的过程中，并没有对特征域进行区分，但在FNN模型中，特征被分成了不同特征域，因此每个特征域具有对应的Embedding层，并且每个特征域Embedding的维度都应与FM隐向量维度保持一致。\nDeepFM——用FM代替Wide部分 FNN把FM的结果作为初始化权重，并没有调整模型结构。而2017年由哈尔滨工业大学和华为公司联合提出的DeepFM则将FM模型与Wide\u0026amp;Deep模型整合：\nFM部分与深度神经网络部分共享相同的Embedding层。左侧FM部分对不同的特征域的Embedding进行两两交叉，也就是将Embedding向量当作原FM中的特征隐向量。最后将FM的输出与Deep部分的输出一同输入最后的输出层，参与最后的目标拟合。\nDeepFM与Deep\u0026amp;Cross模型完全一致，唯一的不同在于Deep\u0026amp;Cross利用多层Cross网络进行特征组合，而DeepFM模型利用FM进行特征组合。当然，具体的应用效果还需要通过实验进行比较。\nNFM——FM的神经网络化尝试 无论是FM还是FFM，归根结底是一个二阶特征交叉的模型，受组合爆炸问题的困扰，FM几乎不可能扩展到三阶以上，这就不可避免地限制了FM模型的表达能力。2017年，新加坡国立大学的研究人员进行了这方面的尝试，提出了NFM模型。\nNFM模型的主要思路是用一个表达能力更强的函数替代原FM中二阶隐向量内积的部分：\n$\\hat{y}{NFM}(x)=w_0 + \\sum^N{i=1}w_ix_i+f(x)$\n传统机器学习可以用来拟合$f(x)$一个表达能力更强的函数，但是进入深度学习时代后，由于深度学习网络理论上有拟合任何复杂函数的能力，$f(x)$的构造工作可以交由某个深度学习网络来完成，并通过梯度反向传播来学习。\nNFM网络架构的特征非常明显，就是在Embedding和神经网络之间加入特征交叉池化层（Bi-Interaction Pooling Layer）：\n$f_{BI}(V_x)=\\sum^n_{i=1} \\sum^n_{j=i+1}(x_iv_i)\\bigodot(x_jv_j)$\n其中，$\\bigodot$代表元素积操作，其中第k维的操作：\n$(v_i\\bigodot v_j)k=v{ik}v_{jk}$\n在进行两两元素积操作后，对交叉特征向量取和，得到池化层的输出向量。再把该向量输入上层的多层全连接神经网络，进行进一步的交叉。\n上图的NFM省略了一阶部分，如果把一阶部分视为一个线性模型，那么NFM的架构也可以视为Wide\u0026amp;Deep模型的进化。相比原始的Wide\u0026amp;Deep模型，NFM模型对其Deep部分加入了特征交叉池化层，加强了特征交叉。这是理解NFM模型的另一个角度。\n七.注意力机制在推荐模型中的应用 “注意力机制”来源于人类最自然的选择性注意的习惯，从2017年开始，推荐领域也开始尝试将注意力机制引入模型之中，这其中影响力较大的工作是由浙江大学提出的AFM和由阿里巴巴提出的DIN。这一机制堆深度学习推荐系统的启发是重大的，使得其更接近用户真实的思考过程。\nAFM——引入注意力机制的NFM 在NFM模型中，不同域的特征Embedding向量经过特征交叉池化层的交叉，将各交叉特征向量进行“加和”，输入最后由多层神经网络组成的输出层。AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络（Attention Net）实现的。AFM的模型结构图：\n注意力网络的作用是为每一个交叉特征提供权重，也就是注意力得分。\n同NFM一样，AFM的特征交叉过程同样采用了元素积操作：\n$f_{PI}(\\varepsilon){(v_i\\bigodot v_j)x_ix_j}_{(i,j)\\in R_x}$\n引入注意力得分后的池化过程：\n$f_{Att}(f_{PI}(\\varepsilon)) = \\sum_{(i,j) \\in R_x} a_{ij}(v_i\\bigodot v_j)x_ix_j$​\n对注意力的分$a_{ij}$来说，最简单的方法就是用一个权重参数来表示，但为了防止交叉特征数据稀疏问题带来的权重参数难以收敛，AFM模型使用了一个在两两特征交叉层（Pair-wise Interaction Layer）和池化层之间的注意力网络来生成注意力得分。 该注意力网络的结构是一个简单的单全连接层加softmax输出层的结构：\n$a^`_{ij}=h^TReLU(W(v_i\\bigodot v_j)x_ix_j+b)$\n$a_{ij}=\\cfrac{exp(a^_{ij})}{\\sum_{(i,j)\\in R_x} exp(a^_{ij})}$\n其中需要学习的参数是特征交叉层到注意力网络全连接层的权重矩阵$W$，偏置向量$b$，以及全连接层到softmax输出层的权重向量$h$。注意力网络将与整个模型一起参与梯度反向传播的学习过程，得到最终的权重参数。\nAFM是研究人员从改进模型角度进行的一次尝试。而阿里巴巴引入注意力机制是基于其对业务观察的一次模型改进，下面介绍阿里巴巴在业界非常知名的推荐模型DIN。\nDIN——引入注意力机制的神经学习网络 它的应用场景是阿里巴巴的电商广告推荐，在计算一个用户$u$是否点击一个广告$a$时，模型的输入特征自然分为两大部分：一部分是用户$u$的特征组，另一部分是候选广告$a$的特征组。无论是用户还是广告，都含有两个非常重要的特征——商品id(good_id)和商铺id(shop_id)。用户特征里的商品特征是一个序列，代表用户曾点击过的商品合集，商铺id同理；而广告特征里的商品id和商铺id就是广告对应的商品id和商铺id（阿里巴巴平台上的广告大部分是参与推广计划的商品）。\n在原来的基础模型中，这些特征进行简单的平均池化操作就后就进入上层神经网络进行下一步训练，序列中的商品既没有区分重要程度，也和广告特征中的商品id没有关系。\n然而事实上，广告特征和用户特征的关联程度是非常强的，假设广告中的商品是键盘，那么用户点击商品序列中的不同商品id：鼠标、T恤和洗面奶。从常识出发，鼠标这个历史商品对预测键盘广告的点击率的重要程度远大于后两者。从模型角度，基于不同特征的注意力理应不同，而且“注意力得分”的计算理应与广告特征有相关性。\n模型中的注意力的强弱，利用候选商品和历史行为商品之间的相关性计算出一个权重，这个权重就代表了“注意力”强弱：\n$V_u=f(V_a)=\\sum ^N_{i=1}w_i \\cdot V_i=\\sum ^N_{i=1}g(V_i,V_a) \\cdot V_i$\n其中$V_u$是用户的Embedding向量，$V_a$是候选广告商品的Embedding向量，$V_i$是用户$u$的第$i$次行为的Embedding向量。这里用户的行为就是浏览商店或店铺，因此行为的Embedding向量就是那次浏览的商品或店铺的Embedding向量。$g(V_i,V_a)$即注意力得分函数采用一个注意力激活单元（activation unit）。其本质上也是一个小的神经网络，其具体结构如上图右上角。可以看出，激活单元的输入层是两个Embedding向量，经过元素减（element-wise minus）操作后，与原Embedding向量一同连接后形成全连接层的输入，最后通过单神经元输出层生成注意力得分。\nDIEN——序列模型与推荐系统的结合 从“注意力机制”开始，越来越多对深度学习模型结构的改进是基于对用户行为的深刻观察而得出。DIEN基于DIN，创新在于用序列模型模拟了用户兴趣的进化过程。序列信息的重要性在于：①加强了最近行为对下次行为预测的影响。②能够学习到购买趋势的信息，如果某个转移概率在全局统计意义上足够高——购买过篮球鞋后购买机械键盘的概率，那么在用户购买篮球鞋时，推荐机械键盘也会成为一个不错的选择。直观上，两者的用户群体很有可能是一致的。\n如果失去序列信息，推荐模型则是基于用户购买历史的综合推荐，而不是针对“下一次购买”的推荐，显然，从业务角度看，后者才是推荐系统正确的推荐目标。\n其中兴趣进化网络分为三层，从下至上依次是：\n①行为序列层（Behavior Layer，浅绿色部分）：其主要作用是把原始的id类行为序列转换成Embedding行为序列。\n②兴趣抽取层（Interest Extractor Layer，米黄色部分）：其主要作用是通过模拟用户兴趣迁移过程，抽取用户兴趣。\n③兴趣进化层（Interest Evolving Layer，浅红色部分）：其主要作用是通过在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程。\n在兴趣进化网络中，行为序列层的结构与普通的Embedding层是一致的，模拟用户兴趣进化的关键在于“兴趣抽取层”和“兴趣进化层”。\n兴趣抽取层的基本结构是GRU（Gated Recurrent Unit）\n本章介绍了以下模型，但深度学习推荐模型从没停下他前进的脚步。从阿里巴巴的多模态、多目标的深度学习模型，到Youtube基于session的推荐系统，再到Airbnb使用Embedding技术构建的搜索推荐模型，深度学习推荐模型不仅进化速度越来越快，而且应用场景也越来越广。在之后的章节中，笔者会从不同的角度出发，介绍深度学习模型再推荐系统中的应用，也希望读者可以在本章的知识结构上，跟踪最新的深度学习推荐模型进展。\n   模型名称 基本原理 特点 局限性     AutoRec 基于自编码器，对用户或者物品进行编码，利用自编码器的泛化能力进行推荐 单隐层神经网络结构简单，可实现快速训练和部署 表达能力较差   Deep Crossing 利用“Embedding层+多隐层+输出层”的经典深度学习框架，预完成特征的自动深度交叉 经典的深度学习推荐模型框架 利用全连接隐层进行特征交叉，针对性不强   NeuralCF 将传统的矩阵分解中用户向量和物品向量的点积操作，换成由神经网络代替的互操作 表达能力加强版的矩阵分解模型 只使用了用户和物品的id特征，没有加入更多其它特征   PNN 针对不同特征域之间的交叉操作，定义“内积”“外积”等多种积操作 在经典深度学习框架上提高特征交叉能力 “外积”操作进行了近似化，一定程度上影响了其表达能力   Wide\u0026amp;Deep 利用Wide部分加强模型的“记忆能力”，利用Deep部分加强模型的“泛化能力” 开创了组合模型的构造方法，对深度学习推荐模型的后续发真产生重大影响 Wide部分需要人工进行特征组合的筛选   Deep\u0026amp;Cross 用Cross网络替代Wide\u0026amp;Deep模型中的Wide部分 解决了Wide\u0026amp;Deep模型人工组合特征的问题 Cross网络的复杂度较高   FNN 利用FM的参数来初始化深度神经网络的Embedding层参数 利用FM初始化参数，加快整个网络的收敛速度 模型的主结构比较简单，没有针对性的特征交叉层   DeepFM 在Wide\u0026amp;Deep模型的基础上，用FM替代原来的线性Wide部分 加强了Wide部分的特征交叉能力 与经典的Wide\u0026amp;Deep模型相比，结构差别不明显   NFM 用神经网络代替FM中二阶隐向量交叉的操作 相比FM,NFM的表达能力和特征交叉能力更强 与PNN模型的结构非常相似   AFM 在FM的基础上，在二阶隐向量交叉的基础上对每个交叉结果加入了注意力得分，并使用注意力网络学习注意力得分 不同交叉特征的重要性不同 注意力网络的训练过程比较复杂   DIN 在传统深度学习推荐模型的基础上引入注意力机制，并利用用户行为历史物品和目标广告物品的相关性计算注意力得分 根据广告物品的不同，进行更有针对性的推荐 并没有充分利用除“历史行为”以外的其他特征   DIEN 将序列模型与深度学习推荐模型结合，使用序列模型模拟用户的兴趣进化过程 序列模型增强了系统对用户兴趣变迁的表达能力，使推荐系统开始考虑时间相关的行为序列中包含的有价值信息 序列模型的训练复杂，线上服务的延迟较长，需要进行工程上的优化   DRN 将强化学习的思路应用于推荐系统，进行推荐模型的线上实时学习和更新 模型对数据实时性的利用能力大大加强 线上部分较复杂，工程实现难度较大    \\未完待续。。。:)\n","id":4,"section":"posts","summary":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和","tags":[],"title":"1小时读懂《深度学习推荐系统》","uri":"https://biofrostyy.github.io/2021/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","year":"2021"},{"content":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和谐统一的。例如YouTube的用户体验和公司利益（时长越长广告曝光越多）在“观看时长”这一点上达成了一致。\n下图是推荐系统的技术架构示意图。其中数据部分为融合了数据离线批处理、实时流处理的数据流框架；算法和模型部分则为集训练(training)、评估(evaluation)、部署(deployment)、线上推断(online inference)为一体的模型框架。\n推荐系统的进化之路 幸运的是，我们开始做这项工作时，作为用户画像组，我们有着丰富的用户画像。\n我们使用线上的实时特征包括拖寄物、上下文、流向等。\n第二版，我们增加了redis的历史购买。\n再后来，因为画像数据太多，我们分析了之后，对用户画像（交叉）进行了客群分析，使用新客群作为分类特征。\n再后来，增加了再次购买率特征，使点击率增加了1%\n再后来，为了解决长尾，我们将高频与长尾分开，max(高频)低于0.5时，进入长尾规则判断，类似冷启动阶段，使用基于矩阵分解的协同过滤。\n再后来，加入组合推荐，apriori分析，概率模型\n再后来，缺失走兜底规则过多，包装服务过多\n再后来，准时保（流向/拖寄物符合）推荐过多，调整准时保位置，降低判断位置\n","id":5,"section":"posts","summary":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和","tags":[],"title":"深度学习推荐系统","uri":"https://biofrostyy.github.io/2021/07/%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/","year":"2021"},{"content":"几乎所有的算法书中，排序算法都是在最开始介绍的算法，不仅仅是因为排序算法非常简单，而且因为排序算法非常基础，在后续其他算法或者处理其他问题时都有广泛的应用。就像我在刷题时，应用到排序就可以直接按时间复杂度$O(nlogn)$计算。所以此次复习算法和数据结构也从排序算法开始，使用到的工具书有\u0026lt;算法4\u0026gt;、\u0026lt;我的第一本算法书\u0026gt;及其app、\u0026lt;labuladong的算法小抄\u0026gt;。刷题网站为leetcode，理论上会把相关主题的题全部刷到，并且整理出笔记与代码沉淀。\n一.排序算法 ①时间复杂度为$O(n^2)$的排序算法——选择排序、插入排序、希尔排序 选择排序的过程为，首先，找到数组中最小的那个元素，其次，将它和数组的第一个元素交换位置（如果第一个元素就是最小元素那么它就和自己交换）。再次，在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置。如此往复，直到将整个数组排序。这种方法叫做选择排序，因为它在不断地选择剩余元素之中的最小者。\n总的来说，选择排序是一种很容易理解和实现的简单排序算法，它有两个很鲜明的特点。其一：运行时间和输入无关。即一个已经有序的数组或是主键全部相等的数组和一个元素随机排列的数组所用的排序时间一样长。一些其他的排序算法可能会更善于利用输入的初始状态。其二：数据移动是最少的。每次在找到最小值时才会进行一次交换，因此选择排序用了N次交换——交换次数和数组的大小是线性关系。我们将研究的其他任何算法都不具备这个特征（大部分的增长数量级都是线性对数或是平方级别）。\n插入排序则像整理桥牌一样一张一张的来，将每一张牌插入到其他已经有序的牌中的适当位置，与选择排序一样，当前索引左边的所有元素都是有序的，但它们的最终位置还不确定，为了给更小的元素腾出空间，它们可能会被移动。但是当索引到达数组的右端时，数组排序就完成了。\n对于随机排列的长度为$N$且主键不重复的数组，平均情况下插入排序需要$\\cfrac{N^2}{4}$次比较以及$\\cfrac{N^2}{4}$次交换。最坏情况下需要$\\cfrac{N^2}{2}$次比较和$\\cfrac{N^2}{2}$次交换，最好情况下需要$N-1$次比较和$0$次交换。其中的情况好坏取决于输入中元素的初始顺序。\n我们可以发现在比较好的情况下，即对于部分有序的数组，插入排序可以获得线性的时间复杂度！\nPS.衡量一个数组是不是部分有序和有序的程度时，可以用到倒置数量这个指标。倒置指的是数组中的两个顺序颠倒的元素。比如E X A M P L E中有11对倒置：E-A、 X-A、 X-M、 X-P、 X-L、 X-E、 M-L、 M-E、 P-L、 P-E以及L-E。如果数组中倒置的数量小于数组大小的某个倍数，那么我们说这个数组是部分有序的。下面是几种典型的部分有序的数组：❏数组中每个元素距离它的最终位置都不远；❏一个有序的大数组接一个小数组；❏数组中只有几个元素的位置不正确。插入排序对这样的数组很有效，而选择排序则不然。事实上，当倒置的数量很少时，插入排序很可能比本章中的其他任何算法都要快。\n上面我们说到，对于部分有序的数组，插入排序非常有效，那么我们可不可以对数组预处理成部分有序，再用插入排序呢。这就是希尔排序。希尔排序又叫缩小增量排序，它是基于插入排序的增强版。时间复杂度是$O(N*(logN)^2)$，在最坏的请款下比较次数和$N^{\\cfrac{3}{2}}$。人们发明了很多递增序列来渐进式地改进最坏情况下所需的比较次数（N4/3,N5/4, N6/5…），但这些结论大多只有学术意义，因为对于实际应用中的N来说它们的递增序列的生成函数（以及与N乘以一个常数因子）之间的区别并不明显。\n②时间复杂度为$O(nlogn)$的排序算法——归并排序、快速排序 当我们看到$logn$时不难想到\u0026quot;二分\u0026quot;的思想。\n归并排序运行速度比简单排序块，但是它需要的空间是原始数组空间的两倍；通常这是一个严重的缺点。\n二.代码沉淀 ①有序数组中查找左右边界 def helper(tar): #左边界 i, j = 0, len(nums) - 1 while i \u0026lt;= j: m = (i + j) // 2 if nums[m] \u0026lt; tar: i = m + 1 else: j = m - 1 return j def helper(tar): # 右边界 i, j = 0, len(nums) - 1 while i \u0026lt;= j: m = (i + j) // 2 if nums[m] \u0026lt;= tar: i = m + 1 else: j = m - 1 return i  ②双指针——将两个数组合并 # 有些题中，为了不占用新内存空间，可以使用逆向双指针在原数组上进行 class Solution: def merge(self, A: List[int], m: int, B: List[int], n: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Do not return anything, modify A in-place instead. \u0026quot;\u0026quot;\u0026quot; pointera = m-1 pointerb = n-1 while pointera \u0026gt;= 0 and pointerb \u0026gt;= 0: if A[pointera] \u0026gt;= B[pointerb]: A[pointera+pointerb+1] = A[pointera] pointera -= 1 else: A[pointera+pointerb+1] = B[pointerb] pointerb -= 1 while pointerb \u0026gt;= 0: A[pointerb] = B[pointerb] pointerb -= 1 '''题目链接：https://leetcode-cn.com/problems/sorted-merge-lcci/ 此题还可以直接把B数组填入A数组中，对A进行排序''' # 另一些题中，希望两种数字（如奇偶），在特定的位置，可以使用双指针，找到后交换两指针的元素  ③python排序实现 ------自定义排序 在python中有自定义排序函数,list.sort(key=函数) sorted_items = sorted(list,key=函数,reverse=False) 可以根据function对一个list进行排序，函数可以返回一个tuple(x[0],-x[1])，表示先根据x[0]排序，如果相同再根据x[1]排序，负号可以达到根据x[1]降序排列的目的 引申的，如果想根据一个元素在list中出现的次数排序，sorted(list,key=lambda x:(list.count(x),-x))，collections.Counter(s)方法可以返回一个字典，表示每个元素的出现次数。对于上述方法的提升---return ''.join([i*j for i, j in sorted([[i, j] for i, j in dict_al.items()], key=lambda x: -x[1])]) ------计数排序，当数据种类较少时可以使用 参考例题：https://leetcode-cn.com/problems/relative-sort-array/  ④python排列组合实现 ------全排列combinations和permutations函数 combinations方法重点在组合，permutations方法重在排列。返回的都是一个iterator。 ------回溯递归 def permutation(self, s: str) -\u0026gt; List[str]: if len(s) \u0026lt;= 1: #当只剩一个的时候返回 return [s] return list(set(s[i] + perm for i in range(len(s)) for perm in self.permutation(s[:i] + s[i+1:]))) # 此时当每个元素都不同时，不会有相同的返回值，当有基本元素相同时(['a','a','b']三个基本元素就会出现两个\u0026quot;aab\u0026quot;)，set()，可以帮助去除重复 def permutation(self, s: str) -\u0026gt; List[str]: # 递归普通写法 result = [] def permutation(ans, s): if not s: return result.append(ans) for i in set(s): new_ans = i new_s = s.copy() new_s.remove(i) permutation(ans + new_ans, new_s) permutation('', list(s)) return result -------回溯非递归 class Solution: def permutation(self, s: str) -\u0026gt; List[str]: n = len(s) curr = list(sorted(s)) end = list(reversed(curr)) ans = [] # 生成下一个排列 while curr != end: ans.append(''.join(curr)) i = n - 2 # 29631 -\u0026gt; 31269 while i \u0026gt; 0 and curr[i] \u0026gt;= curr[i+1]: i -= 1 j = n - 1 while j \u0026gt; i-1 and curr[j] \u0026lt;= curr[i]: j -= 1 curr[i], curr[j] = curr[j], curr[i] curr = curr[:i+1] + sorted(curr[i+1:]) ans.append(''.join(end)) return ans  ⑤python bisect二分查找 \u0026quot;\u0026quot;\u0026quot; bisect 为可排序序列提供二分查找算法 \u0026quot;\u0026quot;\u0026quot; import bisect #使用bisect函数前需要对列表进行排序，否则虽然可以输出数值，但没有意义 a = [1, 5, 6, 10, 9] a.sort() print(\u0026quot;最初的列表：\u0026quot;, a) #bisect.bisect 返回某个数在列表中可以插入的位置，但不会插入该数。 #如果这个数与列表中的元素相同，则返回元素后面的位置 print(\u0026quot;6在列表中可以插入的位置：\u0026quot;, bisect.bisect(a, 6)) #bisect.insort 将某个数插入列表 bisect.insort(a, 7) print(\u0026quot;在列表中插入7：\u0026quot;, a) #处理插入数值与列表元素相同的情况，返回位置，但不会插入该数 #bisect.bisect_left 插入元素左侧位置；bisect.bisect_right 插入元素右侧位置 print(\u0026quot;9在列表中可以插入的位置：\u0026quot;, bisect.bisect_left(a, 9)) print(\u0026quot;9在列表中可以插入的位置：\u0026quot;, bisect.bisect_right(a, 9)) #处理插入数值与列表元素相同的情况，插入该数 #bisect.insort_left 插入元素左侧位置；bisect.insort_right 插入元素右侧位置 bisect.insort_left(a, 9) print(\u0026quot;在列表中插入10：\u0026quot;, a) bisect.insort_right(a, 10) print(\u0026quot;在列表中插入10：\u0026quot;, a)  ","id":6,"section":"posts","summary":"几乎所有的算法书中，排序算法都是在最开始介绍的算法，不仅仅是因为排序算法非常简单，而且因为排序算法非常基础，在后续其他算法或者处理其他问题时","tags":[],"title":"数据结构复习-排序算法","uri":"https://biofrostyy.github.io/2021/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%8E%92%E5%BA%8F/","year":"2021"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等\n二.刷题沉淀 ①两个栈实现队列 # 类似于负负得正的思想，append时直接放入栈1，delete时如果栈2为空，那么把栈1移入栈2（负负得正，栈1最先放进去的最后弹出来，在栈2中最后放进去的最先弹出来），再pop栈2；如果原来栈2不为空，那么表示上次放入的还有剩（上次放入的一定比这次早），直接pop；如果移入后栈2仍为空，返回-1 class CQueue: def __init__(self): self.A,self.B = [],[] def appendTail(self, value: int) -\u0026gt; None: self.A.append(value) def deleteHead(self) -\u0026gt; int: if not self.B: if not self.A: return -1 else: while self.A: self.B.append(self.A.pop()) return self.B.pop() # Your CQueue object will be instantiated and called as such: # obj = CQueue() # obj.appendTail(value) # param_2 = obj.deleteHead()  ②队列实现栈 # 双队列实现栈，等于把一个队列B存放之前的结果，把新的元素放入A后，再把B中之前的元素放入A 执行用时：32 ms, 在所有 Python3 提交中击败了94.87%的用户 内存消耗：15 MB, 在所有 Python3 提交中击败了35.37%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A,self.B = deque(),deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) while self.B: self.A.append(self.B.popleft()) self.A,self.B = self.B,self.A self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.B.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.B[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0 # 单队列实现栈，存储原有元素数量n，把新元素放入队列中后，执行n次pop-append操作，把之前的元素重新放入队列中 执行用时：40 ms, 在所有 Python3 提交中击败了61.83%的用户 内存消耗：14.8 MB, 在所有 Python3 提交中击败了93.24%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A = deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) n = self.size while n \u0026gt; 0: self.A.append(self.A.popleft()) n -= 1 self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.A.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.A[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0  ③字符串匹配——KMP https://leetcode-cn.com/problems/implement-strstr/solution/zhe-ke-neng-shi-quan-wang-zui-xi-de-kmp-8zl57/ class Solution: def strStr(self, haystack: str, needle: str) -\u0026gt; int: a=len(needle) b=len(haystack) if a==0: return 0 next=self.getnext(a,needle) p=-1 for j in range(b): while p\u0026gt;=0 and needle[p+1]!=haystack[j]: p=next[p] if needle[p+1]==haystack[j]: p+=1 if p==a-1: return j-a+1 return -1 def getnext(self,a,needle): next=['' for i in range(a)] k=-1 next[0]=k for i in range(1,len(needle)): while (k\u0026gt;-1 and needle[k+1]!=needle[i]): k=next[k] if needle[k+1]==needle[i]: k+=1 next[i]=k return next  ④摩尔投票——选出数组中超过半数的值 class Solution: def majorityElement(self, nums: List[int]) -\u0026gt; int: # n = len(nums) # dic_nums = Counter(nums) # for key in dic_nums.keys(): # if dic_nums[key] \u0026gt; n/2: # return key # return -1 # 狼人杀归票算法 # 第一轮找到最可能出局的那个人 n = len(nums) ans = -1 count = 0 for num in nums: # 没有票数，暂时认为是当前的人 if not count: ans = num # 有相同的人上票，票数加一；否则票数减一 if num == ans: count += 1 else: count -= 1 # 第二轮确定这个人的票数确实过半 return ans if count and nums.count(ans) \u0026gt; n // 2 else -1  ","id":7,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等 二.刷题沉淀 ①两个栈实现队列 # 类似于负负得正的思想，ap","tags":[],"title":"数据结构-基础结构","uri":"https://biofrostyy.github.io/2021/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84/","year":"2021"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等$n^2$\n二.刷题沉淀 ①花费最少爬楼梯 class Solution: def minCostClimbingStairs(self, cost: List[int]) -\u0026gt; int: # cost.append(0) # def min_cost(i): # # 递归 -- 超时 # if i in (0,1): # return cost[i] # return cost[i] + min(min_cost(i-1),min_cost(i-2)) # return min_cost(len(cost)-1) # # 带记忆的递归 # cost.append(0) # cost_sum = [-1]*len(cost) # def min_cost(i): # if i in (0,1): return cost[i] # elif cost_sum[i] == -1: # cost_sum[i] = cost[i] + min(min_cost(i-1),min_cost(i-2)) # return cost_sum[i] # return min_cost(len(cost)-1) # DP cost.append(0) i, n, cost_num = cost[0],cost[1],0 for k in range(2,len(cost)): cost_num = min(i,n)+cost[k] i,n = n,cost_num return cost_num  ②队列实现栈 # 双队列实现栈，等于把一个队列B存放之前的结果，把新的元素放入A后，再把B中之前的元素放入A 执行用时：32 ms, 在所有 Python3 提交中击败了94.87%的用户 内存消耗：15 MB, 在所有 Python3 提交中击败了35.37%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A,self.B = deque(),deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) while self.B: self.A.append(self.B.popleft()) self.A,self.B = self.B,self.A self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.B.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.B[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0 # 单队列实现栈，存储原有元素数量n，把新元素放入队列中后，执行n次pop-append操作，把之前的元素重新放入队列中 执行用时：40 ms, 在所有 Python3 提交中击败了61.83%的用户 内存消耗：14.8 MB, 在所有 Python3 提交中击败了93.24%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A = deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) n = self.size while n \u0026gt; 0: self.A.append(self.A.popleft()) n -= 1 self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.A.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.A[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0  ","id":8,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等$n^2$ 二.刷题沉淀 ①花费最少爬楼梯 class Solution: def minCostClimbingStairs(self, cost: List[int]) -\u0026gt; int: #","tags":[],"title":"数据结构-基础结构","uri":"https://biofrostyy.github.io/2021/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","year":"2021"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等\n二.刷题沉淀 ①ListNode题 # 可以使用一个空起始点来做开头，这样就不需要制作list保存了，规避了第一个的下一个的悖论  ②str # 实现str倒序，-1表示步长，即每次向前一步 str(x)[::-1]  ③差分数组 差分数组--把区间统一修改转嫁为前后临界点的修改 1109.航班预定统计  ","id":9,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等 二.刷题沉淀 ①ListNode题 # 可以使用一个空起始点来","tags":[],"title":"数据结构-刷题tips","uri":"https://biofrostyy.github.io/2021/06/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%88%B7%E9%A2%98tips/","year":"2021"},{"content":"本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，没有选择自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些相关应用。当然，这本书是一本科普读物，至少在仅有的我熟悉的几章中，书中的介绍还是比较基础的模型（当然这正是现在复杂模型的数学基础）。另外，这是一本让你构建宏观体系的书，它介绍了人工智能运用在工业界的方方面面，让人们对这些问题的解决有一个“道”的思想理解，有趣的是，因为这种宏观的描绘，在读书中会有很多背后蕴藏的深刻理论会吸引你停下读书的脚步，对其进行更深的探索，这正是本书的乐趣所在。\n在此记录读书中的收获和积累，也希望给无暇读书的同行一个十分钟读完本书的可能。\n①统计语言模型 当前词的概率只取决于前面N-1个词，这种假设被称为N-1阶马尔可夫假设，对应的语言模型称为N元模型(N-Gram Model)。N = 2的二元模型就只取决于前一个词，而N = 1的一元模型实际上是一个上下文无关的模型，也就是假定当前词出现的概率与前面的词无关。而在实际中，应用最多的是N=3的三元模型，更高阶的模型就很少使用。\n为什么N取值一般都这么小呢？这里主要有两个原因。首先，N元模型的大小（或者说空间复杂度）几乎是N的指数函数，即O(|V|^N )，这里|V|是一种语言词典的词汇量，一般在几万到几十万个。而使用N元模型的速度（或者说时间复杂度）也几乎是一个指数函数，即O(|V|^(N−1) )。因此，N不能很大。而且当模型从3到4时，效果的提升就不是很显著了，但资源的耗费却增加的非常快，所以，除非是为了做到极致不惜资源，很少有人使用四元以上的模型。Google的罗塞塔翻译系统和语音搜索系统，使用的就是四元模型，该模型存储于500台以上的Google服务器中。\n另外，因为上下文之间的相关性可能跨度非常大，甚至可以从一个段落到另一个段落，所以N无论多大都不能覆盖所有的语言现象。这就需要一些长程的依赖性(Long Distance Dependency)来解决问题了。\n而重新估算概率的估计使用古德-图灵估计(Good-Turing Estimate)，模型的零概率及较小统计值的平滑问题卡茨退避法(Katz Backoff)，一般参数T在8-10之间，频数在T以上的词不用进行古德-图灵估计，同时为了保证总概率为1，所有下调频率总和平均分给未出现的词，内伊(Herman Ney)等人对卡茨退避法进行了一次优化，原理大同小异，参考。PS.二元组的相对频率比三元组更接近概率分布，低阶模型比高阶模型零概率问题轻微，因此用低阶语言模型和高阶语言模型进行线性插值来达到平滑目的，这种方法称为删除差值(Deleted Interpolation)，此方法比卡茨退避法略差，现在已经很少用了。\n需要训练数据和应用数据一致且训练量足够大，所以在语料少的情况下，片面追求高阶的大模型没什么意义。另外，训练语料的噪音高低也会产生影响，因此，一般情况下，对于能找到模式(Pattern)的、量比较大的噪音还是有必要过滤的，比如网页文本中存在的大量制表符。\n②谈谈分词 分词问题属于已经解决的问题，在工业界，只要采用基本的统计语言模型，加上一些业界熟知的技巧就能得到很好的分词结果，提高的空间微乎其微（人工分词也有不同的差异）。另外，在手写体识别中，罗马体系的拼音语言也需要分词方法。\n梁南元教授提出的“查字典”这种最简单的方法可以解决七八成以上的分词问题，1990年前后，郭进博士用统计语言模型成功解决了分词二义性问题，即各种不同的分词方式中，出现概率最高的分词方法。但是，如果穷举所有可能的分词方法并计算出每种可能性下句子的概率，那么计算量是相当大的。因此，可以把它看成是一个（Dynamic Programming）的问题，并利用维特比(Viterbi)算法快速地找到最佳分词。接下来，孙茂松教授解决了没有字典时的分词问题，吴德凯教授最早将中文分词方法用于英文词组的分割，并且将英文词组和中文词组在机器翻译时对应起来。\n词的颗粒度问题，在机器翻译中，颗粒度大效果好，而在搜索中，颗粒度小效果好，因为当用户查询清华时，我们时希望能找到清华大学的。对于上述大小粒度的需求，我们可以构造一个分词器同时支持不同层次的分词\u0026mdash;基于基本词表与复合词表，根据基本词表和复合词表分别建立语言模型\u0026ndash;L1和L2。这就是分词的层次概念。\n分词的不一致性可以分为错误和颗粒度不一致两种，错误又分成两类，一类是越界错误，比如把“北京大学生”分成“北京大学-生”。另一类是覆盖型错误，比如把“贾里克尼”拆成四个字。这些是明显的错误，是改进分词器时要尽可能消除的。接下来是颗粒度的不一致性，人工分词的不一致性大多属于此类。这一类不一致性在衡量分词器的好坏时，可以不作为错误。对于某些应用，需要尽可能地找到各种复合词，而不是将其切分。总之，要继续做数据挖掘，不断完善复合词的词典（它的增长速度较快），这也是近年来中文分词工作的重点。\n③信息的度量和作用 变量的不确定性越大，熵也就越大，所需信息量就越大。一本50万字的中文书平均有多少信息量，常用的前10%的汉字占常用文本的95%以上，那么每个汉字的信息熵约8-9bit，如果考虑上下文，每个汉字的信息熵就只有5bit左右。所以一本50万字的中文书，信息量大约是250万比特，采用较好的算法进行压缩，整本书可以存成一个320kb的文件，而如果直接用两字节的国标编码存储这本书，大约需要1MB大小，是压缩文件的3倍，这两个数量的差距，在信息论中被称作“冗余度”(Redundancy)，需要指出的是，这里的250万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复内容很多，它的信息量就小，冗余度就大。而在不同语言中，汉语是冗余度相对小的。\n一个事物的不确定性U，需要引入信息I来消除，而需要引入的信息量取决于这个不确定性的大小，即I\u0026gt;U才行。在某些时候，不引入新的信息，而在已有的信息上玩数字和公式的游戏，本质上和蒙没有区别。\n说回自然语言的统计模型，其中一元模型就是通过某个词本身的概率分布来消除不确定性，而二元及更高阶的语言模型则还使用了上下文的信息，这些“相关的”信息可以消除不确定性，就像ID3树模型中使用的节点选择依据\u0026ndash;条件熵，也是根据条件熵体现某个特征对目标变量取值的判断是否有消除不确定性的作用。所以说，自然语言处理的大量问题就是寻找相关信息。\n还是使用决策树节点选择作为例子，为了度量一个特征（信息）对目标特征（目标信息）的相关性，或者说消除目标信息不确定性的能力，我们使用互信息，而互信息就是目标特征的不确定性(熵)-条件熵（在已知某特征时目标特征的不确定性），当X和Y完全相关时，它的取值时H（X），同时H（X）=H（Y），当两者完全无关时，它的取值是0。\n信息论中的另一个重要概念，相对熵，又叫交叉熵，是机器学习分类模型中经常使用的损失函数。相对熵也用来度量相关性，但和变量的互信息不同，它用来衡量两个取值为正数的函数的相似性。交叉熵是不对称的，詹森和向明提出了一种新的相对熵的计算方法，将其左右取平均，在Google的自动问答系统中，我们采用了上面的詹森-香农度量来衡量两个答案的相似性。相对熵还可以用来衡量两个常用词（在语义和语法上）在不同文本中的概率分布，看它们是否同义。另外，利用相对熵，还可以得到信息检索中最重要的一个概念：词频率-逆向文档频率(TF-IDF)。\n总的来说，熵、条件熵和相对熵这三个概念与语言模型的关系非常密切。贾里尼克从条件熵和相对熵出发，定义了一个称为语言模型复杂度(Perplexity)的概念来直接衡量语言模型的好坏。复杂度有很清晰的物理含义，它是在给定上下文的条件下，句子中每个位置平均可以选择的单词数量。一个模型的复杂度越小，每个位置的词就越确定，模型越好。\n李开复博士介绍他发明的Sphinx语音识别系统的论文里谈到，如果不用任何语言模型（即零元语言模型），（模型的）复杂度为997，也就是说句子中每个位置有997个可能的单词可以填入。如果（二元）语言模型只考虑前后词的搭配，不考虑搭配的概率，复杂度为60。虽然它比不用语言模型好很多，但与考虑搭配概率的二元语言模型相比要差很多，因为后者的复杂度只有20。\n④贾里尼克和现代语言处理 20世纪70年代的IBM，在其他科学家把语音识别问题当作人工智能和模式匹配问题时，贾里尼克等人在IBM把它当作通信问题，并用两个隐马尔可夫模型（声学模型和语言模型）把语音识别概括得清清楚楚。\n贾里尼克和波尔、库克以及拉维夫的另一大贡献是BCJR算法，这是今天数字通信中应用最广的两个算法之一（另一个是维特比算法）。\n⑤简单之美-布尔代数和搜索引擎 Truth is ever to be found in simplicity, and not in the multiplicity and confusion of things.\n布尔代数对于数学的意义等同于量子力学对于物理学的意义。他们将我们对世界的认知从连续状态扩展到离散状态。\n搜索引擎通过建立索引来在零点零几秒内就找到成千上万甚至上亿的结果。早期的文献检索查询系统，严格要求查询语句复合布尔运算。相比之下，今天的搜索引擎会聪明的自动把用户的查询语句转换成布尔运算的算式，但是基本的原理没有什么不同。\n就像我们理解的ES倒排索引进行搜索的原理一样，如果要找到同时包含“原子能”和“应用”的文章，只要将代表是否有这两个字的二进制数组进行布尔运算AND，就可以得到结果。例如“原子能”的布尔数组为10001，表示第一篇和第五篇文章包含原子能关键字。“应用”的布尔数组为01001，那么同时包含“原子能”和“应用”的文章就是00001，即第五篇文章。\n对于互联网的搜索引擎来说，每一个网页就是一个文献。这个索引是巨大的，在万亿字节这个量级。早期的搜索引擎（比如AltaVista以前的所有搜索引擎），由于受计算机速度和容量的限制，只能对重要、关键的主题词建立索引，但是这样不常见的词和太常见的虚词就找不到了。现在，为了保证对任何搜索都能提供相关的网页，常见的搜索引擎都会对所有的词进行索引。但是，这在工程上却极具挑战性。\n加入互联网上有10^10^个有意义的网页，而词汇表的大小是30万，那么这个索引的大小至少是(10^10^)*30=3000万亿。考虑到大多数词只出现在一部分文本中，压缩为30万亿的量级。为了网页排名方便，索引中还需存有大量附加信息，如这个词出现的位置、次数等。因此，整个索引会通过分布式的方式根据网页的序号将索引分成很多份(Shards)，分别存储在不同服务器中。同时，为了提高效率，需要根据网页的重要性、质量和访问的频率建立常用的非常用等不同级别的索引。常用的索引需要访问速度快，附加信息多，更新也要快；而非常用的要求就低多了。但是不论搜索引擎的索引在工程上如何复杂，原理上依然非常简单，即等价于布尔运算。\n⑥图论和网络爬虫 上文提到搜索引擎，主要由下载、索引和排序组成，布尔代数支撑了索引的逻辑，那么图论中的遍历算法支撑了搜索引擎的下载。\n对于图论相关算法有很多，图的遍历、最短路径、MST、Floyd算法等。而基于图论，还有许多Np-Hard的有趣问题如TSP、VRP问题等。\n假定从一家门户网站的首页出发，先下载这个网站，然后通过分析这个网站，可以找到页面里的所有超链接，也就等于知道了这家门户网站首页所直接链接的全部网页。让计算机不停搜索下去，就能下载整个互联网。在网络爬虫中，人们使用Hash Table来记载哪些网站已经被访问过。现在的互联网非常庞大，一个商业的网络爬虫需要有成千上万个服务器，并且通过高速网络连接起来。如何建立起这样复杂的网络结构，如何协调这些服务器的任务，就是网络设计和程序设计的艺术了。\n网络爬虫对网页遍历的次序不是简单的BFS或DFS，而是有一个相对复杂的下载优先级排序方法，由调度系统(Scheduler)管理。在调度系统里需要存储那些已经发现但尚未下载的URL，它们一般存在一个优先级队列(Priority Queue)，这在工程上与BFS更相似，因此，爬虫中BFS的成分多一些。\n对于页面的分析，过去由HTML书写的网站很容易提取，但是现在很多网页是由一些脚本语言(如JavaScript)生成。打开网页的源代码，URL不是直接可见的文本，而是运行这一段脚本才能得到的结果。因此网络爬虫需要模拟运行这一段脚本。另外，有些脚本写的非常不规范，以至于解析起来非常困难（浏览器可以解析），这就需要解析人员具有非常强大的浏览器内核工程能力。因此，如果一些网页明明存在，但搜索引擎没有收录，一个原因可能是网络爬虫中的解析程序没能成功解析网页中不规范的脚本程序，又或者这个网页搞了反爬（就像GitHub.io的博客）。\n上面提到过，Hash Table被用来记录哪些网站已经被访问过，Hash Table判断一个网页是否在表中只需要1个复杂度，写入某新网站也只需要Hash运算。但在一个成千上万个服务器的爬虫工程中，存储并实时维护一张哈希表就是一个令人头疼的事了。为了各个服务器不做重复的工作，它们需要在下载前和下载后实时访问哈希表，这就造成了通信瓶颈。对于这个问题有两个技术：首先明确各个服务器的分工，也就是一看到某个URL就确定要某一台服务器去做（例如结尾数字为1的URL都要服务器1去下载），这样就可以避免重复下载，在明确分工的基础上，判断URL就可以批处理了，比如每次向Hash Table发送一大批询问，或者每次更新一大批Hash Table的内容。这样通信的次数就大大减少了。\n书中没有提到具体的Hash Table存储的解决方案，个人理解可能参考参数服务器的分布式存储构架进行存储。\n⑦搜索引擎的排序-网页质量度量与相关性度量 搜索引擎中，根据索引返回的网页数有成千上万条，那么该如何排序，把用户最想看到的结果排在前面呢。总的来讲，排名取决于两个信息：网页的质量信息(Quality)，以及这个查询与每个网页的相关性信息(Relevance)。\n网页质量的衡量 最初，一些衡量网络质量的方法或多或少地用到了指向某个网页的链接以及链接上的文本（在搜索技术中成为锚文本，Anchor Text）。真正找到计算网页自身质量的完美的数学模型的是Google创始人拉里·佩奇和谢尔盖·布林。PageRank的高明之处在于把整个互联网当作一个整体来对待，这无意中符合了系统论的观点。核心思想是，如果一个网页被很多其他网页链接，说明它受到普遍的承认和信赖，那么它的排名就高。另外，PageRank也考虑了网页排名高的网站贡献的链接权重大这个因素。现在就出现了一个问题，计算搜索结果的网页排名过程中需要用到网页本身的排名，这不就是\u0026quot;先有鸡还是先有蛋\u0026quot;吗？破解这个怪圈的是布林，他把这个问题变成了一个 二维矩阵相乘的问题，并用迭代的方法解决了这个问题，他们先假定所有网页的排名是相同的，然后根据这个初始值，算出各个网页的第一次迭代排名，然后再根据第一次迭代排名算出第二次迭代排名。而且他们从理论上证明了，不论初始值如何选取，这种算法都能保证网页排名的估计值能收敛到排名的真实值，值得一提的是，这种算法不需要任何人工干预。理论的问题解决了，数量庞大的网页有带来了实际问题，十亿个网页时，这个二维矩阵就是一百亿亿个元素。这么大的矩阵相乘，计算量是非常大的。佩奇和布林利用稀疏矩阵的计算技巧，大大简化了计算量，并实现了这个网页排名算法。\nGoogle早期，并行化是半自动的，这样更新一遍所有网页的周期很长。2003年，Google的工程师迪恩(Jeffrey Dean)和格麦瓦特(Sanjay Ghemawat)发明了二MapReduce（矩阵相乘很容易分解成许多小任务），PageRank的并行计算完全自动化了。\nPageRank的计算方法，结果向量B，为一个$1 \\times N$的向量，初始时为$B_0=(\\cfrac{1}{N},\\cfrac{1}{N},…,\\cfrac{1}{N})$，A为$N \\times N$的矩阵，其中值代表第n个网页指向第m个网页的链接数。通过公式$B_i = A \\cdot B_{i-1}$进行迭代，可以证明$B_i$最终会收敛至$B=B \\cdot A$，一般来讲，只要10次左右的迭代基本上就收敛了，即两次迭代的结果$B_i$和$B_{i-1}$之间的差异非常小。\n由于网页之间链接的数量相比互联网的规模非常稀疏，因此需要对零概率或者小概率事件进行平滑处理。\n$B_i = [\\cfrac{α}{N} \\cdot I + (1-α)A] \\cdot B_{i-1}$\n其中N是互联网网页的数量，α是一个（较小的）常数，I是单位矩阵\n网页与查询的相关性度量 2007年，技术和算法的重要性依然高于数据，因此确定相关性主要依靠算法。但是今天，由于商业搜索引擎已经有了大量的用户点击数据，因此，对搜索相关性贡献最大的是根据用户对常见搜索点击网页的结果得到的概率模型。如今，影响搜索引擎质量的诸多因素，除了用户的点击数据之外，都可以归纳成下面四大因素：\n1.完备的索引。俗话说，巧妇难为无米之炊，如果一个网页不在索引中，那么再好的算法也找不到。\n2.对网页质量的度量。当然现在看来，PageRank的作用比10年前已经小了很多。今天，对网页质量的衡量是全方位的，比如对网页内容权威性的衡量，一些八卦网站的PageRank可能很高，但是他们的内容权威性则很低。\n3.用户偏好。一个好的搜索引擎会针对不同用户，对相同的搜索给出不同的排名。\n4.确定一个网页和某个查询的相关性的方法。\nTF-IDF TF（Term Frequency 单文本词频），词频，关键词的出现次数/总字数。其中停止词，如“的”，“是”，“中”，的权重为0。对于一组关键词，可以得到$TF_1$,$TF_2$,\u0026hellip;,$TF_n$\nIDF（Inverse Document Frequency 逆文本频率指数），对于预测主题能力更强的词，权重越大，这个权重的衡量就是IDF，$log(\\cfrac{D}{D_w})$，其中D为全部文本数，$D_w$为关键词w在$D_w$个文本中出现。即如果一个关键词只在很少的网页中出现，通过它就容易锁定搜索目标，它的权重也就应该大。其实，所谓IDF的概念就是一个特定条件下关键词的概率分布的交叉熵(KL散度)，这样，关于信息检索相关性的度量，又回到了信息论。\n现在，各家搜索引擎对关键词重要性的度量，都在TF-IDF的基础上做了一定的改进和微调，但是，原理上与TF-IDF相差不远。\n⑧搜索引擎反作弊问题和搜索结果的权威性问题 搜索引擎反作弊 搜索引擎中排名靠前的网页不一定就是高质量的、相关的网页，而是商业味儿非常浓的作弊网页，它们采用不正当的手段(SPAM)提高自己网页的排名。\n早期的作弊方法是重复关键词，有时为了不让读者看到过多讨厌的关键词，聪明一点的作弊者常用很小的字体或与背景相同的颜色来掩盖这些关键词，其实这种方法很容易被搜索引擎发现并纠正。有了PageRank后，就有了专门买卖链接的生意，有人会创建成百上千的网站用于链接客户的网页。但是大量地卖链接很容易露出马脚（流通量大后很容易找到源头）。当然，还会有各种各样其他的作弊手段，抓作弊是一种长期的“猫捉老鼠”的游戏。\n作弊者所做的事情，就像是在手机信号中加入噪声，这种人为加入的噪声并不难消除，因为这些作弊方法不是随机的（否则就无法提高排名了），因此，可以在搜集一段时间的作弊信息后，将作弊者抓出来。即针对这些商业相关的搜索，采用一套“刚干扰”强的搜索算法。\n首先，那些卖链接的网站，都有大量的出链(Out Links)，而这些出链的余弦距离如果接近1，则说明这些网站间并无关系！那么很有可能这是一个卖链接的网站。其次，反作弊用到的另一个工具是图论。在图中，如果有几个节点两两互相都连接在一起，它们被称为一个Clique。作弊网站一般需要互相链接以提高自己的排名，图论中有专门发现Clique的方法，可以直接用于找到这些作弊网站的Clique。另外，至于术的方面，方法也很多，例如针对作弊的JavaScript跳转页面，通过解析相应JavaScript内容即可。\n作弊的本质是在网页排名信号中加入了噪声，因此反作弊的关键是去噪声。沿着这个思路可以从根本上提高搜索算法抗作弊的能力，事半功倍，而如果只是根据作弊的具体特征头痛医头，脚痛医脚，则很容易被作弊者牵着鼻子走。\n搜索结果的权威性 当用户问的是一些需要专业人士认真作答的的问题，比如医疗方面的问题，那么如何才能从众多信息源中找到最权威的信息，就成了近年来搜索引擎公司面对的难题。这些网页虽然权威性不高，但是文章常常写的很好看，名气也很大，PageRank也很高，但是它们的内容未必权威。其次，互联网上对同一个问题给出的答案常常互相矛盾。比如奥巴马的出生地，竟然有近百个答案，他的一些政敌说他生于肯尼亚，而官方给出的是夏威夷，虽然大家都知道政敌说的话未必可信，但是互联网又怎么知道谁是政敌呢？\n这就会引出“权威性的度量”，为了引入这一点，我们引入一个概念“提及”(Mention)，如果在各种新闻、学术论文或者其他网络信息页中，讨论到“吸烟危害”这一主题时，国际卫生组织和约翰·霍普金斯大学这两个组织作为信息源被多次提及，那么我们就有理由相信这两个组织是谈论“吸烟危害”这个主题的权威机构。需要指出的是，“提及”不像是超链接那样一目了然，它隐含在文章的自然语句中，需要通过自然语言处理的方式分析出来，即使有了好的算法，计算量也是非常大的。并且权威性与搜索主题是相关的，毕竟每个组织都只在自己的领域有权威性，使得存储量非常大，比如M个网页，N个搜索关键词时，我们要计算和存储$O(M \\cdot N)$个结果，而一般的网页只需要M个结果。因此，只有在今天有了云计算和大数据技术的情况下，计算权威性才成为可能。\n在计算权威度时，我们采用了句法分析、互信息和短语（词组）聚类这三种方法。\n计算权威度的步骤：\n1.对网站的文字进行句法分析，找到涉及主题的短语，以及对信息源的描述，这样大量的运算得益于皮耶尔领导开发的Google句法分析器足够快，而且有大量的服务器可供使用。\n2.利用互信息，找到主题短语和信息源的相关性，即出现主题短语时更容易出现某些信息源。\n3.对主题短语进行聚合，聚合那些字面上不同，但意义相同的主题短语，如“吸烟的危害”、“吸烟是否致癌”等等。这样我们就得到了一些搜索的主题。至于聚类的方法，可以采用前面提过的矩阵运算的方法。\n4.对一个网站中的网页聚合，比如把一个网站下面的网页按照子域(Subdomain)或者子目录(Subdirectory)进行聚类。这一步的目的是，即使一个权威的网站，它下面的一些子域却未必具有权威性。比如约翰·霍普金斯大学的网站，它下面可能有很多子域的内容与医学无关。因此，权威性的度量只能建立在子域或者子目录这一级。\n完成上述四个步骤后，我们就可以得到一个针对不同主题，哪些信息源(网站)具有权威性的关联矩阵。当然，在计算这个关联矩阵时，也可以像计算PageRank那样，对权威度高的网站给出“提及”关系更高的权重，并且通过迭代算法，得到收敛后的权威度关联矩阵。这样便可以在搜索结果中提升那些权威度高的信息源的结果，使得用户对搜索结果更放心。\n⑨有限状态机和动态规划-地图与本地搜索的核心技术 2008年9月23日，Google、T-mobile和HTC宣布了第一款基于开源操作系统Android的3G智能手机，它可以利用全球卫星定位系统实现的全球导航功能。此时这个系统可以媲美任何一个卫星导航仪，加上他的地址识别技术（采用有限状态机）比卫星导航仪严格的地址匹配技术（不能输错一个字母）要好得多。智能手机的定位和导航功能，其实只有三项关键技术：第一，利用卫星定位，这一点传统的导航仪都能做到；第二，地址的识别；第三，根据用户输入的起点和终点，在地图上规划最短线路。\n地址分析和有限状态机 地址的写法各种各样，且有比较复杂的上下有关的文法，例如 上海市北京东路XX号，南京市北京东路XX号，当识别器扫描到“北京东路”时，它和后面的门牌号能否构成一个正确的地址，取决于上下文，即城市名。在统计语言模型中有介绍，上下文有关的文法分析非常复杂又耗时，如果没有好的模型，这个分析器写出来很难看不说，很多情况无法覆盖。所幸的是，地址的文法是上下文有关文法中相对简单的一种，因此有许多识别和分析的方法，其中最有效的是有限状态机。有限状态机还能帮助google对用户输入的查询进行分析，挑出其中描述地址的部分，当然，剩下的关键词就是用户要找的内容。\n如果一个地址能从状态机的开始状态进过状态机的若干中间状态，走到终止状态，那么这条地址有效，否则无效。\n使用有限状态机识别地址，关键要解决两个问题，首先，要通过给定地址建立状态机，然后，给定这个有限状态机后，地址字串的匹配算法。好在这两个问题都有现成的算法。\n在实际情况中，用户输入地址不标准或有错别字时，有限状态机因为只能进行严格匹配，所以无法识别。其实，有限状态机在计算机科学早期的成功主要用于程序语言编译器的设计中。为了实现可以模糊匹配，基于概率的有限状态机被提出，它可以给出一个字串为地址的可能性，原理与离散的马尔科夫链基本等效。\n值得一提的是，有限状态机的用途远不止于对地址这样的状态序列进行分析，在Google新一代的产品中，有限状态机被应用在Google Now，一个在智能手机上的基于个人信息的服务软件。他会根据个人的地理位置信息、日历和一些其他信息（对应于有限状态机里面的状态），以及用户当前语音或者文字输入，回答个人的问题，提供用户查找的信息，或者提供相应服务（如打开地图导航、拨打电话等）。Google Now的引擎和AT\u0026amp;T的有限状态机工具库从功能上讲完全等价。\n有一些领域使用一种特殊的有限状态机——加权的有限状态机(Weighted Finite State Transducer, WFST)，有限状态传感器的特殊性在于，每一个状态由输入和输出符号定义，任何一个词的二元组，都可以对应到WFST的一个状态，WFST是天然的自然语言处理的分析和解码工具。在语音识别中，每个句子都可以用一个WFST表示，WFST中的每一条路径就是一个备选句子，其中概率最大的那条路径就是这个句子的识别结果。而这个算法的原理是动态规划。\n全球导航和动态规划 所有的导航系统都采用了动态规划(Dynamic Programming, DP)的办法，关于动态规划的思想及实现相信在数据结构及算法中已经讲解的很深刻，我在这里根据我在路径规划的研究和工作实践，补充一些其他经验。\n路径规划问题的精确解，都会采用动态规划。而在一些节点数庞大，又对完全精确解需求不高的情况下，解决这种NP-hard问题运筹学中的启发式算法，近年来有很多强化学习被应用在路径规划中，但是由于强化学习的加入带来了时间成本的增加，算法整体效率并没有太大提升。\n⑩余弦定理和新闻分类 所谓新闻的分类，或者更广义地讲文本的分类，无非是要把相似的新闻归入同一类中。这就要求我们先把文字的新闻变成一组可计算的数字，然后再设计一个算法来算出任意两篇新闻的相似性。\n一个新闻可以用一个向量表示，根据TF-IDF，对其中的每一个词在词典中进行标识，例如一个65535个词的词典，那么就会形成一个65535维的向量，称为新闻的特征向量(Feature Vector)。和计算搜索相关性一样，出现在文本不同位置的词再分类时的重要性也不相同，一般，标题\u0026gt;正文开头结尾\u0026gt;正文中间。可以对标题和重要位置的词进行额外加权，以提高文本分类的准确性。\n现在我们有了向量来代表一个新闻的内容，可以发现，同一类新闻用词一定是相似的。因此，可以通过计算两个向量的夹角来判断对应的新闻主题的接近程度，即余弦相似度。\n$cosθ = \\cfrac{\u0026lt;b,c\u0026gt;}{|b| \\cdot |c|} = \\cfrac{x_1y_1+x_2y_2+\u0026hellip;+x_{65535}y_{65535}}{\\sqrt{x_1^2+x_2^2+\u0026hellip;+x_{65535}^2} \\cdot \\sqrt{y_1^2+y_2^2+\u0026hellip;+y_{65535}^2}}$\n在工程中，为了简化余弦相似度的计算复杂度$O(N^2·|a|)$，有以下三种方向，首先，分母部分（向量的长度）不需要重复计算，计算向量a和向量b的余弦时，可以先将它们的长度存起来，等计算向量a和向量c的余弦时，直接取用a的长度即可；其次，在计算分子，即两个向量内积时，只需考虑向量中的非零元素，如果一篇新闻有2000个词，那么非零元素一般只有1000个，这样计算的复杂度可以下降1000/词典总词数，计算时间直接从“天”下降到十几分钟这个量级；第三，可以删除虚词，进一步减少非零元素个数，另外删除虚词，不仅可以提高计算速度，对新闻分类的准确性也大有好处，因为虚词的权重其实是一种噪声。经过这些操作，10万篇新闻两两比较，计算时间也就几分钟而已，如果做几十次迭代，可以在一天内计算完。\n具体的分类算法分为两种情形，第一种，假定我们已经有了已知每一类的特征向量，新的新闻特征向量就可以通过计算它与各类新闻特征向量的余弦相似度，来决定将它划分到哪一类中去。第二种，当我们是类似层次聚类，从第向上，一步一步合并直至类别越累越少，而每个类越来越大，当某一类太大时，这一类里一些新闻之间的相似性就很小了，这是就要停止迭代，这就是自动分类的方法。\n本章介绍的这种新闻归类的方法，准确性好，适用于被分类的文本集合再百万数量级。如果大到亿这个数量级，那么计算时间还是比较长的。对于更大规模的文本处理，将在下一章介绍一种更快速但相对粗糙的方法。\n⑪矩阵运算和文本处理中的两个分类问题 自然语言处理中，最常见的两个分类问题分别是，将文本按主题归类（比如新闻分类）和将词汇表中的字词按意思归类（比如将各种运动的项目名称都归到体育类）。这两个分类问题都可以通过矩阵运算来圆满地、一次性地解决。\n上一章中的文本分类其实是一个聚类问题，关键是计算两篇新闻（向量）的相似度。运用余弦相似度时需要两两计算并且多次迭代，虽然算法漂亮但耗时长。我们希望有一个办法，一次就能把所有新闻相关性计算出来。这个一步到位的方法利用的是矩阵运算中的奇异值分解(Singular Value Decomposition, SVD)。\n$M = UΣV$ $$ M=\\left[ \\begin{matrix} a_{11} \u0026amp; \\cdots \u0026amp; a_{1j} \u0026amp; \\cdots \u0026amp; a_{1N} \\ \\cdots\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\cdots \\a_{i1} \u0026amp; \\cdots\u0026amp;a_{ij} \u0026amp; \\cdots \u0026amp; a_{iN} \\ \\cdots\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\cdots\\ a_{M1} \u0026amp; \\cdots\u0026amp;a_{Mj} \u0026amp; \\cdots \u0026amp; a_{MN} \\ \\end{matrix} \\right] $$ 其中，每一行对应一篇文章，每一列对应一个词，如果有N个词，M篇文章，则得到一个$M \\times N$的矩阵，第i行、第j列的元素$a_{ij}$，是字典中第j个词在第i篇文章中出现的加权词频(比如TF-IDF值)。\n奇异值分解，就是把M，分解成三个小矩阵相乘。这三个矩阵有非常清晰的物理含义。\n由于对角矩阵B对角线上的元素的很多值相对其他的值非常小，或者干脆为0，故可以省略。\n$U$的每一行表示一个词，每一列表示一个语义相近的词类，或者称为语义类，数值越大越相关。\n$V$是对文本的分类结果，它的每一列对应一篇文本，每一行对应一个主题。这一列中的每个元素表示这篇文本在不同主题中的相关性。\n$Σ$表示词的类和文章的类之间的相关性。\n因此只要完成对关联矩阵$M$进行一次奇异值分解，就可以同时完成近义词分类和文章的分类。另外，还能得到每个主题和每个词的语义类之间的相关性。这个结果非常漂亮！\n奇异值分解一般分为两步。首先，将矩阵A变成一个双对角矩阵，这个过程的计算量是$O(MN^2)$，当然这里假设M\u0026gt;N。我们仍然可以利用矩阵$A$的稀疏性大大缩短计算时间。第二步是将双对角矩阵变成奇异值分解的三个矩阵，这一步的计算量只是第一步的零头，可以忽略不计。在文本分类中，$M$对应文本的数量，$N$对应词典大小。奇异值分解的计算复杂度与余弦定理一次迭代的时间复杂度处于一个量级，但是它不需要多次迭代。奇异值分解的另一个大问题是存储量较大，因为整个矩阵都需要存在内存里，而利用余弦定理的聚类则不需要。实际应用中，可以先进行奇异值分解，得到粗分类结果，再利用计算余弦向量的方法，在粗分类的基础上，进行几次迭代，得到比较精确的结果。\n理论的问题解决了，下面就是工程的实际问题，怎么利用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值，以及数值分析的各种算法等就统统派上用场了。对于不大的矩阵，比如几万乘几万的矩阵，用计算机上的数学工具MATLAB就可以计算。但是更大的矩阵，比如上百万乘上百万，奇异值分解的计算量非常大。虽然Google早就有了MapReduce等并行计算工具，但是由于奇异值分解很难拆成不相关的子运算，即使在Google内部以前也无法利用并行计算的优势来进行矩阵分解。直到2007年，Google中国（谷歌）的张智威博士带领几个中国的工程师及实习生实现了奇异值分解的并行算法，这是Google中国对世界的一个贡献。\n⑫信息指纹及其应用 所谓信息指纹，可以简单理解为一段信息（文字、图片、音频、视频等）随机地映射到一个多维二进制空间中的一个点（一个二进制数字）。只要这个随机函数做得足够好，那么不同信息对应的这些点就不会重合，因此，这些二进制的数字就成了原来的信息所具有的独一无二的指纹。\n字符串的信息指纹的计算方法一般分为两步，首先，把这个字符串看成是一个特殊、很长的整数，这一步非常容易，因为在计算机里本来就是这样存储的。接下来就需要用到一个产生信息指纹的关键算法：伪随机数产生器算法（Pseudo-Random Number Generator, PRNG），通过它把任意很长的整数转换成定长的伪随机数，现在常用的算法是梅森旋转算法(Mersenne Twister)。\n另外，信息指纹的不可逆性为网络加密传输提供了很好的支持。一些网站会将用户访问的cookie（本来已经加密，但是如果某一访问所有网站的cookie都一样还是会暴漏行为）通过HTTPS加密。加密的可靠性，取决于是否很难人为地找到具有某一指纹的信息，即解密。从加密的角度来讲，梅森旋转算法还不够好，因为它产生的随机数还有一定的相关性，破解一个就等于破解了一大批。\n在互联网上加密要使用基于加密的伪随机数产生器(Cryptographically Secure Pseudo-Random Number Generator, GSPRNG)。常用的算法有MD5或SHA-1等标准，它们可以将不定长的信息变成定长的128位或160位的二进制随机数。\n除了上述信息指纹可以帮助快速且高效得判断文本相同和互联网加密外，信息指纹在互联网和自然语言处理中还有很多应用。\n集合相同的判定 普通的先将两个集合排序，再进行比较的算法时间复杂度为$O(NlogN)$。或者将第一个表放在哈希表里，再用第二个表比对，这个方法可以达到最佳的$O(N)$时间复杂度，但是额外使用了$O(N)$空间，而且代码复杂。完美的办法是计算这两个集合中元素的指纹和，然后进行比较，加法的交换率，保证了集合的指纹不因元素次序变化而变化。类似如检测网络上某首歌曲是否盗版别人，只需要计算这两个音频的信息指纹即可。\n判定基本相同 判定两个邮箱群发的接收电子邮件地址清单(Email Address List)是否相同，可以用来判断是否群发垃圾邮件。但是如果其中有一点小小的不同，上述信息指纹甚至会产生很大的不同。这个时候可以按照规则，挑选几组电子邮箱地址集，例如尾数为24的地址集与尾数为34的地址集，由于挑选的地址集数量一般为个位数，因此可以得到80%，90%重复这样的结果。又或者对网页中的，刨除常见词如“的”，“是”与只出现一次的噪声词，只需要找出剩下的词中IDF值最大的几个词，并且算出它们的信息指纹即可，为了允许有一定的容错能力，Google采用了一种特定的信息指纹——相似哈希(SimHash)。\n假定一个网页中有若干词$t_1,t_2,\u0026hellip;,t_k$，它们的权重（如TF-IDF）为$w_1,w_2,\u0026hellip;,w_k$。先计算出这些词的信息指纹（假定有8位）。第一步扩展，如$t_1$的信息指纹为10100110，那么对$t_1$的权重$w_1$，逢1相加，逢0相减。这样在k次加减$w$之后，可以获得8个实数值。第二步收缩，这些实数值正数变1，负数变0，就可以重新获得一个8位的二进制数，这个数就是这个网站的SimHash。相似哈希的特点是，相同网页或只有小权重词不同网页的相似哈希必定相同，如果两个网页的相似哈希不同但相差很小，则对应的网页也非常相似。工程上使用的64位相似哈希，如果只差一两位，那么对应网页内容重复的可能性大于80%。\n⑬密码学的数学原理 加密函数不应该通过几个自变量和函数值就能推出函数本身，这样一来，破译一篇密文就可能破译以后全部的密文。\n香农提出的信息论为密码学的发展提供了理论基础，根据信息论，密码的最高境界是敌方在解惑密码后，信息量没有增加。一般来讲，当密码之间均匀分布且统计独立时，提供的信息量最少。均匀分布使地方无从统计，而统计独立可保证敌人即使知道了加密算法，并且看到了一段密码和明码后，无法破解另一段密码。\nDiffie和Hellman在1976年的开创性论文“密码学的新方向”($\\it{New \\ Directions \\ in \\ Cryptography}$)，介绍了公钥和电子签名的方法，这是今天大多数互联网安全协议的基础。\n虽然公开密钥下面有许多不同的具体加密方法，比如早期的RSA算法、Rabin算法、和后来的ElGamal算法、椭圆曲线算法(Elliptic curve)，它们的原理基本一致，并不复杂：\n1.它们都有完全不同的密钥，一个用于加密，一个用于解密\n2.这两个看上去无关的密钥，在数学上是关联的\n以下是一个公开密钥的原理：\n公开密钥的好处：\n1.简单，就是一些乘除而已。\n2.可靠，公开密钥方法保证产生的密文是统计独立而分布均匀的。也就是说，不论给出多少份明文与密文的对应，也无法破解下一份密文。更重要的是N、E可以公开给任何人进行加密，但是只有掌握私钥D的人才可以解密，即使加密者也是无法破解的。\n3.灵活，可以产生很多的公开密钥E和私钥D的组合给不同的加密者。\n最后让我们看看破解这种密码的难度。至今的研究结果表明最彻底的方法还是对大数N进行因数分解，即通过N反过来找到P和Q，这样密码就被破解了。而找到P和Q目前只有一个方法：用计算机把所有可能的数字试一遍，这实际上实在拼计算机的速度，这也就是为什么P和Q都需要非常大。一种加密方法只要保证50年内计算机破解不了，也就算是令人满意了。\n遗憾的是，虽然在原理上非常可靠，但是很多加密系统在工程实现上却留下了不少漏洞。因此，很多攻击者从攻击算法转而攻击实现方法。\n⑭数学模型的重要性 1.一个正确的数学模型应当在形式上是简单的\n2.一个正确的模型一开始可能还不如一个精雕细琢过的错误模型来的准确，但是，如果我们认定大方向是对的，就应该坚持下去\n3.大量准确的数据对研发很重要\n4.正确的模型也可能受噪声干扰，而显得不准确；这时不应该用一种凑合的修正方法加以弥补，而是要找到噪声的根源，这也许能通往重大的发现\n⑮最大熵模型 当需要综合几十甚至上百种不同的信息，最大熵模型可以保留全部的不确定性（除了已知的信息，不做主管假设），将风险降到最小。最初，计算能力还不足时，只能处理计算量相对不太大的自然语言处理问题，拉纳帕体成功将上下文信息、词性（名词、动词和形容词）以及主谓宾等句子成分，通过最大熵模型结合起来，做出了当时世界上最好的词性标识系统和句法分析器。且至今仍然是使用单一方法的系统中效果最好的。在2000年前后，计算机速度的提升以及训练算法的改进，很多复杂的问题，包括句法分析、语言模型和机器翻译都可以采用最大熵模型了。最大熵模型和一些简单组合了特征的模型相比，效果可以提升几个百分点，对于投资这种很少的提升也能带来巨大的利润的行业来说非常有效，所以很多对冲基金开始使用最大熵模型。\n$P(d|x_1,x_2,\u0026hellip;,x_{20}) = \\cfrac{1}{Z(x_1,x_2,\u0026hellip;,x_{20})} e^{λ_1(x_1,d)+λ_2(x_2,d)+\u0026hellip;+λ_{20}(x_{20},d)}$\n其中，归一化因子为：\n$Z(x_1,x_2,\u0026hellip;,x_{20}) = \\sum\\limits_{d}e^{λ_1(x_1,d)+λ_2(x_2,d)+\u0026hellip;+λ_{20}(x_{20},d)}$\n在实现方法上，从GIS(Generalized Iterative Scaling)到IIS(Improved Iterative Scaling)缩短了一到两个数量级，再到吴军找到的数学变换在IIS的基础上减少两个数量级，再加上现代的MapReduce并行计算，现在，再1000台计算机上并行计算一天就可以完成了。\n逻辑回归损失函数的基于伯努利分布的最大似然法，与交叉熵损失函数本质相同（经过log变换后公式相同），可见信息论的思想，变换无穷但又不离本质。\n⑯拼音输入法的数学原理 输入汉字的快慢取决于汉字编码的平均长度诚意寻找这个键所需的时间。单纯地编短编码长度未必能提高输入速度，因为寻找一个键的时间可能变得很长。\n对汉字的编码分为两部分：对拼音的编码和消除歧义性的编码。早期的输入法常常只注重第一部分而忽视第二部分，中国最早的汉字输入微机中华学习机和长城0520采用的都是双拼，即每个字母代表一个或多个声母或韵母，这种输入方法看似节省了编码长度，但是输入一点也不快，一是因为增加了歧义性（很多韵母不得不共享一个字母键），二是双拼比按照拼音的全拼增加了拆字的过程，三是容错性不高，比如an,ang的编码完全没有相似性，一个好的输入法不能要求用户读准每个字的发音。早期的输入法都是这样，一味的追求更少的输入的编码（当然它们都声称自己的编码比其他的输入法更合理，但从信息论的角度上看都是一个水平线上的），而忽视了找到每个键所用的时间，要求普通用户背下这些输入方法里所有汉字的编码是不现实的。\n最终，用户还是选择了拼音输入法。它不需要专门学习，并且输入自然，即找每个键的时间很短，另外因为编码长，有信息冗余，容错性好。于是，拼音输入法要解决的问题只剩下排除一音多字的歧义性了，这也是目前各种拼音输入法的主要工作。接下来我们就分析平均输入一个汉字可以做到最少几次击键。\n按照香农第一定理，对于一个信息，任何编码的长度都不小于它的信息熵。以GB2312，6700个常见字集计算，汉字的信息熵在10个比特以内，如果假定输入法只能用26个字母，那么每个字母可以代表log26≈4.7bit（$2^{4.7}≈26$），也就是输入一个汉字平均需要敲10/4.7≈2.1次键盘。而如果把汉字组成词，再以词为单位统计信息熵，不考虑上下文相关性，那么每个汉字的平均信息熵就会减少为8bit，这也是现在所有输入法都基于词输入的根本原因。如果再考虑上下文相关，可以将每个汉字的信息熵降到6bit，这是输入一个汉字只需要敲6/4.7≈1.3次键盘。但事实上没有一种输入法能接近这个效率，有两个原因，首先，需要根据词频进行特殊编码，而上一段说过特殊编码会增加找到每个键的思考时间，其次上，在个人电脑上，很难安装非常大的语言模型。\n事实上，现在汉语全拼的平均长度为2.98，如果能利用上下文解决一音多字的问题，全拼输入法的平均击键次数应该小于3次。而解决上下文问题，10年前的拼音输入法（以紫光为代表）解决这个问题的办法是建立大词库，但是这种方法多少根据经验和直觉，一直是在打补丁。而利用上下文最好的方法是借助语言模型，目前，各家输入法（Google、腾讯和搜狗）基本处在同一个量级，将来技术上进一步提升的关键在于看谁能准确而有效地建立语言模型。\n拼音转汉字的算法和在导航中寻找最短路径的算法相同，都是动态规划。\n每一条路，都是一个可选结果，找到概率最高的一条就是最优的句子：\n$w_!,w_2,\u0026hellip;,w_N = \\mathop{ArgMax}\\limits_{w∈W}P(W_1,W_2,\u0026hellip;,w_n|y_1,y_2,\u0026hellip;,y_N)$\n上述公式的简化方法：\n$w_!,w_2,\u0026hellip;,w_N = \\mathop{ArgMax}\\limits_{w∈W}P(y_1,y_2,\u0026hellip;,y_N|W_1,W_2,\u0026hellip;,w_n)\\ \\cdot \\ P(W_1,W_2,\u0026hellip;,w_n)≈ \\mathop{ArgMax}\\limits_{w∈W}\\prod\\limits_{i=1}^{N}P(w_i|w_{i-1})·P(y_i|w_i)$\n如果对上述简化公式中的概率取对数同时取反，即定义$d(W_{i-1},W_i)=-logP(w_i|w_{i-1})·P(y_i|w_i)$，上面的连乘关系，就变成了加法关系，寻找最大概率的问题，就变成了最短路径问题。\n当输入速度超过一定阈值后，用户的体验可能更重要，客户端上虽然不能放置太大的语言模型，但是可以建立个性化的语言模型。那么就有两个问题：如何训练一个个性化的语言模型，其次是怎样处理好它和通用语言模型的关系。为某个人训练一个特定的词汇量在几万的二元语言模型需要几千万的语料，这对一个人来说可能一辈子也达不到，没有足够多的训练数据，高阶的语言模型根本没用，一些输入法就找到了一种经验做法：用户词典，这实际上是一个小规模的一元模型加上非常小量的元组（比如一个用户定义的词ABC，实际是一个三元组）。\n更好的办法是找到大量符合用户经常输入的内容和用语习惯的语料，训练一个用户特定的语言模型。这就要用到余弦定理和文本分类的技术了，即找到某个人输入文本的特征向量Y和已有文本$X_1,X_2,\u0026hellip;,X_{1000}$，选择前K个和Y距离最近的类对应的文本，作为这个特定用户语言模型的训练数据。\n大部分情况下，$M_1$对这个特定用户的输入比通用模型$M_0$好，但是对于相对冷僻的内容，$M_1$的效果就远不如$M_0$了，所以我们需要综合两个模型。把各种特征综合在一起的最好方法是采用最大熵模型，当然，如果为每个人都建立这样一个模型，成本较高。因此可以采用一个简化的模型：线性插值模型，能得到最大熵模型约80%的收益，Google拼音输入法的个性化语言模型就是这么实现的。\n⑰布隆过滤器 判断一个元素是否在一个集合中，按12章信息指纹所描述的，可以通过8个字节的信息指纹哈希表，但由于哈希表的存储效率一般只有50%，因此一个元素需要占用16个字节，一亿个地址大约要1.6GB，即16亿字节的内容，那么存储几十亿个邮件地址需要上百GB，除非是超级计算机，一般服务器是存不下的。\n而我们现在介绍的一种称作布隆过滤器的数学工具，它只需要哈希表$\\cfrac{1}{8}$到$\\cfrac{1}{4}$的大小就能解决同样的问题。布隆过滤器(Bloom Filter)是由伯顿·布隆(Burton Bloom)于1970年提出的，它实际上是一个很长的二进制向量和一系列随机映射函数。\n假定存储一个亿个电子邮件，那么我们使用16亿个比特位，每个比特位初始值为0。对于每个电子邮件地址X，用8个不同的随机数产生器（$F_1,F_2,\u0026hellip;,F_8$）产生8个信息指纹，再用一个随机数产生器G把这8个信息指纹映射到1-16亿中的8个自然数$g_1,g_2,\u0026hellip;,g_8$。然后把这8个位置的比特位全部设置为1。在把所有邮件都处理后，一个针对这些电子邮件的布隆过滤器就建成了。当有一个新的电子邮件地址Y时，就将Y重复上述的处理结果，然后检查这8个位置是否都是1就行了。当然根据这个逻辑，布隆过滤器决不会漏掉任何一个可以地址，但是，当某个好的电子邮件对应的8个位置都恰好被设置成1时，就会被误判为黑地址。好在这种可能性很小，对于一个元素的集合，这种误识别率在万分之一以下。常见的补救方法是建立一个小的白名单，存储那些可能被误判的地址。\n误判率的计算，先计算某个比特被设为1个概率，根据伯努利分布，$1-(1-\\cfrac{1}{m})^{kn}$。而对于一个新的不在集合中的元素，它的k个bit每个都命中的概率为$(1-(1-\\cfrac{1}{m})^{kn})^{k}$，化简后为\n$P = (1-e^{-\\cfrac{ln{(\\cfrac{m}{n}ln2)n}}{m}})^{ln(\\cfrac{m}{n}ln2)}$\n如果n比较大，可以近似为\n$(1-e^{-k(n+0.5)/(m-1)})^k ≈ (1-e^{\\cfrac{kn}{m}})^k$\n假定一个元素用16bit，即k=8，那么假阳性的概率为万分之五。当然这根据m(布隆过滤器的总bit)，n(集合中元素数)，k(判定一个元素使用的bit数)的不同有所差别。根据直觉，m越大误判率越小，n越大误判率越大，而k从1开始误判率会经过一个先减小再增加的过程。\n⑱马尔可夫\u0026quot;网络\u0026quot;——贝叶斯网络 马尔可夫链描述了一种状态序列，其每一个状态值取决于前面有限个状态，但实际问题中，很多事物相互的关系并不能用一条链串起来，很可能是交叉的、错综复杂的，如图：\n每一个edge都表示一个量化的可信度(belief)，马尔可夫假设保证了贝叶斯网络便于计算，在网络中，每个节点的概率，都可以用贝叶斯公式来计算，贝叶斯网络因此得名，又因为每个edge都有一个可信度，贝叶斯网络也被称作信念网络(Belief Networks)。\n使用贝叶斯网络必须先确定这个网络的拓扑结构，然后还要知道各个状态之间相关的概率，得到拓扑结构和这些参数的过程分别叫做结构训练和参数训练，统称训练。和训练马尔可夫模型一样，训练贝叶斯网络要用一些已知的数据，并且贝叶斯网络的训练比较复杂，从理论上讲，它是一个NP-hard问题。但是对于某些应用，这个训练过程可以简化，并在计算机上实现。\n优化的贝叶斯网络要保证它产生的序列从头走到尾的可能性最大，即后验概率最大。当然产生一个序列可以有多条路径，从理论上讲，需要完备的穷举搜索（Exhaustive Search），但这就是Np-Hard的了，因此一般采用贪心算法（Greedy Algorithm），也就是在每一步时，沿着箭头的方法寻找有限步，为了防止陷入局部最优，可以采用蒙特卡洛（Monte Carlo），用许多随机数在贝叶斯网络中试一试，看看是否陷入局部最优，这个方法计算量比较大。最近，新的方法是利用信息论，计算节点之间两两的互信息，只保留互信息较大的节点直接的连接，然后再对简化了的网络进行完备的搜索，找到全局优化的结构。\n找到结构后就要对参数进行训练了，假定这些权重用条件概率来度量，我们需要优化贝叶斯网络的参数，使得观察到的这些数据的概率（即后验概率）$P(D|θ)$达到最大，这个过程就是EM过程。在计算后验概率时，计算的是条件X和结果Y之间的联合概率$P(X,Y)$，我们的训练数据会提供一些$P(X,Y)$之间的限制条件，而训练出来的模型要满足这些条件。并且这个模型应该是满足最大熵模型的，因此涉及最大熵模型的训练方法在这里都能使用。\n值得一提的是，结构的训练和参数的额训练通常是交替进行的。IBM华生实验室的茨威格博士(Geoffrey Zweig)和西雅图华盛顿大学的比尔默教授（Jeff Bilmer）完成了一个通用的贝叶斯网络的工具包，提供给贝叶斯网络有兴趣的研究者免费使用。\n贝叶斯网络再图像处理、文字处理、支持决策等方面有很多应用。在文字处理方面，语义相近的词之间的关系可以用一个贝叶斯网络来描述，我们利用贝叶斯网络，可以找出近义词和相关的词，因而在Google搜索和Google广告中都有直接的应用。\n贝叶斯网络可用于分析文本，抽取概念，分析主题，这是一种主题模型（Topic Model）。前面提到的余弦相似性和奇异值分解方法都是主题模型的一种。这里介绍通过贝叶斯网络建立的另一种模型——Google的Rephil。在主题模型中，一个主题可以包含多个词，一个词也可以属于多个主题，那么就可以用贝叶斯网络建立一个文章、概念和关键词之间的联系：\n其中，文章和关键词本身有直接的关联，它们两者都还和概念有直接关联，同时它们通过主题还有间接的关联。\n其他参考资料：\n比尔默于茨威格共同发表的论文：http://people.ece.uw.edu/~bilmes/pgs/sort_Date.html\n斯坦福大学科勒（Daphne Koller）教授的巨著：$\\it{Probabilistic \\ Graphical \\ Models \\ :Principles \\ and \\ Techniques}$\n⑲条件随机场、文法分析与其他 句法分析（Sentence Parsing），指根据文法对一个句子进行分析，建立这个句子的语法树，即句法分析（Syntactic Parsing）。20世纪80年代前，人们受形式语言学的影响，采用基于规则的方法，这种方法规则非常多，并且一旦某一步走岔了，需要回溯很多步，计算复杂度大的不得了。20世纪80年代后，布朗大学计算机系的计算语言学家尤金·查尼阿克（Eugene Charniack）统计出文法规则的概率，在选择文法规则时坚持一个原则——让被分析的句子的语法树概率达到最大。马库斯的学生拉纳帕提把文法分析看成是一个括括号的过程，每次从左到右扫描句子的每个词（或句子成分）时，只需要判断是否属于以下三个操作之一：\nA1.是否开始一个新的左括号，比如在“美联储”是新括号的开始。\nA2.是否继续留在这个括号中，比如在“保险公司”的位置，是否继续留在括号中。\nA3.是否结束一个括号，即标上右括号，比如“资金”的位置是一个括号的结束。\n为了判断采用哪个操作，拉纳帕提建立了一个统计模型$P(A|prefix)$，其中A表示行动，句子前缀$prefix$是指句子从开头到目前为止所有的词和语法成分。拉纳帕提用最大熵模型实现了这个模型，当然，拉纳帕提还用了一个统计模型来预测句子成分的种类。但是这种方法对于非常“规矩“的语句，分析正确率在80%以上，但是2000年后，随着互联网的普及，非严谨的句子中这个模型的正确率连50%也达不到，所幸在很多自然语言处理的应用中，并不需要对语句做深入的分析，只要做浅层的分析（Shallow Parsing），比如找出句子中主要的词语以及它们之间的关系即可。\n到了20世纪90年代以后，随着计算机计算能力的增强，科学家们采用了一种新的数学模型工具——条件随机场，大大提高了句子浅层分析的正确率，可达到95%，使文法分析得以应用到很多产品，比如机器翻译上。\n⑳维特比和他的维特比算法 维特比算法是一个特殊但应用最广的动态规划算法，维特比算法是针对一个特殊的图——篱笆网络（Lattice）的有向图最短路径问题而提出的。它之所以重要，是因为凡是使用隐马尔可夫模型描述的问题都可以用它来解码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词等。\n以输入法拼音转汉字为例，输入可见的序列$y_1,y_2,\u0026hellip;,y_N$，而产生他们的隐含序列使$x_1,x_2,\u0026hellip;,x_N$，$P(x_i|x_{i-1})$是状态之间的转移概率，$P(y_i|x_i)$是每个状态的产生概率。这个马尔可夫链的每个状态的输出是固定的，但是每个状态的值可以变化，比如输出读音”zhong“的字可以是”中“”钟“等多个字。\n用符号$X_{ij}$表示状态$x_i$的第$j$个可能的值，如果把每个状态按照不同的值展开，就得到下图的篱笆网络\n每一条路径都能产生我们观察到的输出序列Y，我们要找到的就是最可能（P最大）的一条路径。但是这样的路径组合数非常多，假定句长为10个字，那么这个组合数为$13^5~5 \\times10^{14}$。假定计算每条路径需要20次乘法，就是$10^{16}$次计算，而今天的处理器每秒计算$10^{11}$次的话，也需要一天时间。因此需要一个最好能和状态数目成正比的算法，这就是维特比算法。\n维特比算法的基础可以概括成以下三点：\n1.如果概率最大的路径$P$（或者说最短路径）经过某个点，那么这条路径上从起始点$S$到这个点的子路径$Q$一定是$S$到这个点的最短距离，否则，用$S$到这个点的最短路径$R$替代$Q$，便构成了一条比P更短的路径，这显然是矛盾的\n2.从$S$到$E$路径必定经过第$i$时刻的某个状态，即对于每个$x$，都要经过它的一个状态，不能跳过其中一个时刻。假定第$i$时刻有$k$个状态，那么如果记录了从$S$到第$i$个状态的所有$k$个节点的最短路径，那么最终的最短路径一定经过其中一条。这样，在任何时刻，只要考虑非常有限条候选路径即可\n3.结合上述两点，假定当我们从状态$i$进入状态$i+1$时，从$S$到状态$i$上各个节点的最短路径已经找到，那么在计算从起点$S$到第$i+1$状态的某个节点的最短路径时，只要考虑从S到钱一个状态$i$所有的$k$个节点的最短路径，以及从这$k$个节点到$x_{i+1},j$的距离即可\n基于上述三点基础，维特比算法过程如下：\nstep1.从S出发，对于第一个状态$x_1$的各个节点，不妨假定有$n_1$个，计算出$S$到它们的距离$d(S,x_{1i})$，其中$x_{1i}$代表任意状态1的节点。因为只有一步，所以这些距离都是S到它们各自的最短距离\nstep2.对于第二个状态$x_2$的所有节点，要计算出从S到它们的最短距离。路径长度$d(S,x_{2i}) = d(S,x_{1j})+d(x_{ij},x_{2i})$，由于$j$有$n_1$种可能，我们要一一计算，然后找到最小值，即\n$d(S,x_{2i})=min_{I=1,n_1}d(S,x_{1j})+d(x_{1j},x_{2i})$\n此时，第二个状态有多少个个节点（假设每个节点只有一条最短路径），就有多少个可能路径，与第一个状态的节点数就无关了，这样一直走到最后一个状态，就得到了整个网格从头到尾的最短路径，每一步计算的复杂度都和相邻两个状态$S_I$和$S_{i+1}$各自的节点数$n_i,n_{i+1}$成正比，即$O(n_i \\cdot n_{i+1})$。如果假定在这个隐马尔可夫链中节点最多的状态有D个节点，也就是说整个网格的宽度为D，那么任何一步的复杂度不超过$O(D^2)$，由于网格长度是$N$，所以整个维特比算法的复杂度是$O(N \\cdot D^2)$。回到上述那个输入法问题，计算量基本上是$13 \\times 13 \\times 10 = 1690≈10^3$，这和原来的$10^{16}$有天壤之别。更重要的是，维特比算法与长度$N$成正比，无论是在通信中，还是在语音识别、打字中，输入都是按照流（Stream）的方式进行的，制药处理每个状态的时间比讲话或者打字速度快（这点很容易做到），那么无论输入有多长，解码过程永远是实时的。\n不知道有没有同学对这个算法的原理有非常似曾相识的感觉，其实这个算法与最短路径算法中的Dijkstra算法很相似——它们的基础理论是相似的，但是具体实现及应用场景有些许不同。首先，维特比只能用于有向无环图求最长最短路径，因为有环的时候 维特比的动态规划的递推公式会相互依赖形成类似于死锁的结构，没法解出来。其次，Dijkstra不适用于有负权值的场景。再次，Dijkstra是找一个节点到其他所有节点到最短路径，viterbi则是在篱笆网络里找一条从起点到终点（可能有多个起点，多个终点）的最短路径。HMM，CRF等算法中使用Viterbi算法，是因为Viterbi算法的效率比Dijsktra算法高。\n㉑CDMA技术——3G移动通信的基础 最早，海蒂·拉玛尔（Hedy Lammarr）与她的邻居乔治·安泰尔（George Antheil）一道发明了一种称为”保密通信系统“的调频通信技术。这种传输方式是在一个较宽的扩展频带上进行的，因此它称为扩频传输(Spread-Spectrum Transmission)。和固定频率的传输相比，它有三点明显的好处：\n1.它的抗干扰能力强。当有人想用噪音干扰固定的广播频率时，对于扩频传输来说基本不可能，因为不能把所有的频带都干扰了，否则整个国家的通信就中断了。\n2.扩频传输的信号很难被截获。以极低的功率在很宽的频带上发送加密信号，对于试图截获者来讲，这些信号能量非常低，很难获取。\n3.扩频传输利用带宽更充分。固定频率的通信由于邻近的频率相互干扰，载波频率的频点不能分布得太密集，两个频点之间的频带就浪费了。扩频通信由于抗干扰能力强，浪费的频带较少。\n虽然这种扩频技术和调频技术早在20世纪60年代就应用于军事，但是转为民用则是20世纪80年代以后的事情，在CDMA以前，移动通信使用过两种技术：频分多址(FDMA)和时分多址(TDMA)。\n频分多址，是对频率进行切分，每一路通信使用一个不同的频率，对讲机采用的就是这个原理。由于相邻频率会互相干扰，因此每个信道要有足够的带宽。如果用户数量增加，总带宽就必须增加。我们知道空中的频带资源是有限的，因此要么必须限制通信人数，要么降低话音质量。\n时分多址是将同一频带按时间分成很多份。每个人的（语音）通信数量在压缩后只占用这个频带传输的$\\cfrac{1}{N}$时间，这样同一个频带可以被多个人同时使用了。第二代移动通信的标准都是基于TMDA的。\n前面讲了，扩频传输对频带的利用率比固定频率传输高，因此，如果把很多细分的频带合在一起，很多路信息同时传输，那么应该可以提高带宽的利用率。\n由于每个发送者有不同的密码，接收者在接到不同信号时，通过密码过滤掉自己无法解码的信号，就可以避免相邻频率干扰的问题。由于这种方法是根据不同的密码区分发送的，因此称为码分多址(CDMA)。\n㉒上帝的算法——期望最大化算法 在一般性问题中，如果有非常多的观测数据，定义一个最大化函数，经过若干次迭代，我们需要的模型就训练好了。这实在是太美妙了。\n前面介绍过的很多算法，其实都是EM算法，比如隐马尔可夫模型的训练方法Baum-Welch算法，以及最大熵模型的训练方法GIS算法。在Baum-Welch算法中，E过程就是根据现有的模型计算每个状态之间转移的次数（可以是分数值）以及每个状态产生它们输出的次数，M过程就是根据这些次数重新估计隐马尔可夫的参数。这里最大化的目标函数就是观测值的概率。在最大熵模型的通用迭代算法GIS中，E过程就是跟着现有的模型计算每一个特征的数学期望值，M过程就是根据这些特征的数学期望值和实际观测值的比值，调整模型参数。这里，最大化的目标函数是熵函数。\n值得一提的是，在凸函数中，EM算法一定能达到最优解，但是在非凸函数中，EM算法可能陷于局部最优解。\n㉓逻辑回归和搜索广告 搜索广告基本上走过了三个阶段。第一个阶段以早期的Overture和百度的广告系统为代表，按广告主出价高低来排名。第二个阶段，Google按照预测哪个广告可能被点击，综合出价和点击率(Click Through Rate, CTR)等因素决定广告的投放。第三个阶段是进一步的全局优化。这章主要介绍第二阶段。\n预估点击率有以下几点问题，首先，对于新广告的冷启动问题，第二，点击数据少，统计数据不足的问题，第三，广告的点击量显然与展示位置有关，因此，在预估点击率时，必须消除这些噪音。最后还要指出，影响点击率的因素非常多，这些都是在预估点击率时要考虑的。\n需要整合这些特征，工业界普遍采用了逻辑回归模型（Logistic Regression Model）。逻辑回归作为最基础的广义线性模型，这里就不多介绍了。一个广告系统中，点击率预估机制的好坏决定了能否城北提高单位搜索的广告收入。而目前Google和腾讯的广告系统在预估点击率都采用了逻辑回归函数。\n另外提一句，CTR在推荐中的实现，现在工业上会使用DeepFM、Wide\u0026amp;Deep等深度学习模型。\n㉔分布式和Google云计算的基础 MapReduce实Google云计算的基础，将一个大任务拆分成小的子任务，并且完成了子任务的计算，这个过程叫做Map，将中间结果合并成最终结果，这个过程叫做Reduce。\n㉕Google大脑和人工神经网络 无论是在计算机科学、通信、生物统计和医学，还是在金融和经济学（包括股市预测）中，大多数与“智能”有点关系的问题，都可以归结为一个在多维空间进行模式分类的问题。模式分类的任务就是要在空间里切一刀，将多个类分开，如语音识别中，就是把韵母a和e分开，有了深度学习，我们就可以实现特征工程自动化了。\n除了神经元之间的线性关系之外，毕竟线性关系上的线性关系还是线性关系，并无意义，所以为了实现复杂的弯曲的世界，我们会增加非线性映射。为了通用性，一般在人工神经网络中，规定神经元函数只能对输入变量线性组合后的结果进行一次非线性变换。请注意是组合后的结果，将每一个输入值都进行非线性变换后再线性组合在一起是不被允许的（这样的计算量太大了）。\n从理论上讲，人工神经网络只要设计得当，就可以实现任何复杂曲线（在高维空间里是曲面）的边界。\n总的来说，人工神经网络是一个分层的有向图，第一层输入节点$X_1,X_2,\u0026hellip;,X_n$接受输入的信息，也成为输入层。来自这些点的数值$x_1,x_2,\u0026hellip;,x_n$按照它们输出的弧的权重($w_0,w_1,w_2,\u0026hellip;,w_n$)进行加权，然后再做一次函数变换$f(G)$，赋给第二层的节点Y。\n第二层的节点照此将数值向后传递，直到最后一层，最后一层又被称为输出层，输出层哪个节点的数值最大，输入的模式就被分在了哪一层。\n在人工神经网络中，需要设计的部分只有两个，一个是它的结构，即网络分几层，每层几个节点，节点之间如何连接等等；第二就是非线性函数$f(·)$的设计，常用的函数有指数函数，sigmoid等，在指数函数时，它的模式分类能力等价于最大熵模型。\n值得指出的是，如果我们把不同输出节点上得到的值看成是一种概率分布，那么实际上人工神经网络就等价于一个概率模型了，比如前面提到的统计语言模型。\n关于人工神经网络的训练，一般情况下，在拥有大量训练数据时，我们定义一个cost function，然后按照梯度下降法找到让成本达到最小的参数，当然除了书中提到的外，还有很多值得我们去深入学习的内容，例如，反向传播导致梯度消失和爆炸，以及衍生的用于不同场景的深度学习模型，例如用于图像的CNN、用于推荐的DeepFM等。\n上述都是在有大量训练数据时的选择，当我们无法获得大量标注好的数据时，就需要借助无监督的训练得到人工神经网络的参数。此时，我们需要定义一种新的成本函数，它能够在不知道正确的输出值的情况下，确定（或者预估）训练出的模型是好是坏。最简单的，我们希望分完类后，同一类样本应该靠的比较近，而不同类比较远，这样就可以把每一个样本点到训练出来的聚类中心（Centroid）的欧氏距离作为成本函数。对于估计语言模型的条件概率，就可以用熵作为成本函数。定义了成本函数后，就可以用梯度下降法进行无监督的参数训练了。对于结构复杂的人工神经网络，它的训练计算量非常大，而且是个Np-Hard问题（梯度下降可能会陷入局部最优），因此有很多机器学习的专家在寻找各种好的近似方法。\n需要指出的是，人工神经网络的规模决定了它能做多大的事情，但是要想要人工神经网络上规模并非易事，因为网络节点之间是连接的，它的复杂度会随着网络规模的扩大呈指数上升。幸运的是，从20世纪90年代到2010年，计算机处理能力的增长和云计算的兴起让人工神经网络的用处大大增加，并且因为并行计算，过去训练人工神经网络的方法就必须改变，以适应云计算的要求。Google大脑就是在这样的前提下诞生的，其创新之处也在于利用了云计算的并行处理技术。\nGoogle大脑采用人工神经网络主要有以下三点原因：首先，人工神经网络理论上可以在多维空间“画出”各种形状的模式分类边界，有很好的通用性。其次，人工神经网络的算法比较稳定且通用，可以一次设计长期并多场景使用。第三，人工神经网络容易并行化，被贝叶斯网络则不然。\nGoogle大脑的实现与MapReduce的分治思想相似，但是更加复杂。它会把一整个神经网络模型切成数千块，与MapReduce不同，每一块的计算并不是完全独立的，而是要考虑上下左右很多块，相互的关联总数大致和块数的平方成正比，虽然会让这一部分关联的计算变得复杂，但是达到了分解一个大任务成多个小任务的目的。\n除了并发训练，Google大脑在减少计算量方面做了两个改进。首先，降低了每次迭代的计算量，Google大脑采用了随机梯度下降（Stochastic Gradient Descent）,这种算法秩序随机抽取少量数据来计算成本函数，可以大大降低计算量，虽然会牺牲一点点准确性。第二个改进是减少迭代次数，Google大脑采用比一般梯度下降法收敛的更快的L-BFGS方法（Limited-memory Broyden Fletcher Goldfard Shanno Method），其原理和随机梯度法相似，但是略微复杂一些。它的好处是可以根据离最后目标的“远近”调整每次迭代的步长，这样经过很少次的迭代就能收敛，但是它每一次迭代的计算量也会增加一点（因为要计算二阶导数）。另外，L-BFGS方法更容易并行化计算。借助这两点，Google大脑才能完成外界认为计算量大的难以承受的人工神经网络的训练任务。\n㉖人工神经网络与贝叶斯网络 共同点：\n1.它们都是有向图，每一个节点的取值只取决于前一级的节点，而与更前面的节点无关，也就是说遵从马尔可夫假设\n2.它们的训练方法相似\n3.对于很多模式分类问题，这两种方法在效果上相似，也就是说很多用人工神经网络解决的问题，也能用贝叶斯网络解决，反之亦然，但是它们的效率可能会不同。如果把人工神经网络和贝叶斯网络都看成事统计模型，那么这两种模型的准确性也是类似的\n4.它们的训练计算量都特别大，大家在使用人工神经网络时要有心理准备\n不过，人工神经网络与贝叶斯网络还是有不少差别的：\n1.人工神经网络在结构上完全是标准化的，而贝叶斯网络更灵活。Google大脑选用人工神经网络，就是因为看中了它的标准化这一特点\n2.虽然神经元函数为非线性函数，但是各个变量智能先进性线性组合，然后对一个变量（即前面组合出来的结果）进行非线性变换，因此用计算机实现起来比较容易。而在贝叶斯网络中，变量可以组合成任意的函数，毫无限制，在获得灵活性的同时，也增加了复杂性\n3.贝叶斯网络更容易考虑（上下文）前后的相关性，因此可以解码一个输入的序列，比如将一段语音识别成问题，或者将一个英语句子翻译成中文。而人工神经网络的输出相对孤立，它可以识别一个个字，但是很难处理一个序列，因此它主要的应用常常是估计一个概率模型的参数，比如语音识别中声学模型参数的训练、机器翻译中语言模型参数的训练等，而不是作为解码器\n㉗区块链——椭圆曲线加密原理 想要保护私有信息，特别是隐私，必须有一套比对称的机制，做到在特定授权的情况下，不需要拥有信息也能使用信息；在不授予访问信息的权限时，也能验证信息。比特币的意义就在于，它证实了利用区块链能够做到上述这两件事。\n㉘量子密钥分发——随机性带来的好处 数据泄露无外乎两种可能：在数据存储的地方被盗取，或者在数据传输的过程中被解惑。要解决这个问题，最好的方法就是对数据进行加密。那么是否存在一种无法破解的密码呢？其实信息论的发明人香农早就指出了，一次性密码从理论上讲永远是安全的。而近年来非常热门的量子通信，便是试图实现加密密钥的安全传输，确保保密通信不被识破。这就是量子密钥分发技术。量子通信并不是像很多媒体曲解的那样——靠量子纠缠实现通信，而是靠光量子的偏振特性承载信息，靠数学和信息论的基本原理保证它的保密性。\n光子既是一种粒子，又是一种波，其传播方向与振动方向垂直，这就是爱因斯坦指出的光的波粒二象性。光子的振动频率和偏振的方向都可以人为控制，激光震动的频率已应用到激光通信中，而量子密钥分发利用了光子的偏振特性。即，我们可以在发送方调整光的振动方向来传递信息，比如把光偏振的方向调成水平的，代表0；调成垂直的，代表1。在接收端，我们放置一个垂直的偏振镜就能检测到所传递来的垂直信息，我们收到信号，就认为发送方送来的信息是1。当然，这么做不是很可靠，因为没有收到信号时，不容易确认是对方没有发送，还是发送过来的是0，因此，更好的办法就是在接收方用一个十字交叉的光栅，让垂直和水平的信号都通过，这样就不会把信息0与没有发送信息这件事混淆了。\n当然，（激光）光子的偏振方向可以有各种角度，未必一定要是水平或者垂直的，而如果偏振的方向是其他角度，经过一个水平的光栅，它是否能通过就是随机的了。例如，如果发送方发射了一个偏振方向是45°的光子，它经过垂直光栅的概率是50%，即被检测为1的概率为50%、这个信息就是随机的了。\n利用这个特性，我们就可以来分发密钥了，具体做法是这样的。首先，发送方和接收方约定好有两组信息编码方式，一组用垂直的偏振光代表1，水平的代表0，另一组则分别用45°和135°代表1和0。其次，发送方采用哪种编码方式完全是随机的，而且是交替进行的，它并不告诉接收方。接收方也随机调整偏振镜（光栅）的方向。此时的期望值如下，有50%的可能接收方和发送方的解调和调制方法一致，另外50%不一致的情况下，又有50%的几率随机蒙对，也就是说，不论接收方如何设置偏振镜解调的方向，最后得到的信息大约有75%是一致的，或者说误码率为25%。\n如果在传输过程中，信息被中间的窃听者截获了。由于光子在经过被错误放置的光栅中，光子的偏振方向就无从得知了，得到的是0还是1是完全随机的（可以被认为是噪声）。如果窃听者再将这些信息转发给原本的接收者，接收者得到的信息只有$75% \\times 75% = 56.25%$。接下来，如果接收方再将自己的信息发还给发送者确认，发送者就会发现只有56.25%的一致性，这时他们就知道信息被截获了。\n解决了信息安全问题后，我们就需要消除信息的不确定性，来确定以下双方通信的密钥。这一步其实非常简单，发送方只要用明码将它调制偏振方向的基传给接收方即可。这样，接收方就知道在哪些信息位它设置对了（其实是蒙对了），然后再用明码把它设置对的信息位告诉发送方即可。此时这个密钥一般有一半传送信息的长度（偏振器设置对的概率为50%），且完全是随机的。上述通信都是明码进行的，但是因为窃听者并不知道原本的信息，所以获知哪些位置应该设置什么样的解调器并没有意义，并没有破坏密钥的安全性。\n上述这种通信协议被称为BB84协议，时查理斯·贝内特(Charles Bennett)和吉勒·布拉萨(Gills Brassard)在1984年发表的。后来，人们又在这个协议的基础上进行改进，有了其他的协议，但是其加密和通信原理并没有本质的变化。\n在使用上述协议通信的过程中，发送方和接收方需要通过几次通信彼此确认密钥，而这个密钥只使用一次。如果需要继续通信，就需要再产生和确认新的密钥。因此，这种做法实际上是用时间换取通信的安全性。\n值得一提的是，量子通信绝不像很多媒体讲的是万能的。加入通信卫星真的被黑客攻击了，或者通信的光纤在半途被破坏了，虽然通信的双方知道有人在偷听，能够中断通信，不丢失保密信息，但是于此同时，它就无法保证正常的信息能送出去了，就如同情报机关虽然抓不到对方的信使，却能把对方围堵在家里，不让消息发出。此外，虽然今天已经实现了上千千米量级的量子密钥分发，但是量子通信从实验到工程，再到商用，还有很长的路要走。\n㉙数学的极限——希尔伯特第十问题和机器智能的极限 就如物理学上无法超越的光速极限或绝对零度的极限一样，人工智能的能力有数学的边界。这一边界与技术无关，仅取决于数学本身的限制。具体到今天大家使用的计算机，它有着两条不可逾越的边界，分别是由图灵和希尔伯特划定的。\n图灵划定计算机可计算问题的边界：机器智能显得极为强大，靠的是人们找到了让及其拥有智能的方法，即大数据、摩尔定律和数学模型这三个支柱。我们在前面各个章节中所介绍的内容，其实依然只是一部分数学模型而已。这些数学模型将各种形形色色的实际问题变成了计算问题，当然，这里面有一个前提，就是那些问题本质上就是数学问题，而且是可以用计算机计算的数学问题。但是，当计算机科学家们揭开了一个又一个这样的问题的数学本质之后，人们自然会贪心地以为这样的进步是没有极限的，一致浪费时间去解决根本解决不了、可能也没有必要解决的问题。图灵思考问题的方式恰恰和常人一步步进步的方式相反，他会先划定计算这件事情的边界，在他眼中边界内的问题都是可以通过计算来解决的，当然在边界外可能还有更多的问题，它们与计算无关，无法通过计算来解决，因此图灵并不打算考虑它们。图灵就划定了这样一条边界，后人就不必再浪费时间纠结没有意义的事情，也就不必试图超越边界或极限做事情。\n计算对应于确定性的机械运动，这保证了在相同条件下计算出来的结果是可重复的，而人的意识则可能来自于测不准原理（量子力学中一个微粒的位置与速度无法同时被准确测量，即越精确的知道速度，就越测不准位置，而人则是由无数的测不准堆积出来的意识）。所以，关于人的意识，图灵认为是不确定的，不属于计算的范畴，如果真是像图灵想的那样，那么宇宙本身就存在着大量数学问题之外的问题。事实上，与图灵同时代的数学家哥德尔在1930年便证明了数学不可能既是完备的，又是一致的，也就是说，一些命题即使是对的，我们也无法用数学证明它们。这被称为哥德尔不完全性定理，它说明数学的方法不是万能的。事实上除了意识上的不确定性，一些数学问题也是无法用数学证明的，今天很多数学问题并没有数学答案，比如不存在一组正整数$x,y,z$，让它们满足$x^3+y^3=z^3$。这个问题实际上是著名的费尔马大定理的一个特例，而这个定理本身已于1994年由英国著名的数学家怀尔斯（Andrew Wiles）证明了，也就是说，它是无解的。当然，不管怎么说，知道一个问题无解也算是有了答案。但是，是否还存在一些问题，我们根本无法判定答案存在与否呢？如果有，那么这类问题显然是无法通过计算来解决的。在这一方面给了图灵启发的是希尔伯特。\n1900年，希尔伯特在国际数学大会上提出了23个（当时还无解的）著名的数学问题，其中第十个问题讲的是：\n“任何一个（多项式）不定方程，能否通过有限步的运算，判定它是否存在整数解”\n所谓不定方程（也被称为丢番图方程，Diophantine equation），就是指有两个或更多未知数的方程，它们的解可能有无穷多个。为了对这个问题有根性认知，我们来看三个特例：\n例一 $x^2+y^2=z^2$\n这个方程有三个未知数，它有很多正整数解，每一组解其实就是一组勾股数，构成直角三角形的三边。\n例二 $x^N+y^N=z^N$\n这些方程都没有正整数解，这就是著名的费尔马大定理。\n例三 $x^3+5y^3=4z^3$\n这个方程是否有正整数解，就不那么直观了。更糟糕的是，我们没有办法一步一步地判定它是否存在整数解。此外，需要指出的是，即使我们能判定它有整数解，也未必找得出来。\n希尔伯特第十问题在1970年被苏联数学家尤里·马蒂亚塞维奇（Yuri Matiyasevich）严格地证明了，除了极少数特例，在一般情况下，无法通过有限步的运算，判定一个不定方程是否存在整数解，那么就说明很多数学问题其实上帝也不知道答案是否存在。对于连答案存在都无法判定的问题，答案自然是找不到的，我们也就不用费心去解决这一类问题了。正是希尔伯特对数学问题边界的思考，让图灵明白了计算的极限所在。\n第十问题向世人宣告了很多问题我们无从得知是否有解。如果连是否有解都不知道，就更不可能通过计算来解决它们了。更重要的是，这种无法判定是否有解的问题，要远比有答案的问题多得多。即，有答案的问题\u0026mdash;\u0026gt;可判定的问题\u0026mdash;\u0026gt;数学问题\u0026mdash;\u0026gt;所有问题。\n那么对于可判定且有答案的数学问题是否都能够用计算机解决吗？那就要看看计算机是怎么设计的了，1963年，图灵提出了一种抽象的计算机的数学模型，这就是后来人们常说的图灵机，图灵机的核心思想就是用机器来模拟人进行数学运算的过程，这个过程其实是在不断重复两个动作：在纸上写或擦掉一些符号，用笔在纸上不断移动书写位置。图灵机这种数学模型在逻辑上非常强大，任何可以通过有限步逻辑和数学运算完成的问题，从理论上讲都可以遵循一个设定的过程，在图灵机上完成。今天的各种计算机也不过是图灵机这种模型的一种具体实现方式。不仅如此，今天那些还没有实现的假想计算机，比如量子计算机，在逻辑上也并没有超出图灵机的范畴，因此，在计算机科学领域，人们就把能够用图灵机计算的问题称为可计算的问题。\n是否所有有答案问题都是可计算问题，这一问题依旧有争议，一方面，人们总可以构建出一些类似悖论的数学问题，显然无法用图灵机来解决；另一方面，在现实世界里是否有这样的问题存在，或者说这些构建出来的我呢提是否有意义，很多人觉得暂时没有必要去考虑。但不管怎么讲，依据丘奇和图灵这两位数学家对可计算问题的描述（也就是所谓的丘奇-图灵论题），有明确算法的任何问题都是可计算的，至于没有明确算法的问题，计算也无从谈起。另外，对于理论上可计算的问题，在工程上未必能实现，比如NP-Hard问题，可能需要算到宇宙的尽头，并且图灵机没有存储内容的限制，这在现实中也是不可能的。\n联系一下上述的子集关系，即所有问题\u0026mdash;\u0026gt;数学问题\u0026mdash;\u0026gt;可判定的问题\u0026mdash;\u0026gt;有答案的问题\u0026mdash;\u0026gt;可计算问题\u0026mdash;\u0026gt;工程可解问题\u0026mdash;\u0026gt;人工智能问题。这一种层层递进的包含关系，就可以清楚的看到人工智能的边界了。可以看到，理想状态的图灵机可以解决的问题，只是有答案问题的一部分，而在今天和未来，在工程上可以解决的问题都不会超出这个可计算的范畴。\n完结撒花✨✨✨\n","id":10,"section":"posts","summary":"本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，没有选择自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些","tags":[],"title":"十分钟读完数学之美","uri":"https://biofrostyy.github.io/2021/06/10%E5%88%86%E9%92%9F%E8%AF%BB%E5%AE%8C%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/","year":"2021"},{"content":" 评前碎碎念\n 翻译的好坏总是能左右我对一本书的兴趣，仿佛我试图入坑克苏鲁神话被译本晦涩仿佛机翻的语言劝退的那天，一直以来我对译本总是抱着怀疑的态度，我觉得翻译过就缺失了原作者的最初想法和语言造诣。尤其是日文译本，包括早些年看过村上春树，我不懂日文，不知道是不是日文本身的原因会造成译本晦涩难懂，还好这本书除了开头的几章有些晦涩，后面也没什么阅读障碍，我得以顺利的读完。另外，本书更多聚焦在社会派的推理，直到中间部分才开始出现紧张而烧脑的案件推进情节，对于本格推理爱好者可能吸引不大，对死刑相关及当下社会的思考才是这本小说最注重且可贵的地方。\n 读后感\n 说回这本书的本身，关于刑法的正义性和是否废除死刑的喧嚣从来没有停止过，\u0026lt;大卫·戈尔的一生\u0026gt;，\u0026lt;一级恐惧\u0026gt;等，都让我印象深刻。我也渐渐对于各种故事（尤其是日式推理）里面所有被害人都是罪有应得的设定，开始审美疲劳起来。这本书带来了思考，我也庆幸作者的立场大体上与我的并没有相悖，不至于看得难受。从理性角度私刑永远不该被选择，从感情角度我只关注受害者的故事，无论犯罪者有多么凄惨的过去，只要他伤害的是无辜者，那么他永远得不到我感情上的同情。如果感情上总是站在加害者的立场上，为他找各种理由得到宽恕，遵纪守法的公民们又怎么能感到安全呢？毕竟我真的有一头牛。\n作为这本书的读后感，我将分别从两个主角-纯一和南乡-的角度观察他们的行为与立场。\n因为纯一是个杀人犯，所以他一直在把自己代入罪犯的立场，他会在翻案时犹豫，会认为他们查案去抓住真正的凶手只不过是把一个人解救出来，又把另一个人送上绞刑台吗？他这种想法这在我看来是无法理解的。但是他确实是我感情上的同情者，在这起案件中，佐村恭介是“罪有应得”的受害人，他对于杀害佐村恭介没有懊悔，他也对于佐村恭介的父亲想要杀死自己没有任何责备\u0026ndash;“就像我杀死了佐村恭介那样，他的父亲也可以杀死我”。他体会到了私刑会引起一次又一次的复仇，为了避免这种情况的发生，就必须有人来代替他们做这件事，他安慰了南乡，至少给不是被冤枉的死刑犯实施死刑确确是正确的事。\n再说到审判官南乡，最让我震撼也是感同身受的就是南乡的想法变化，最开始他认为自己在做正义的替受害者惩罚犯罪者的事，后来他看到了一封家属宽恕犯罪者的信，开始怀疑如果受害者家属都希望宽恕犯罪者，自己忍着人类对\u0026quot;杀人\u0026quot;的厌恶实施死刑是为了谁，但其实这里有两个逻辑问题：一是家属可以代替受害者宽恕吗，如果可以那会不会有人伙同罪犯杀掉家人然后再作为家属为罪犯脱罪？二是文中这个家属所说的宽恕是希望施暴者永生都在监狱中，但事实上，按书中所说，如果不是死刑，那么如果考虑表现优异的减刑政策，无期徒刑的平均服刑时间是十八年。南乡一直在挣扎，这些事情本就像是一万个哈姆雷特，我认为或者纯一认为南乡是正义的都没有用，最重要的是南乡可以走出自己心里的牢，毕竟就算是大慈大悲的佛教也为那些无法挽救的愚昧众生准备了破坏神\u0026ndash;不动明王。\n说到本书表达的社会性问题。首先，对死刑问题持有疑问，其中一个原因人们把死刑与杀人的不快感混同在一起导致的，正如南乡在亲自执行死刑之前，他一直是支持死刑制度的，而为了规避私刑，执行死刑必须有第三者，也就是国家机器行使刑罚。这是很多支持废除死刑的人的论据之一，他们认为这对行刑者是一种折磨。有些工作肯定是大家都不想去做的，例如随时会死的矿工工人，但是当他的社会责任感或是朴素的钱给够了，总是会有一些人愿意去做，请注意，是愿意去做，他们有选择权且他们选择了去做，我们尊敬他们，但为了他们废除一种刑罚则大可不必。再一点，有人认为，人生而平等，人命也是平等的，那么法律剥夺罪犯的生命，不和罪犯剥夺被害者的生命一样吗。这种想法我实在是无法苟同。首先罪犯剥夺被害者的生命是一个自然人对另一个自然人生命权力的剥夺，是主动的，而死刑则是报复式刑法的一种，是第三方对一个自然人生命的剥夺。另外，作为社会中拥有权利的公民，首先他是要遵守社会的秩序的，那么在罪犯剥夺受害人生命的时候，就已经放弃了自身作为合法公民的权利，那么法律刑罚应用在他的身上一点都不令人奇怪。\n第二点令我震撼的是，整个故事中反映出的对受害者的恶意，性侵是亲告罪，纯一与友里不敢声张，害怕受到检方和社会更大的而侮辱；南乡是属于自卫杀人，但是在他杀人的一瞬间，他的south wind面包店和家人就已经成为遥不可及的梦。身体上的伤害尚且可愈，心灵上的伤害对于受害者可能是永远的无期徒刑了。\n最后，对于十三级台阶，就像挑水的三个和尚。虽然着墨不多，但是也几次描写了死刑审判员们，因为怕麻烦或担心支持率，虽然案件存疑，也推动了树原亮的死刑进程。作为替受害者”审判\u0026quot;的第三者，也需要有良好的制度与责任感，才能承担法务国家机器的责任。\nPS.十三级台阶，指对于一个死刑犯的判决，共经过13个人的审核。第一步刑事局3人审核；第二步矫正局3人审核；第三步保护局3人审核；第四步法务大臣事务局秘书科科长审核；第五步法务大臣事务局局长审核；第六步法务大臣的次官审核；最后第七步法务大臣审核。一共13人。\n","id":11,"section":"posts","summary":"评前碎碎念 翻译的好坏总是能左右我对一本书的兴趣，仿佛我试图入坑克苏鲁神话被译本晦涩仿佛机翻的语言劝退的那天，一直以来我对译本总是抱着怀疑的态","tags":[],"title":"消失的第13级台阶","uri":"https://biofrostyy.github.io/2021/05/%E8%AF%BB%E5%90%8E%E6%84%9F-%E6%B6%88%E5%A4%B1%E7%9A%84%E7%AC%AC13%E7%BA%A7%E5%8F%B0%E9%98%B6/","year":"2021"},{"content":"一.问题介绍 工作中遇到一个问题，一个列表中，存在多个以\u0026quot;-\u0026ldquo;连接的范围string，希望可以把规则集压缩，即融合的范围融合\nI：[10-30kg,3-10kg,3-10kg,1-3kg,3-10kg,3-10kg,50-100kg]\nO：[1-30kg,50-100kg]\n注：范围string无序且有重复\n二.python实现 ①去重并排序 def takeFirst(elem): return int(elem.split('-')[0]) input = list(set(input.split(\u0026quot;,\u0026quot;))) # 去重 input.sort(key=takeFirst) # 排序 '''此时input为[3-10kg,10-30kg,50-100kg]'''  ②规则融合 def merge(elem1,elem2): elem1 = elem1.split('-') # 将元素基于-分割，次数为没有后缀的情况，如果像我上面的实例有kg作为后缀，可以先对string将进行切片，如elem1[0:-2] elem2 = elem2.split('-') # 如果前一个元素的最大值等于后一个元素的最小值，则进行融合，返回融合值，否则返回空 if elem1[1] == elem2[0]: return ''.join([elem1[0],'-',elem2[1]]) def mergelist(list): res = [] # 如果此input列表只有一个元素，则返回原列表 if len(list) == 1: return list # 从列表后端遍历所有相邻元素对 a = len(list)-2 while a \u0026gt;= 0: new_elem = merge(list[a],list[a+1]) # 如果可以融合，删除两元素，并在原位置插入新元素 if merge(list[a],list[a+1]) is not None: list[a] = new_elem del list[a+1] a -= 1 return list # 如果存在'\u0026gt;100kg'这样的情况，可以先转换成'100-infikg'进行处理，结束后再统一反转 mergedinput = mergelist(input) # 融合  三.你可能还需要\u0026hellip; ①融合前整合数据 我们拿到手的数据，都是松散的，所以需要我们先整合再进行上述融合操作，pandas库的groupby函数可以帮我们做到这一点\nI：\n   name classify weight     小王 手表 0-1kg   小王 日用品 3-10kg   小李 手机 1-3kg   小王 日用品 1-3kg   小李 手机 \u0026lt;1kg   小李 手机 10-30kg   小李 手机 1-3kg    O：\n   name classify weight     小王 手表 0-1kg   小王 日用品 3-10kg,1-3kg   小李 手机 1-3kg,\u0026lt;1kg,10-30kg,1-3kg    outputDataframe = inputDataframe.groupby(['name','classify'])['weight'].apply(lambda x:x.str.cat(sep=',')).reset_index()  ②融合后，无法融合的部分松散数据 对于无法融合成一条的多条规则，因为dataframe后续处理问题，我们也不能将这些规则继续挤在一行用逗号隔开，我们需要将他们拆成多行，保证每一行只有一个规则\nI：\n   name classify weight     小王 手表 0-1kg   小王 日用品 1-10kg   小李 手机 0-3kg,10-30kg    O：\n   name classify weight     小王 手表 0-1kg   小王 日用品 1-10kg   小李 手机 0-3kg   小李 手机 10-30kg    ''' 本质上，使用numpy来存储累加数据，再重新转为dataframe ''' newvalues=np.dstack((np.repeat(inputDataframe.name.values,list(map(len,inputDataframe.weight.values))),np.repeat(inputDataframe.classify.values,list(map(len,inputDataframe.weight.values))),np.concatenate(inputDataframe.weight.values))) outputDataframe = pd.DataFrame(data=newvalues[0],columns=inputDataframe.columns)  ","id":12,"section":"posts","summary":"一.问题介绍 工作中遇到一个问题，一个列表中，存在多个以\u0026quot;-\u0026ldquo;连接的范围string，希望可以把规则集压缩，即融合的范围","tags":[],"title":"python 实现规则集分类并融合压缩","uri":"https://biofrostyy.github.io/2021/05/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80/","year":"2021"}],"tags":[]}