{"categories":[{"title":"“python\"","uri":"https://biofrostyy.github.io/categories/python/"},{"title":"“数据结构\"","uri":"https://biofrostyy.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"title":"“日记\"","uri":"https://biofrostyy.github.io/categories/%E6%97%A5%E8%AE%B0/"},{"title":"“机器学习”","uri":"https://biofrostyy.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"title":"“系统工程”","uri":"https://biofrostyy.github.io/categories/%E7%B3%BB%E7%BB%9F%E5%B7%A5%E7%A8%8B/"},{"title":"“读书笔记”","uri":"https://biofrostyy.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"title":"“读后感\"","uri":"https://biofrostyy.github.io/categories/%E8%AF%BB%E5%90%8E%E6%84%9F/"}],"posts":[{"content":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和谐统一的。例如YouTube的用户体验和公司利益（时长越长广告曝光越多）在“观看时长”这一点上达成了一致。\n下图是推荐系统的技术架构示意图。其中数据部分为融合了数据离线批处理、实时流处理的数据流框架；算法和模型部分则为集训练(training)、评估(evaluation)、部署(deployment)、线上推断(online inference)为一体的模型框架。\n推荐系统的进化之路 传统推荐模型 一. 协同过滤算法 UserCF基于用户相似度进行推荐，它符合人们直觉上的“兴趣相似的朋友喜欢的物品，我也喜欢”的思想，这使其具有更强的社交属性，这样的特点使其非常适合新闻推荐场景，因为新闻本身的兴趣点往往是分散的，相比用户对不同兴趣的偏好，新闻的及时性、热点性往往是其更重要的属性，而UserCF正适用于发现热点，以及跟踪热点的趋势。而ItemCF适用于兴趣变化较稳定的应用，例如Amazon、Youtube等。但从技术角度，它主要有两个缺点，首先是，互联网应用场景下，用户数往往远大于物品数，而UserCF需要维护用户相似度矩阵以便快速找出$Top n$相似用户，这使得存储开销非常大$O(n^2)$，第二点是用户的历史数据向量非常稀疏，对于只有几次购买或点击的用户来说，找到相似用户的准确度是非常低的，这导致UserCF不适用于哪些正反馈获取困难的场景（如酒店预订、大件商品购买等低频应用）。\nItemCF解决了上述存储开销大的问题，但是由于数据稀疏，它仍然有协同过滤的天然缺陷——推荐结果的头部效应较明显，处理稀疏向量的能力弱。为了增强模型的泛化能力，矩阵分解技术被提出。相比协同过滤，矩阵分解的①泛化能力强，可以在一定程度上解决数据稀疏问题，②空间复杂度低，只需存储用户和物品隐向量，空间复杂度由$O(n^2)$降低到$O((m+n)\\cdot k)$级别。③具有更好的扩展性和灵活性，这其实与Embedding思想不谋而合，因此矩阵分解的结果也非常方便与其他特征进行组合和拼接，并便于与深度学习网络进行无缝结合。\n与此同时，矩阵分解也有一定局限性。它不方便加入其他特征，丧失了利用很多有效信息的机会。为了解决这个问题，逻辑回归及其后续发展出的因子分解机等模型，凭借其天然的融合不同特征的能力，逐渐在推荐系统领域得到更广泛的应用。\n1.UserCF 共现矩阵中，和你评分行为相似的TopN用户对物品p的评分。其中值得注意的两部分为 ①用户相似度和②最终结果排序\n①理论上，任何合理的“向量相似度定义方式”都可以作为相似用户计算的标准。例如余弦相似度，皮尔森相关系数，相比余弦相似度减小了用户评分偏置的影响。\n②最常用的方式是利用用户相似度和相似用户的评价的加权平均获得目标用户的评价预测\n$R^(u,p)=\\cfrac{\\sum_{s\\in{S}}(W_{u,s}\\cdot{R_{s,p}})}{\\sum_{s\\in{S}}W_{u,s}}$\n其中，权重$W_{u,s}$是用户$u$和用户$s$的相似度，$R_{s,p}$是用户$s$对物品$p$的评分。\n2.ItemCF 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的$Topk$物品，对这些物品进行相似度分值排序，相似度分值为与已有正反馈物品相似度的累加。\n$R_{u,p} = \\sum_{h\\in{H}}(W_{p,h} \\cdot R_{u,h})$\n其中，$H$是目标用户的正反馈物品合集，$w_{p,h}$是物品$p$与物品$h$的物品相似度，$R_{u,h}$是用户$u$对物品$h$的已有评分。\n3.矩阵分解\n该方法在协同过滤共现矩阵的基础上，使用更稠密的隐向量表示用户和物品，挖掘用户和物品的隐含兴趣和隐含特征，在一定程度上弥补了协同过滤模型处理稀疏矩阵能力不足的问题。但仍无法引入用户画像信息、物品画像信息和实时上下文信息，这就需要机器学习模型来解决了。\n矩阵分解算法将$m \\times n$维的共现矩阵R分解为$m \\times k$维的用户矩阵$U$和$k \\times n$维的物品矩阵$V$相乘的形式。其中$k$是隐向量的维度，$k$的大小决定了隐向量表达能力的强弱。$k$的取值越小，隐向量包含的信息越少，模型的泛化程度越高；反之，$k$的取值越大，隐向量的表达能力越强。在具体应用中，$k$的取值要经过多次试验找到一个推荐效果和工程开销的平衡点。\n矩阵分解主要有三种方法：①特征值分解(Eigen Decomposition)、奇异值分解(Singular Value Decomposition, SVD)和梯度下降(Gradient Descent)。其中特征值分解只能作用于方阵，显然用户-物品矩阵不是。奇异值分解存在两点缺陷，使其不宜作为互联网场景下矩阵分解的主要方法：①奇异值分解要求共现矩阵是稠密的，如果要应用，就要对缺失元素进行填充。②传统奇异值分解的计算复杂度达到了$O(mn^2)$，这对于动辄上千万的互联网场景来说不可接受。\n由上，梯度下降成了进行矩阵分解的主要方法，加入正则化项的目标函数入下：\n$\\underset {q^,p^}{min}\\underset {(u,i)\\in K}{\\sum}(r_{ui}-q_i^Tp_u)^2+\\lambda(||q_i||+||p_u||)^2$\n隐向量的生成过程其实是对共现矩阵进行全局拟合的过程，因此隐向量其实是利用全局信息生成的，有更强的泛化能力；而协同过滤中只利用用户和物品自己的信息进行相似度计算，这就使协同过滤不具备泛化利用全局信息的能力。\n为了消除用户和物品打分的偏差(Bias)，常用的做法是在矩阵分解时加入用户和物品的偏差向量：\n$r_{ui} = \\mu + b_i + b_u +q_i^Tp_u$\n其中$\\mu$是全局偏差常数，$b_i$是物品偏差系数，可使用物品$i$收到的所有评分的均值，$b_u$是用户偏差系数，可使用用户$u$给出的所有评分的均值。与此同时，目标函数也要有相应改变：\n$\\underset {q^,p^,b_*}{min}\\underset {(u,i)\\in K}{\\sum}(r_{ui}- \\mu -b_u-b_i-q_i^Tp_u)^2+\\lambda(||q_i||+||p_u||+b_u^2+b_i^2)^2$\n二.逻辑回归 逻辑回归作为广义线性模型的一种，使用softmax(二分类退化为sigmoid)映射线性模型至0-1，符合点击率的物理性质。目标函数可以分别通过交叉熵和服从伯努利的最大似然推导（最大似然取log后与交叉熵损失函数等价），参数训练常采用的方法为梯度下降法、牛顿法、拟牛顿法等。\n逻辑回归的优点在于，①数学含以上的支撑，点击率这个行为服从伯努利分布的这个假设，采用逻辑回归作为CTR模型是符合“点击”这一事件的物理意义的。②权重可解释性强。③易于并行化、模型简单、训练开销小。\n但它也有局限性，它表达能力不强，无法进行特征交叉、特征筛选等一系列较为“高级”的操作，因此不可避免地造成信息的损失。为了解决这一问题，衍生出因子分解机等高维的复杂模型，在进入深度学习时代后，多层神经网络强大的表达能力可以完全替代逻辑回归模型。\n三.从FM到FMM-自动特征交叉的解决方案 算法工程师手动组合特征，再通过各种分析手段筛选特征的，这种方法无疑是低效的，并且人类的经验往往有局限性，程序员的精力和时间无法支撑找到最优的特征组合。于是，模型自动特征交叉的方案应运而生。\n1.POLY2模型——特征交叉的开始 $\\phi POLY2(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}w_h(j_1,j_2)x_{j_1}x_{j_2}$\n该模型对所有特征两两交叉(特征$x_{j_1}$和$x_{j_2}$)，并对所有的特征组合赋予权重$w_{h(j_1,j_2)}$。POLY2通过暴力组合特征的方式，在一定程度上解决了特征组合的问题。POLY2模型本质上仍是线性模型，训练方法与逻辑回归并无区别，因此便于工程上的兼容。但是PLOY2模型存在两大缺陷，①one-hot编码的稀疏特征，交叉后更加稀疏，导致大部分交叉特征的权重缺乏有效数据训练，无法收敛。②权重参数的数量由$n$直接上升到$n^2$，极大地增加了训练的复杂度。\n2.FM模型——隐向量特征交叉 与POLY2不同的是，FM用两个向量的内积$(w_{j_1} \\cdot w_{j_2})$取代了单一的权重系数$w_h(j_1,j_2)$。具体地说，FM为每个特征学习了一个隐权重向量(latend vector)。在特征交叉时，使用两个隐向量的内积作为权重，这和矩阵分解的隐向量有着异曲同工之妙：\n$\\phi FM(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}(w_{j_1} \\cdot w_{j_2})x_{j_1}x_{j_2}$\n此时参数数量为$nk$（$k$为隐向量维度，$n\u0026raquo;k$），使用梯度下降法进行训练时复杂度可被同样降低到$nk$级别，极大降低了训练开销。\n同时，隐向量更好地解决了数据稀疏性问题。POLY2中只有两种特征取值同时出现时，才能学习这个组合的权重，例如(‘male’,\u0026lsquo;earrings\u0026rsquo;)，当这两种特征出现次数非常少时，则此参数缺乏有效训练。而隐向量可以通过(\u0026lsquo;male\u0026rsquo;,xx)和(xx,\u0026lsquo;earrings\u0026rsquo;)分别训练隐向量。这样，甚至对于一个从未出现过的组合，由于模型之前已经学习过两个的分别隐向量，也具备了计算该特征组合权重的能力。所以，相比POLY2，FM虽然丢失了某些具体特征组合的精确记忆，但是泛化能力大大提高。\n在工程方面，FM同样可以使用地图下降法，使其不失实时性和灵活性。相比之后深度学习复杂的网络结构导致难以部署和线上服务。FM较容易实现的模型结构使其线上推断的过程相对简单，也更容易进行线上部署和服务。因此，FM在2021-2014年前后，成为业界主流的推荐模型之一。\n3.FFM模型——引入特征域的概念 相比FM模型，FMM模型引入了特征域感知(field-aware)这一概念，使模型的表达力更强。\n$\\phi FMM(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}(w_{j_1,f_2} \\cdot w_{j_2,f_1})x_{j_1}x_{j_2}$\n当$x_{j1}$特征与$x_{j2}$特征进行交叉时，$x_{j1}$特征会从$x_{j1}$的这一组隐向量中挑出与特征$x_{j2}$的域$f_2$对应的隐向量$w_{j1,f_2}$进行交叉。这里说的 域(field)是指某个分类特征one-hot形成的一段特征向量。\nFMM保留了域的概念增强了模型表达能力，这也导致计算复杂度上升到$kn^2$，在实际工程应用中，需要在模型效果和工程投入之间进行权衡。\n四.GBDT+LR——特征工程模型化的开端 无论是FM还是FMM都是在做二阶特征交叉，如果继续提高特征交叉维度，会不可避免地产生组合爆炸和计算复杂度过高的问题。2014年，Facebook提出了基于GBDT+LR的组合模型解决方案。利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型输入。GBDT和LR这两步是独立训练的，所以不存在如何将LR的梯度回传到GBDT这类复杂问题。\nGDBT是由多棵回归树组成的树林，后一棵树以前面树林的结果与真实结果的残差为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此书中每个节点的分裂是一个自然的特征选择过程，而多层节点的结构则对特征进行了有效的自动组合，GDBT中每一个树都是一个交叉特征，而树的深度决定了交叉的阶数。\n虽然GDBT有如此强大的特征组合能力，但GBDT容易产生过拟合，以及丢失了大量特征的数值信息，因此不能直接说GBDT的交叉能力强，效果就比FMM好，在模型的选择和调试上，永远都是多种因素综合作用的结果。\n五.LS-PLM——阿里巴巴曾经的主流推荐模型 LS-PLM(Large Scale Piece-wise Linear Model,大规模分段线性模型)虽然在2017年才被阿里巴巴公之于众，但其实早在2012年，它就是阿里巴巴主流的推荐模型，并在深度学习模型提出之前长时间应用于阿里巴巴的各类广告场景。LS-PLM的结构与三层神经网络极其相似，在深度学习来临的前夜，可以将它看作推荐系统领域连接两个时代的节点。\nLS-PLM，又被称为MLR(Mixed Logistic Regression，混合逻辑回归)，它在逻辑回归的基础上采用分而治之的思想，先对样本分片，再在样本分片中引用逻辑回归进行预估，其灵感来自对广告推荐领域样本特点的观察。为了让CTR模型对不同用户群体、不同使用场景更有针对性，其采用的方法是先对全量样本进行聚类，再对每个分类施以逻辑回归模型进行CTR预估。\n$f(x)= \\sum ^m _{i=1} \\pi_i(x) \\cdot \\eta_i(x) = \\sum ^m {i=1} \\cfrac {e^{\\mu_i \\cdot x}}{\\sum^m{j=1} e^{\\mu_j \\cdot x}} \\cdot \\cfrac {1}{1+e^{-w_i \\cdot x}} $\n先用聚类函数$\\pi$对样本进行分类(这里的$\\pi$采用了$softmax$函数对样本进行多分类)，这个样本对每个分类都有一个概率值，这些值的和为1($softmax$的性质)。再用LR模型计算每个切片（类）的CTR，然后求加权CTR和。其中的超参数“分片数”m可以较好地平衡模型，当m=1时，LS-PLM就退化为普通LR模型，m越大，模型的拟合能力越强。但与此同时，模型参数规模也随m的增长而线性增长，模型收敛所需的训练样本也随之增长。在实践中，阿里巴巴给出的m的经验值为12.\nLS-PLM模型适用于工业级的推荐、广告等大规模稀疏数据的场景，主要是有以下两个优势①端到端的非线性学习能力②模型的稀疏性强（L1范数比L2范数更容易产生稀疏解），部署更加轻量级。\n从深度学习角度重新审视LS-PLM模型，LS-PLM模型可以看作一个加入了注意力(Attention)机制的三层神经网络模型，其中输入层是样本的特征向量，中间层是由m个神经元组成的隐层，其中m是分片的个数，对于一个CTR预估模型，LS-PLM的最后一层自然是由单一神经元组成的输出层。那么，注意力机制又是哪里应用的呢？其实是在隐层和输出层之间，神经元之间的权重是由分片函数得出的注意力得分来确定的，也就是说，样本属于哪个分片的概率就是其注意力得分。\n传统推荐模型总结    模型名称 基本原理 特点 局限性     协同过滤 根据用户的行为历史生成用户-物品共现矩阵，利用用户相似性和物品相似性进行推荐 原理简单、直接，应用广泛 泛化能力差，处理稀疏矩阵的能力差，推荐结果的头部效应明显   矩阵分解 将协同过滤算法中的共现矩阵分解为用户矩阵和物品矩阵，利用用户隐向量和物品隐向量的内积进行排序并推荐 相较协同过滤，泛化能力有所增强，对稀疏矩阵的处理能力有所增强 除了用户历史行为数据，难以利用 其他用户、物品特征及上下文特征   逻辑回归 将推荐问题转换成类似CTR预估的二分类问题，将用户、物品、上下文等不同特征转换成特征向量，再按照预估CTR进行排序并推荐 能够融合多种类型的不同特征 模型不具备特征组合能力，表达能力较差   FM 再逻辑回归的基础上，再模型中假如二阶特征交叉部分，为每一维特征训练得到相应特征隐向量，通过隐向量的内积运算得到交叉特征权重 相比逻辑回归，具备了二阶特征交叉能力，模型的表达能力有所增强 由于组合爆炸问题的限制，模型不易扩展到三阶特征交叉阶段   FFM 在FM模型的基础上，加入“特征域”的概念，使每个特征在与不同域的特征交叉时采用不同的隐向量 相比FM，进一步加强了特征交叉能力 模型的训练开销达到了O(n2)的量级，训练开销较大   GBDT+LR 利用GBDT进行“自动化”的特征组合，将原始特征向量转换成离散型特征向量，并输入逻辑回归模型，进行最终的CTR预估 特征工程模型化，使模型具备了更高阶特征组合的能力 无法进行完全的并行训练，模型更新所需的训练时长较长   LS-PLM 首先对样本进行“分片”，在每个“分片”内部构建逻辑回归模型，将每个样本的各个“分片”概率与逻辑回归的得分进行加权平均，得到最终的预估值 模型结构类似三层神经网络，具备了较强的表达能力 模型结构相比深度学习模型仍比较简单，有进一步提高的空间    2006年，矩阵分解的技术成功应用在推荐系统领域，其隐向量的思想与深度学习中Embedding技术的思路一脉相承；2010年，FM被提出，特征交叉的概念被引入推荐模型，其核心思想——特征交叉的思路也将在深度学习模型中被发扬光大；2012年，LS-PLM在阿里巴巴大规模应用，其结构已经非常接近三层神经网络；2014年，Facebook用GBDT自动化处理特征，揭开了特征工程模型化的篇章。\n另外，Alex Krizhevsky站在Geoffrey Hinton、Yann LeCun、Yoshua Bengio等大师的肩膀上，于2012年提出了引爆整个深度学习浪潮的AlexNet，将深度学习的大幕正式拉开，其应用快速地从图像扩展到语音，再到自然语言处理领域，推荐系统领域也必然紧随其后，投入深度学习的大潮之中。\n从2016年开始，随着FNN、Wide\u0026amp;Deep、Deep Crossing等一大批优秀的推荐模型架构的提出，深度学习模型逐渐席卷推荐和广告领域，成为新一代推荐模型当之无愧的主流。\n深度学习在推荐系统中的应用 随着微软的Deep Crossing，谷歌的Wide\u0026amp;Deep，以及FNN、PNN等一大批优秀的深度学习推荐模型在2016年被提出，推荐系统和计算广告领域全面进入深度学习时代。深度学习时代主要在以下两方面取得重大进展：\n①深度学习模型的表达能力更强，能够挖掘出更多数据中潜藏的模式\n②深度学习的模型结构非常灵活，能够根据业务场景和数据特点，灵活调整模型结构，使模型与应用场景完美契合\n沿着特征工程自动化的思路，深度学习模型从PNN一路走来，经过了Wide\u0026amp;Deep、Deep\u0026amp;Cross、FNN、DeepFM、NFM等模型，进行了大量的、基于不同特征互操作思路的尝试。但特征工程的思路走到这里已经穷尽了可能的尝试，模型进一步提升的空间很小，这也是这类模型的局限性所在。从这之后，越来越多的深度学习推荐模型开始探索更多”结构“上的尝试，诸如注意力机制、序列模型、强化学习等在其他领域大放异彩的模型结构也逐渐进入推荐系统领域，并且在推荐模型的效果提升上成果显著。\n一.AutoRec——单隐层神经网络推荐模型 AutoRec在2015年由澳大利亚国立大学提出。它将自编码器（AutoEncoder）的思想和协同过滤结合，提出了一种单隐层神经网络推荐模型。\n自编码器的原理类似于协同过滤中的共现矩阵，主成分分析等，相当于在重建函数$h(r； \\theta)$中存储了所有数据向量的“精华”。\n如上图，AutoRec是一个非常标准的三层神经网络，紫色单隐层的数量k远小于输入/输出评分向量的维度m，所以可以达到“泛化”的效果。\n重建函数的具体形式：\n$h(r; \\theta) = f(W \\cdot g(Vr+\\mu) + b)$\n其中，$f(\\cdot)$，$g(\\cdot)$分别为输出层神经元和隐层神经元的激活函数。\n为防止重构函数的过拟合，在加入L2正则化项后，AutoRec目标函数的具体形式：\n$\\underset {\\theta}{min} \\sum^m_{j=1}||r^{(i)}-h(r^{(i)} ; \\theta)||^2_O + \\cfrac{\\lambda}{2} \\cdot(||W||^2_F+||V||^2_F)$\n由于AutoRec是一个非常标准的三层神经网络，模型的训练利用梯度反向传播即可完成。\nAutoRec与协同过滤一样，有基于Item的I-AutoRec（Item based AutoRec），当输入物品$i$的评分向量$r^{(i)}$时，模型的输出向量$h(r^{(i)}; \\theta)$就是所有用户对$i$的评分预测。通过遍历，就可以得到一个用户$u$对所有物品的评分预测，进而根据评分预测排序得到推荐列表。U-AutoRec（User based AutoRec）相比I-AutoRec的优势在于仅需输入一次目标用户的用户向量，就可以重建用户对所有物品的评分向量，劣势是用户向量的稀疏性可能会影响模型效果。\n总体来说，AutoRec使用一个单隐层的AutoEncoder泛化用户或物品评分，有泛化和表达能力但是并不足。在模型结构上，AutoRec模型和后来的词向量模型(Word2vec)完全一致，但优化目标和训练方法有所不同。\n二.Deep Crossing——经典的深度学习框架 Deep Crossing由微软在2016年提出，应用在搜索引擎Bing的搜索广告推荐场景。广告点击率则作为Deep Crossing模型的优化目标，即CTR模型。\nDeep Crossing模型特征可以分为三类：一类是可以被处理成one-hot或multi-hot的类别型特征，一类是数值型特征，一类是需要进一步处理的特征，包括广告计划（campaign）、曝光样例（impression）、点击样例（click）等。\n为了完成端到端的训练，Deep Crossing解决了以下三个问题：\n①稀疏特征稠密化——Embedding层以经典的全连接层（Fully Connected Layer）结构为主，另有衍生出的Word2vec、Graph Embedding等。一般来说，Embedding向量的维度应远小于原始的稀疏特征向量，大多几十到上百维。数值型特征不需要Embedding，直接进入Stacking层。\n②自动交叉组合——Multiple Residual Units，相比标准的以感知机为基本单元的神经网络，Deep Crossing采用了多层残差网络（Multi-Layer Residual Network）作为MLP的具体实现。\n③输出层达成CTR预测的目标——Scoring层采用sigmoid（图像分类等多分类问题多采用softmax）\nStacking层比较简单，是把不同的Embedding特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量，该层通常也成为连接层（concatenate）。\n残差神经网络\n最著名的残差网络是在ImageNet大赛中由微软研究员何凯明提出的152层残差网络。推荐模型中的应用也是残差网络首次在图像识别领域之外的成功推广。残差神经网络就是由残差单元（Residual Unit）组成的神经网络，\n上面的残差单元与传统感知机的区别主要有两个不同：\n①输入经过两层以ReLU为激活函数的全连接层后，生成输出向量。\n②输入可以通过一个短路（shortcut）通路直接与输出向量进行元素加（element-wise plus）操作，生成最终的输出向量。\n此时，残差单元中的两层ReLU网络其实拟合的是输出和输入之间的残差（$x^o-x^i$），这就是残差神经网络名称的由来。\n残差神经网络的诞生主要为了解决两个问题：\n①神经网络加深后，容易产生过拟合。残差网络中，由于有输入向量短路的存在，很多时候可以越过两层ReLU网络，减少过拟合的发生。\n②残差单元使用ReLU激活函数取代sigmoid，越靠近0梯度越大。并且输入向量短路相当于直接把梯度毫无变化地传递到下一层，这也使残差网络收敛速度更快。\n三.NeuralCF——CF与深度学习的结合 Embedding层的主要作用是将稀疏向量转换成稠密向量，那么矩阵分解层的用户隐向量和物品隐向量完全可以看作一种Embedding方法。而用户隐向量和物品隐向量的内积操作则可以看作Scoring层。在实际使用矩阵分解来训练和评估模型的过程中，往往会发现模型容易处于欠拟合状态。究其原因是因为矩阵分解的模型结构相对比较简单，特别是Scoring层，无法对优化目标进行有效的拟合。这就要求模型有更强的表达能力，在此动机的启发下，新加坡国立大学的研究人员提出了NeuralCF模型。\nNeuralCF用“多层神经网络+输出层”的结构替代了矩阵分解中简单的内积操作。这样做的收益是直观的，一是让用户向量和物品向量做更充分的交叉，得到更多有价值的特征组合信息；二是引入更多的非线性特征，让模型的表达能力更强。\n以此类推，事实上，用户和物品向量的互操作层可以被任意的互操作形式所代替，这就是所谓的“广义矩阵分解”模型（Generalized Matrix Factorization）。例如，Scoring元素积+输出层逻辑回归。再进一步，可以把不同互操作网络得到的特征向量拼接起来，交由输出层进行拟合。NeuralCF的论文中给出了整合两个网络的例子：\nNeuralCF模型实际上提出了一个模型框架，它基于 用户向量和物品向量这两个Embedding层，利用不同的互操作层进行特征的交叉组合，并且可以灵活地进行不同互操作层的拼接。从这里可以看出深度学习构建推荐模型的优势——利用神经网络理论上能够拟合任意函数的能力，灵活地组合不同的特征，按需增加或减少模型的复杂度。\n在实践中要注意：并不是模型越复杂、特征越多越好。一是要防止过拟合的风险，而是往往需要更多数据和更长的训练时间才能使复杂的模型收敛，这需要算法工程师在模型的实用性、实时性和效果之间进行权衡。\nNeuralCF模型也存在局限性。由于是基于协同过滤的思想进行构造的，所以NeuralCF模型并没有引入更多其他类型的特征，这在实际应用中无疑浪费了其他有价值的信息。此外，对于模型中互操作的种类并没有做进一步的探究和说明。这就需要后来者进行更深入的探索。\n四.PNN——加强特征交叉能力 NeuralCF只提到了用户向量和物品向量两组特征向量，如果加入多组特征向量又该如何设计特征交互的方法呢？2016年，上海交通大学提出的PNN模型，给出了特征交互方式的几种设计思路。\n相比Deep Crossing，PNN模型在输入、Embedding层、多层神经网络，以及最终的输出层部分并没有结构上的不同，唯一的区别在于PNN模型用乘积层（Product Layer）代替了Deep Crossing模型中的Stacking层。也就是说，不同特征的Embedding向量不再是简单的拼接，而是用Product操作进行两两相交，更有针对性地获取特征之间的交叉信息。\nPNN的Product层的多种特征交叉方式 PNN模型对于深度学习的创新主要在于乘积层的引入。具体地说，PNN模型的乘积层由线性操作部分（上图z部分，对各特征向量进行线性拼接）和乘积操作部分（上图p部分）。其中，乘积特征交叉部分又分为内积操作和外积操作，其中内积操作的PNN模型被称为IPNN（Inner Product-based Neural Network），使用外积操作的PNN模型被称为OPNN（Outer Product-based Neural Network）。\n其中外积操作，$g_{outer}(f_i,f_j) = f_if_j^T$，外积互操作生成的是特征向量$f_i,f_j$各维度两两交叉而成的一个$M \\times M$的方形矩阵（其中$M$是输入向量的维度）。这样的外积操作无疑会将问题的复杂度从$M$提升到$M^2$，为了一定程度上减少训练负担，PNN模型的论文中介绍了一种降维的方法，就是把所有两两特征Embedding向量外积互操作结果叠加（Superposition），形成一个叠加外积操作矩阵$p$：\n$p=\\sum^N_{i=1} \\sum^N_{j=1}g_{outer}(f_i,f_j) = \\sum^N_{i=1} \\sum^N_{j=1} f_if_j^T=f_ \\sum f_\\sum^T,f_\\sum=\\sum^N_{i=1}f_i$\n从公式看，叠加矩阵$p$的最终形式类似于让所有特征Embedding向量通过一个平均池化层（Average Pooling）后，再进行外积互操作。在实际应用中，还应对平均池化操作谨慎对待。因为把不同特征对应维度进行平均，实际上是假设不同特征的对应维度有类似含义。但显然，年龄和地域两个特征在经过各自的Embedding后，两者的Embedding向量不在一个向量空间中，显然不具备任何可比性。这是做平均池化，会模糊很多有价值的信息。平均池化的操作经常发生在同类Embedding上，例如，将用户浏览过的多个物品的Embedding进行平均。因此，PNN模型的外积池化操作也需要谨慎，在训练效率和模型效果上进行权衡。\n事实上，PNN模型在对特征的线性和乘积操作后，并没有把结果直接送入上层的$L_1$全连接层，而是在乘积层内部又进行了局部全连接的转换，分别将线性部分$z$，乘积部分$p$映射成了$D_1$维的输入向量$l_z$和$l_p$（$D_1$为$L_1$隐层的神经元数量），再将$l_z$和$l_p$叠加，输入$L_2$隐层。这部分操作不具备创新性，并且可以被其他转换操作完全替代，因此不再详细介绍。\nPNN的结构特点在于强调了特征Embedding向量之间的交叉方式是多样化的，相比于简单的交由全连接层进行无差别化的处理，PNN模型定义的内积和外积操作显然更有针对性地强调了不同特征之间的交互，从而让模型更容易捕获特征交叉信息。\n但PNN模型同样存在局限性，例如在外积操作时，为了优化$M \\times M$的训练效率，对所有特征进行无差别交叉（平均池化），这一定程度上忽略了原始特征向量中包含的有价值信息。如何综合原始特征及交叉特征，让特征交叉的方式更加高效，后续的Wide\u0026amp;Deep模型和基于FM的各类深度学习模型将给出他们的解决方案。\n五.Wide\u0026amp;Deep——记忆能力和泛化能力的综合 谷歌于2016年提出Wide\u0026amp;Deep模型，由单层的Wide部分和多层的Deep部分组成的混合模型。其中，Wide部分的作用是让模型具有较强的“记忆能力”（memorization）；Deep部分的主要作用是让模型具有“泛化能力”（generalization），使模型兼具了逻辑回归（简单模型的记忆能力强）和深度神经网络（深度学习网络不断进行的交叉处理，会减弱记忆能力，但会拥有泛化能力）的优点——能够快速处理并记忆大量历史特征，并且具有强大的表达能力，不仅在当时迅速成为业界争相应用的主流模型，而且衍生出了大量以Wide\u0026amp;Deep模型为基础结构的混合模型，影响力一直延续至今。\nWide\u0026amp;Deep模型把单输入层的Wide部分与由Embedding层和多隐层组成的Deep部分连接起来，一起输入最终的输出层（逻辑回归）。其中Wide部分善于处理大量稀疏特征，而Deep部分善于挖掘特征背后的数据模式。这种把不同特征使用不同处理方法的组合模型，就需要对业务场景的深刻理解。从下图可以详细地了解到Google Play的推荐团队到底将哪些特征作为Deep输入，哪些作为Wide部分输入。\nDeep部分输入全量的特征向量，拼接成1200维的Embedding向量，再经过3层ReLU全连接层，最终输入LogLoss输出层。\nWide部分输入仅仅是已安装应用和曝光应用两类特征，其中已安装应用代表用户的历史行为，而曝光应用代表当前的待推荐应用。选择这两类特征的原因是充分发挥Wide部分的记忆能力，使用简单模型善于记忆用户行为特征中的信息，并根据此类信息直接影响推荐结果。\nWide部分组合“已安装应用”和”曝光应用“两个特征的函数被称为交叉积变换（Cross Product Transformation）函数，其形式化定义如：\n$\\phi_k(X)= \\prod_{i=1}^{d}x_i^{c_ki}$ $c_{ki} \\in {0,1}$\n$c_{ki}$是一个布尔变量，当第$i$个特征属于第$k$个组合特征时，$c_{ki}$的值为1，否则为0；$x_i$是第$i$​个特征的值。例如，对于”AND(user_installed_app=netflix, impression_app=pandora)“这个组合特征来说，只有当\u0026quot;user_installed_app=netflix\u0026quot;和”impression_app=pandora“这两个特征同时为1时，其对应的交叉积变换层的结果才为1，否则为0。\n在通过交叉积变换层操作完成特征组合之后，Wide部分将组合特征输入最终的LogLoss输出层，与Deep部分的输出一同参与最后的目标拟合，完成Wide与Deep的融合部分。\nWide\u0026amp;Deep开启了不同网络融合的新思路，日后有比较经典的2017年由斯坦福大学和谷歌的研究人员提出的Deep\u0026amp;Crossing模型。其主要思路是使用Cross网络替代原来的Wide部分。\n使用Cross网络的目的是增加特征之间的交互力度，使用多交叉层（Cross layer）对输入向量进行特征交叉。假设第$l$层交叉层的输出向量为$x_l$，那么第$l+1$层的输出向量：\n$x_{l+1}=x_0x_l^TW_l + b_l + x_l$\n可以看到，交叉层操作的二阶部分类似于PNN模型中的外积操作，在此基础上增加了外积操作的权重向量$w_l$，以及原输入向量$x_l$和偏置向量$b_l$。\n可以看出，Cross层在参数方面是比较”克制“的，每一层仅增加了一个$n$维的权重向量$w_l$（n维输入向量维度），并且在每一层均保留了输入向量，因此输入与输出之间变化不会非常明显。由多层交叉层组成的Cross网络在Wide\u0026amp;Deep模型中的Wide部分的基础上进行特征的自动化交叉，避免了很多基于业务理解的人工特征组合。同Wide\u0026amp;Deep模型一样，Deep\u0026amp;Cross模型的Deep部分相比Cross部分表达能力更强，使模型具备更强的非线性学习能力。\nWide\u0026amp;Deep模型的影响力无疑是巨大的，不仅是其本身成功应用于多家一线互联网公司，而且其后续的改进创新工作也延续至今。事实上，DeepFM、NFM等模型都可以看成Wide\u0026amp;Deep模型的延伸：\nWide\u0026amp;Deep模型能够取得成功的关键在于：\n①抓住了业务问题的本质特点，能够融合传统模型记忆能力和深度学习模型泛化能力的优势\n②模型的结构并不复杂，易于工程实现、训练和上线，这加速了业界推广\n也正是从Wide\u0026amp;Deep模型之后，越来越多的模型结构被加入推荐模型中，深度学习模型的结构开始朝着多样化、复杂化的方向发展。\n六.FM与深度学习的结合 FNN——用FM的隐向量完成Embedding层初始化 FNN由伦敦大学学院的研究人员于2016年提出，以FM改进Embedding层的Deep crossing模型，用FM模型训练好的各特征向量初始化Embedding层的参数代替随机初始化，相当于在初始化神经网络参数时，已经引入了有价值的先验信息。也就是说，神经网络训练的起点更接近目标最优点，自然加速了整个神经网络的收敛过程。\n一般情况下，模型的收敛速度往往受限于Embedding层。主要有两个原因：①Embedding层的参数量巨大。假设输入层维度维100,000，Embedding层输出维度为32，上层再加5层32维的全连接层，最后输出层维度为10，那么输入层到Embedding层的参数数量是$32 \\times100,000=3,200,000$，其余所有层的参数总数是$(32 \\times 32) \\times4+32\\times 10 = 4416$​。此时Embedding层参数占比99.86%。这就导致大部分的训练时间和计算开销都被Embedding层占据。②由于输入向量过于稀疏，在随机梯度下降时，只有与非零特征相连的Embedding层权重会被更新，这进一步降低了Embedding层的收敛速度。\n需要说明的是，在训练FM的过程中，并没有对特征域进行区分，但在FNN模型中，特征被分成了不同特征域，因此每个特征域具有对应的Embedding层，并且每个特征域Embedding的维度都应与FM隐向量维度保持一致。\nDeepFM——用FM代替Wide部分 FNN把FM的结果作为初始化权重，并没有调整模型结构。而2017年由哈尔滨工业大学和华为公司联合提出的DeepFM则将FM模型与Wide\u0026amp;Deep模型整合：\nFM部分与深度神经网络部分共享相同的Embedding层。左侧FM部分对不同的特征域的Embedding进行两两交叉，也就是将Embedding向量当作原FM中的特征隐向量。最后将FM的输出与Deep部分的输出一同输入最后的输出层，参与最后的目标拟合。\nDeepFM与Deep\u0026amp;Cross模型完全一致，唯一的不同在于Deep\u0026amp;Cross利用多层Cross网络进行特征组合，而DeepFM模型利用FM进行特征组合。当然，具体的应用效果还需要通过实验进行比较。\nNFM——FM的神经网络化尝试 无论是FM还是FFM，归根结底是一个二阶特征交叉的模型，受组合爆炸问题的困扰，FM几乎不可能扩展到三阶以上，这就不可避免地限制了FM模型的表达能力。2017年，新加坡国立大学的研究人员进行了这方面的尝试，提出了NFM模型。\nNFM模型的主要思路是用一个表达能力更强的函数替代原FM中二阶隐向量内积的部分：\n$\\hat{y}{NFM}(x)=w_0 + \\sum^N{i=1}w_ix_i+f(x)$\n传统机器学习可以用来拟合$f(x)$一个表达能力更强的函数，但是进入深度学习时代后，由于深度学习网络理论上有拟合任何复杂函数的能力，$f(x)$的构造工作可以交由某个深度学习网络来完成，并通过梯度反向传播来学习。\nNFM网络架构的特征非常明显，就是在Embedding和神经网络之间加入特征交叉池化层（Bi-Interaction Pooling Layer）：\n$f_{BI}(V_x)=\\sum^n_{i=1} \\sum^n_{j=i+1}(x_iv_i)\\bigodot(x_jv_j)$\n其中，$\\bigodot$代表元素积操作，其中第k维的操作：\n$(v_i\\bigodot v_j)k=v{ik}v_{jk}$\n在进行两两元素积操作后，对交叉特征向量取和，得到池化层的输出向量。再把该向量输入上层的多层全连接神经网络，进行进一步的交叉。\n上图的NFM省略了一阶部分，如果把一阶部分视为一个线性模型，那么NFM的架构也可以视为Wide\u0026amp;Deep模型的进化。相比原始的Wide\u0026amp;Deep模型，NFM模型对其Deep部分加入了特征交叉池化层，加强了特征交叉。这是理解NFM模型的另一个角度。\n七.注意力机制在推荐模型中的应用 “注意力机制”来源于人类最自然的选择性注意的习惯，从2017年开始，推荐领域也开始尝试将注意力机制引入模型之中，这其中影响力较大的工作是由浙江大学提出的AFM和由阿里巴巴提出的DIN。这一机制堆深度学习推荐系统的启发是重大的，使得其更接近用户真实的思考过程。\nAFM——引入注意力机制的NFM 在NFM模型中，不同域的特征Embedding向量经过特征交叉池化层的交叉，将各交叉特征向量进行“加和”，输入最后由多层神经网络组成的输出层。AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络（Attention Net）实现的。AFM的模型结构图：\n注意力网络的作用是为每一个交叉特征提供权重，也就是注意力得分。\n同NFM一样，AFM的特征交叉过程同样采用了元素积操作：\n$f_{PI}(\\varepsilon){(v_i\\bigodot v_j)x_ix_j}_{(i,j)\\in R_x}$\n引入注意力得分后的池化过程：\n$f_{Att}(f_{PI}(\\varepsilon)) = \\sum_{(i,j) \\in R_x} a_{ij}(v_i\\bigodot v_j)x_ix_j$​\n对注意力的分$a_{ij}$来说，最简单的方法就是用一个权重参数来表示，但为了防止交叉特征数据稀疏问题带来的权重参数难以收敛，AFM模型使用了一个在两两特征交叉层（Pair-wise Interaction Layer）和池化层之间的注意力网络来生成注意力得分。 该注意力网络的结构是一个简单的单全连接层加softmax输出层的结构：\n$a^`_{ij}=h^TReLU(W(v_i\\bigodot v_j)x_ix_j+b)$\n$a_{ij}=\\cfrac{exp(a^_{ij})}{\\sum_{(i,j)\\in R_x} exp(a^_{ij})}$\n其中需要学习的参数是特征交叉层到注意力网络全连接层的权重矩阵$W$，偏置向量$b$，以及全连接层到softmax输出层的权重向量$h$。注意力网络将与整个模型一起参与梯度反向传播的学习过程，得到最终的权重参数。\nAFM是研究人员从改进模型角度进行的一次尝试。而阿里巴巴引入注意力机制是基于其对业务观察的一次模型改进，下面介绍阿里巴巴在业界非常知名的推荐模型DIN。\nDIN——引入注意力机制的神经学习网络 它的应用场景是阿里巴巴的电商广告推荐，在计算一个用户$u$是否点击一个广告$a$时，模型的输入特征自然分为两大部分：一部分是用户$u$的特征组，另一部分是候选广告$a$的特征组。无论是用户还是广告，都含有两个非常重要的特征——商品id(good_id)和商铺id(shop_id)。用户特征里的商品特征是一个序列，代表用户曾点击过的商品合集，商铺id同理；而广告特征里的商品id和商铺id就是广告对应的商品id和商铺id（阿里巴巴平台上的广告大部分是参与推广计划的商品）。\n在原来的基础模型中，这些特征进行简单的平均池化操作就后就进入上层神经网络进行下一步训练，序列中的商品既没有区分重要程度，也和广告特征中的商品id没有关系。\n然而事实上，广告特征和用户特征的关联程度是非常强的，假设广告中的商品是键盘，那么用户点击商品序列中的不同商品id：鼠标、T恤和洗面奶。从常识出发，鼠标这个历史商品对预测键盘广告的点击率的重要程度远大于后两者。从模型角度，基于不同特征的注意力理应不同，而且“注意力得分”的计算理应与广告特征有相关性。\n模型中的注意力的强弱，利用候选商品和历史行为商品之间的相关性计算出一个权重，这个权重就代表了“注意力”强弱：\n$V_u=f(V_a)=\\sum ^N_{i=1}w_i \\cdot V_i=\\sum ^N_{i=1}g(V_i,V_a) \\cdot V_i$\n其中$V_u$是用户的Embedding向量，$V_a$是候选广告商品的Embedding向量，$V_i$是用户$u$的第$i$次行为的Embedding向量。这里用户的行为就是浏览商店或店铺，因此行为的Embedding向量就是那次浏览的商品或店铺的Embedding向量。$g(V_i,V_a)$即注意力得分函数采用一个注意力激活单元（activation unit）。其本质上也是一个小的神经网络，其具体结构如上图右上角。可以看出，激活单元的输入层是两个Embedding向量，经过元素减（element-wise minus）操作后，与原Embedding向量一同连接后形成全连接层的输入，最后通过单神经元输出层生成注意力得分。\nDIEN——序列模型与推荐系统的结合 从“注意力机制”开始，越来越多对深度学习模型结构的改进是基于对用户行为的深刻观察而得出。DIEN基于DIN，创新在于用序列模型模拟了用户兴趣的进化过程。序列信息的重要性在于：①加强了最近行为对下次行为预测的影响。②能够学习到购买趋势的信息，如果某个转移概率在全局统计意义上足够高——购买过篮球鞋后购买机械键盘的概率，那么在用户购买篮球鞋时，推荐机械键盘也会成为一个不错的选择。直观上，两者的用户群体很有可能是一致的。\n如果失去序列信息，推荐模型则是基于用户购买历史的综合推荐，而不是针对“下一次购买”的推荐，显然，从业务角度看，后者才是推荐系统正确的推荐目标。\n其中兴趣进化网络分为三层，从下至上依次是：\n①行为序列层（Behavior Layer，浅绿色部分）：其主要作用是把原始的id类行为序列转换成Embedding行为序列。\n②兴趣抽取层（Interest Extractor Layer，米黄色部分）：其主要作用是通过模拟用户兴趣迁移过程，抽取用户兴趣。\n③兴趣进化层（Interest Evolving Layer，浅红色部分）：其主要作用是通过在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程。\n在兴趣进化网络中，行为序列层的结构与普通的Embedding层是一致的，模拟用户兴趣进化的关键在于“兴趣抽取层”和“兴趣进化层”。\n兴趣抽取层的基本结构是GRU（Gated Recurrent Unit）\n本章介绍了以下模型，但深度学习推荐模型从没停下他前进的脚步。从阿里巴巴的多模态、多目标的深度学习模型，到Youtube基于session的推荐系统，再到Airbnb使用Embedding技术构建的搜索推荐模型，深度学习推荐模型不仅进化速度越来越快，而且应用场景也越来越广。在之后的章节中，笔者会从不同的角度出发，介绍深度学习模型再推荐系统中的应用，也希望读者可以在本章的知识结构上，跟踪最新的深度学习推荐模型进展。\n   模型名称 基本原理 特点 局限性     AutoRec 基于自编码器，对用户或者物品进行编码，利用自编码器的泛化能力进行推荐 单隐层神经网络结构简单，可实现快速训练和部署 表达能力较差   Deep Crossing 利用“Embedding层+多隐层+输出层”的经典深度学习框架，预完成特征的自动深度交叉 经典的深度学习推荐模型框架 利用全连接隐层进行特征交叉，针对性不强   NeuralCF 将传统的矩阵分解中用户向量和物品向量的点积操作，换成由神经网络代替的互操作 表达能力加强版的矩阵分解模型 只使用了用户和物品的id特征，没有加入更多其它特征   PNN 针对不同特征域之间的交叉操作，定义“内积”“外积”等多种积操作 在经典深度学习框架上提高特征交叉能力 “外积”操作进行了近似化，一定程度上影响了其表达能力   Wide\u0026amp;Deep 利用Wide部分加强模型的“记忆能力”，利用Deep部分加强模型的“泛化能力” 开创了组合模型的构造方法，对深度学习推荐模型的后续发真产生重大影响 Wide部分需要人工进行特征组合的筛选   Deep\u0026amp;Cross 用Cross网络替代Wide\u0026amp;Deep模型中的Wide部分 解决了Wide\u0026amp;Deep模型人工组合特征的问题 Cross网络的复杂度较高   FNN 利用FM的参数来初始化深度神经网络的Embedding层参数 利用FM初始化参数，加快整个网络的收敛速度 模型的主结构比较简单，没有针对性的特征交叉层   DeepFM 在Wide\u0026amp;Deep模型的基础上，用FM替代原来的线性Wide部分 加强了Wide部分的特征交叉能力 与经典的Wide\u0026amp;Deep模型相比，结构差别不明显   NFM 用神经网络代替FM中二阶隐向量交叉的操作 相比FM,NFM的表达能力和特征交叉能力更强 与PNN模型的结构非常相似   AFM 在FM的基础上，在二阶隐向量交叉的基础上对每个交叉结果加入了注意力得分，并使用注意力网络学习注意力得分 不同交叉特征的重要性不同 注意力网络的训练过程比较复杂   DIN 在传统深度学习推荐模型的基础上引入注意力机制，并利用用户行为历史物品和目标广告物品的相关性计算注意力得分 根据广告物品的不同，进行更有针对性的推荐 并没有充分利用除“历史行为”以外的其他特征   DIEN 将序列模型与深度学习推荐模型结合，使用序列模型模拟用户的兴趣进化过程 序列模型增强了系统对用户兴趣变迁的表达能力，使推荐系统开始考虑时间相关的行为序列中包含的有价值信息 序列模型的训练复杂，线上服务的延迟较长，需要进行工程上的优化   DRN 将强化学习的思路应用于推荐系统，进行推荐模型的线上实时学习和更新 模型对数据实时性的利用能力大大加强 线上部分较复杂，工程实现难度较大    \\未完待续。。。:)\n","id":0,"section":"posts","summary":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和","tags":["“推荐系统”","读后感"],"title":"1小时读懂《深度学习推荐系统》","uri":"https://biofrostyy.github.io/2021/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","year":"2021"},{"content":"终于又是个周末啦，最近每天工作任务重，又赶上和毕业生大部队的校招生培训，真的是忙的团团转，毕业正式进入职场也有半年了，参加了一些项目也学到了很多，工作时候的成就感真的很令人快乐！\n虽然忙但是非常充实的六七月，首先是七月份终于把leetcode捡起来了，虽然没有校招刷题时量那么大，但是至少不要让自己手生。\n然后就是由于工作的原因开始学习spark，我本人对工程方面的知识确实有些薄弱，这次打算从工程环境搭建、spark基础组件、spark分布式架构原理和性能优化以及函数式编程四个方面系统学习spark知识。\n除了工作和学习，最近这几个月我也有好好在运动哦，下周打算去参加网球课，为了变成梦想中的 \u0026lsquo;fit shape\u0026rsquo; 努力。当然吃也没落下，虽然深圳号称美食荒漠，但是和家人朋友们也探索了好多不错的店。感觉有时间可以搞一个探店日志哈哈\n7月份的最后一天啦，继续快乐拼搏下去吧。\n","id":1,"section":"posts","summary":"终于又是个周末啦，最近每天工作任务重，又赶上和毕业生大部队的校招生培训，真的是忙的团团转，毕业正式进入职场也有半年了，参加了一些项目也学到了","tags":["“MyEveryday'"],"title":"MyEveryday","uri":"https://biofrostyy.github.io/2021/07/myeveryday202107731/","year":"2021"},{"content":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和谐统一的。例如YouTube的用户体验和公司利益（时长越长广告曝光越多）在“观看时长”这一点上达成了一致。\n下图是推荐系统的技术架构示意图。其中数据部分为融合了数据离线批处理、实时流处理的数据流框架；算法和模型部分则为集训练(training)、评估(evaluation)、部署(deployment)、线上推断(online inference)为一体的模型框架。\n推荐系统的进化之路 幸运的是，我们开始做这项工作时，作为用户画像组，我们有着丰富的用户画像。\n我们使用线上的实时特征包括拖寄物、上下文、流向等。\n第二版，我们增加了redis的历史购买。\n再后来，因为画像数据太多，我们分析了之后，对用户画像（交叉）进行了客群分析，使用新客群作为分类特征。\n再后来，增加了再次购买率特征，使点击率增加了1%\n再后来，为了解决长尾，我们将高频与长尾分开，max(高频)低于0.5时，进入长尾规则判断，类似冷启动阶段，使用基于矩阵分解的协同过滤。\n再后来，加入组合推荐，apriori分析，概率模型\n再后来，缺失走兜底规则过多，包装服务过多\n再后来，准时保（流向/拖寄物符合）推荐过多，调整准时保位置，降低判断位置\n","id":2,"section":"posts","summary":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和","tags":["“推荐系统”","读后感"],"title":"深度学习推荐系统","uri":"https://biofrostyy.github.io/2021/07/%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/","year":"2021"},{"content":"   书名 读书日期 心得\u0026amp;读书笔记标签     上帝掷色子吗 2021.3.5\u0026mdash;\u0026gt;2021.4.11 高中的时候读到了一本量子计算机相关的书，现在已忘记书名，但是给我的震撼还是非常大的，所以特意pick了这本书，献给科学与神学🤣 记录于2021.3.27晚 p.313/473本书看到了一半，因为工作繁忙进度一度落后于后面两本机器学习相关著作，其中又穿插了一些相关的书，其中印象最深刻的是最近很火的半小时漫画科学史（两三个小时在书店看完了，说实话对于我这种历史盲超级有趣），前半部分看起来是高中物理化学的复习课，后面20世纪初那段绚烂的各路豪杰显神通的时代又让我格外震撼，暂时把自己算作多世界的投影理论的支持者😀，我认为科学至少应该是客观的，希望神秘的量子世界总有一天可以被客观的绝对的简单而完美的定理解答吧记录于2021.4.11午后 p.313/473阳光明媚的午后阳台躺椅上完成了这次量子之旅，作者花费了太多笔墨做比喻和感慨，信息密度有点低，不过一点都没有减少对我的震撼，对于一个个量子实验所提供的信息，各位勇士们各显神通提出各种对于宇宙奥秘的猜想，其中包括哥本哈根、多宇宙、隐变量、系综、GRW、退相干等等，至今仍未同时保留定域性和确定性，统一广义相对论和量子论。最后还见到了被Sheldon称为dead end并抛弃的老熟人string theory（宇宙的琴弦byB.格林）🤓但此书对于量子应用着墨不多，现在我脑袋里还环绕着超弦的具体理论、量子计算机，量子通信是什么原理这些问号，只能日后找相关书籍了解了   强化学习精要 核心算法与TensorFlow实现 2021.3.20 \u0026mdash;\u0026gt; 暂停于2021.4.5 记录于2021.4.5晚 p.61/308因为关注的VRP问题最新很多paper都开始应用强化学习（强化学习天然适合于这种输出与loss没有直接关系（不可导）的情况），所以选取了这本书了解强化学习。在工业应用中，强化学习还是更多应用于游戏行业，在VRP问题中的尝试应用暂时没有提升太多时间上的优化（找到更优的算子往往会伴随更长的学习时间），所以我在总体了解了强化学习的思想后暂停了对更深入的变体、调优等部分的了解，后续如果有其他相关工作实践，再重新开启吧 :)   美团机器学习实践 2021.3.22\u0026mdash;\u0026gt;2021.7.01 变成打工人之后，更加注重算法实践方面的经验（什么？业务方说要达到50毫秒级实时预测？🤣），了解了很多相关知识包括大规模机器学习、并行计算等，任重而道远啊🤓记录于2021.3.27晚 p.61/308干货还是很多的，因为最近工作中有在做CTR预测相关模型，所以很仔细的注意了其中的特征工程部分，总体读到了模型介绍部分，其中逻辑回归模型在学校时本来我觉得是个非常基础的算法，但其实实践中为了解决稀疏、大数据问题有非常多的变体，例如其中以FTRL为首的相关算法都让我非常感兴趣记录于2021.5.3劳动节清晨劳动节的劳动人民加班结束了TT，来书单看一看，发现自己这本书进度缓慢，主要因为最近工作真的忙到团团转，刚接触spark，xshell什么的，先看了本书16章算法工程，这个假期不出门玩的时候就老老实实充（gan）实（jin）自己（du）吧   给未来人类的终极12问 2021.4.11\u0026mdash;\u0026gt;2021.5.3 杜海涛同款😂记录于2021.5.3劳动节清晨早上起来把最后一个topic看完了，怎么说呢，简直就是抱着我就看看到底哪里好的想法看完了整本迷你书，整本书就是两个大佬的对话，有举例争辩也有相互支持，但是可能是因为按出版时间算此书已不算前沿，或者只有两个人主观的辩论信息量太少，总体来说体验并不是很好   labuladong的算法小抄 2021.4.11\u0026mdash;\u0026gt; 对于一个大数据从业者来说，工程能力很重要，此算法小抄配leetcode，药到病除;)   数学之美 2021.5.9\u0026mdash;\u0026gt;2021.6.10 本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，没有选择自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些相关应用。这是一本让你构建宏观体系的书，介绍人工智能运用在工业界的方方面面，让人们对这些问题的解决有一个“道”的框架理解，有趣的是，因为这种宏观的描绘，在读书中会有很多句子背后蕴藏的深刻理论会吸引你停下读书的脚步，对其进行更深的探索，这正是本书的乐趣所在。   消失的第13级台阶 2021.5.22\u0026mdash;-\u0026gt;2021.5.23 一本非常有人气的日系悬疑推理小说，本书更多聚焦在社会派的推理，直到中间部分才开始出现紧张而烧脑的案件推进情节，对于本格推理爱好者可能吸引不大，对死刑相关及当下社会的思考才是这本小说最注重且可贵的地方。   阿加莎·克里斯蒂 2021.5.29\u0026mdash;\u0026gt; 看过消失的第13级台阶之后，就很想看悬疑小说😐，我的青少年时期的悬疑推理小说主要来源是福尔摩斯，神探夏洛克（勉强算吧哈哈），蔡骏和各种在我初中书架上已经忘记名字的小说。但是阿加莎大名鼎鼎的著作，除了拍成电影的，我一部都没看过，得益于现在电子书籍的普及，我终于可以足不出户，开启这一段注定漫长（八十部）但又精彩的旅程   时间的形状·相对论史话 2021.5.25\u0026mdash;\u0026gt;2021.6.25 是《上帝掷色子吗》的完美补充，一起代表物理界的两朵乌云\u0026ndash;相对论与量子力学。在上下班的路上听作者的有声书讲解，还是很有趣的。   给忙碌者的天体物理 2021.6.3\u0026mdash;\u0026gt; 向⭐出发   深度学习推荐系统 2021.7.26\u0026mdash;\u0026gt; 王喆大佬是我在知乎上最喜欢的博主之一，他总能用最简单的语言精准的讲述，同时又带有自己丰富经验的智慧，让我受益匪浅。因为工作原因，需要系统使用推荐系统，所以购买了大佬的这本书，希望构建自己推荐系统方面完整的知识构架。    大数据之路：阿里巴巴大数据实践 阿里云天池大赛赛题解析机器学习 霍金三部曲 算法与数据中台 智能搜索和推荐系统 果壳中的宇宙\n博物 中国国家地理\n百年孤独\n银河系搭车客指南\n万物解释者\n神的九十亿个名字 by 阿瑟·克拉克\n深度学习推荐系统 by王喆\n给好奇者的暗黑物理学 一想到还有95%的问题留给人类我就放心了 给仰望者的天文朝圣之旅 给未来人类的终极12问 数据仓库 谷歌分析宝典/谷歌数据分析方法 计算广告-互联网商业变现的市场与技术 数据结构 Vehicle Routing Problems, Methods and Applications second edition Metahuristics for Vehicle Routing Problems 运筹需论坛-EURO2018论坛 KDD/ICLR/IEEE论坛paper IEEE\nICDM\nACM MM\n","id":3,"section":"posts","summary":"书名 读书日期 心得\u0026amp;读书笔记标签 上帝掷色子吗 2021.3.5\u0026mdash;\u0026gt;2021.4.11 高中的时候读到了一本量子计算机相关的书，现在已忘记书名，但是给我的震撼还是非常大的，所以","tags":null,"title":"⭐♥Book List♥⭐","uri":"https://biofrostyy.github.io/2021/07/%E4%B9%A6%E5%8D%95/","year":"2021"},{"content":"几乎所有的算法书中，排序算法都是在最开始介绍的算法，不仅仅是因为排序算法非常简单，而且因为排序算法非常基础，在后续其他算法或者处理其他问题时都有广泛的应用。就像我在刷题时，应用到排序就可以直接按时间复杂度$O(nlogn)$计算。所以此次复习算法和数据结构也从排序算法开始，使用到的工具书有\u0026lt;算法4\u0026gt;、\u0026lt;我的第一本算法书\u0026gt;及其app、\u0026lt;labuladong的算法小抄\u0026gt;。刷题网站为leetcode，理论上会把相关主题的题全部刷到，并且整理出笔记与代码沉淀。\n一.排序算法 ①时间复杂度为$O(n^2)$的排序算法——选择排序、插入排序、希尔排序 选择排序的过程为，首先，找到数组中最小的那个元素，其次，将它和数组的第一个元素交换位置（如果第一个元素就是最小元素那么它就和自己交换）。再次，在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置。如此往复，直到将整个数组排序。这种方法叫做选择排序，因为它在不断地选择剩余元素之中的最小者。\n总的来说，选择排序是一种很容易理解和实现的简单排序算法，它有两个很鲜明的特点。其一：运行时间和输入无关。即一个已经有序的数组或是主键全部相等的数组和一个元素随机排列的数组所用的排序时间一样长。一些其他的排序算法可能会更善于利用输入的初始状态。其二：数据移动是最少的。每次在找到最小值时才会进行一次交换，因此选择排序用了N次交换——交换次数和数组的大小是线性关系。我们将研究的其他任何算法都不具备这个特征（大部分的增长数量级都是线性对数或是平方级别）。\n插入排序则像整理桥牌一样一张一张的来，将每一张牌插入到其他已经有序的牌中的适当位置，与选择排序一样，当前索引左边的所有元素都是有序的，但它们的最终位置还不确定，为了给更小的元素腾出空间，它们可能会被移动。但是当索引到达数组的右端时，数组排序就完成了。\n对于随机排列的长度为$N$且主键不重复的数组，平均情况下插入排序需要$\\cfrac{N^2}{4}$次比较以及$\\cfrac{N^2}{4}$次交换。最坏情况下需要$\\cfrac{N^2}{2}$次比较和$\\cfrac{N^2}{2}$次交换，最好情况下需要$N-1$次比较和$0$次交换。其中的情况好坏取决于输入中元素的初始顺序。\n我们可以发现在比较好的情况下，即对于部分有序的数组，插入排序可以获得线性的时间复杂度！\nPS.衡量一个数组是不是部分有序和有序的程度时，可以用到倒置数量这个指标。倒置指的是数组中的两个顺序颠倒的元素。比如E X A M P L E中有11对倒置：E-A、 X-A、 X-M、 X-P、 X-L、 X-E、 M-L、 M-E、 P-L、 P-E以及L-E。如果数组中倒置的数量小于数组大小的某个倍数，那么我们说这个数组是部分有序的。下面是几种典型的部分有序的数组：❏数组中每个元素距离它的最终位置都不远；❏一个有序的大数组接一个小数组；❏数组中只有几个元素的位置不正确。插入排序对这样的数组很有效，而选择排序则不然。事实上，当倒置的数量很少时，插入排序很可能比本章中的其他任何算法都要快。\n上面我们说到，对于部分有序的数组，插入排序非常有效，那么我们可不可以对数组预处理成部分有序，再用插入排序呢。这就是希尔排序。希尔排序又叫缩小增量排序，它是基于插入排序的增强版。时间复杂度是$O(N*(logN)^2)$，在最坏的请款下比较次数和$N^{\\cfrac{3}{2}}$。人们发明了很多递增序列来渐进式地改进最坏情况下所需的比较次数（N4/3,N5/4, N6/5…），但这些结论大多只有学术意义，因为对于实际应用中的N来说它们的递增序列的生成函数（以及与N乘以一个常数因子）之间的区别并不明显。\n②时间复杂度为$O(nlogn)$的排序算法——归并排序、快速排序 当我们看到$logn$时不难想到\u0026quot;二分\u0026quot;的思想。\n归并排序运行速度比简单排序块，但是它需要的空间是原始数组空间的两倍；通常这是一个严重的缺点。\n二.代码沉淀 ①有序数组中查找左右边界 def helper(tar): #左边界 i, j = 0, len(nums) - 1 while i \u0026lt;= j: m = (i + j) // 2 if nums[m] \u0026lt; tar: i = m + 1 else: j = m - 1 return j def helper(tar): # 右边界 i, j = 0, len(nums) - 1 while i \u0026lt;= j: m = (i + j) // 2 if nums[m] \u0026lt;= tar: i = m + 1 else: j = m - 1 return i  ②双指针——将两个数组合并 # 有些题中，为了不占用新内存空间，可以使用逆向双指针在原数组上进行 class Solution: def merge(self, A: List[int], m: int, B: List[int], n: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Do not return anything, modify A in-place instead. \u0026quot;\u0026quot;\u0026quot; pointera = m-1 pointerb = n-1 while pointera \u0026gt;= 0 and pointerb \u0026gt;= 0: if A[pointera] \u0026gt;= B[pointerb]: A[pointera+pointerb+1] = A[pointera] pointera -= 1 else: A[pointera+pointerb+1] = B[pointerb] pointerb -= 1 while pointerb \u0026gt;= 0: A[pointerb] = B[pointerb] pointerb -= 1 '''题目链接：https://leetcode-cn.com/problems/sorted-merge-lcci/ 此题还可以直接把B数组填入A数组中，对A进行排序''' # 另一些题中，希望两种数字（如奇偶），在特定的位置，可以使用双指针，找到后交换两指针的元素  ③python排序实现 ------自定义排序 在python中有自定义排序函数,list.sort(key=函数) sorted_items = sorted(list,key=函数,reverse=False) 可以根据function对一个list进行排序，函数可以返回一个tuple(x[0],-x[1])，表示先根据x[0]排序，如果相同再根据x[1]排序，负号可以达到根据x[1]降序排列的目的 引申的，如果想根据一个元素在list中出现的次数排序，sorted(list,key=lambda x:(list.count(x),-x))，collections.Counter(s)方法可以返回一个字典，表示每个元素的出现次数。对于上述方法的提升---return ''.join([i*j for i, j in sorted([[i, j] for i, j in dict_al.items()], key=lambda x: -x[1])]) ------计数排序，当数据种类较少时可以使用 参考例题：https://leetcode-cn.com/problems/relative-sort-array/  ④python排列组合实现 ------全排列combinations和permutations函数 combinations方法重点在组合，permutations方法重在排列。返回的都是一个iterator。 ------回溯递归 def permutation(self, s: str) -\u0026gt; List[str]: if len(s) \u0026lt;= 1: #当只剩一个的时候返回 return [s] return list(set(s[i] + perm for i in range(len(s)) for perm in self.permutation(s[:i] + s[i+1:]))) # 此时当每个元素都不同时，不会有相同的返回值，当有基本元素相同时(['a','a','b']三个基本元素就会出现两个\u0026quot;aab\u0026quot;)，set()，可以帮助去除重复 def permutation(self, s: str) -\u0026gt; List[str]: # 递归普通写法 result = [] def permutation(ans, s): if not s: return result.append(ans) for i in set(s): new_ans = i new_s = s.copy() new_s.remove(i) permutation(ans + new_ans, new_s) permutation('', list(s)) return result -------回溯非递归 class Solution: def permutation(self, s: str) -\u0026gt; List[str]: n = len(s) curr = list(sorted(s)) end = list(reversed(curr)) ans = [] # 生成下一个排列 while curr != end: ans.append(''.join(curr)) i = n - 2 # 29631 -\u0026gt; 31269 while i \u0026gt; 0 and curr[i] \u0026gt;= curr[i+1]: i -= 1 j = n - 1 while j \u0026gt; i-1 and curr[j] \u0026lt;= curr[i]: j -= 1 curr[i], curr[j] = curr[j], curr[i] curr = curr[:i+1] + sorted(curr[i+1:]) ans.append(''.join(end)) return ans  ⑤python bisect二分查找 \u0026quot;\u0026quot;\u0026quot; bisect 为可排序序列提供二分查找算法 \u0026quot;\u0026quot;\u0026quot; import bisect #使用bisect函数前需要对列表进行排序，否则虽然可以输出数值，但没有意义 a = [1, 5, 6, 10, 9] a.sort() print(\u0026quot;最初的列表：\u0026quot;, a) #bisect.bisect 返回某个数在列表中可以插入的位置，但不会插入该数。 #如果这个数与列表中的元素相同，则返回元素后面的位置 print(\u0026quot;6在列表中可以插入的位置：\u0026quot;, bisect.bisect(a, 6)) #bisect.insort 将某个数插入列表 bisect.insort(a, 7) print(\u0026quot;在列表中插入7：\u0026quot;, a) #处理插入数值与列表元素相同的情况，返回位置，但不会插入该数 #bisect.bisect_left 插入元素左侧位置；bisect.bisect_right 插入元素右侧位置 print(\u0026quot;9在列表中可以插入的位置：\u0026quot;, bisect.bisect_left(a, 9)) print(\u0026quot;9在列表中可以插入的位置：\u0026quot;, bisect.bisect_right(a, 9)) #处理插入数值与列表元素相同的情况，插入该数 #bisect.insort_left 插入元素左侧位置；bisect.insort_right 插入元素右侧位置 bisect.insort_left(a, 9) print(\u0026quot;在列表中插入10：\u0026quot;, a) bisect.insort_right(a, 10) print(\u0026quot;在列表中插入10：\u0026quot;, a)  ","id":4,"section":"posts","summary":"几乎所有的算法书中，排序算法都是在最开始介绍的算法，不仅仅是因为排序算法非常简单，而且因为排序算法非常基础，在后续其他算法或者处理其他问题时","tags":["“python”","代码沉淀","数据结构","leetcode"],"title":"数据结构复习-排序算法","uri":"https://biofrostyy.github.io/2021/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%8E%92%E5%BA%8F/","year":"2021"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等\n二.刷题沉淀 ①两个栈实现队列 # 类似于负负得正的思想，append时直接放入栈1，delete时如果栈2为空，那么把栈1移入栈2（负负得正，栈1最先放进去的最后弹出来，在栈2中最后放进去的最先弹出来），再pop栈2；如果原来栈2不为空，那么表示上次放入的还有剩（上次放入的一定比这次早），直接pop；如果移入后栈2仍为空，返回-1 class CQueue: def __init__(self): self.A,self.B = [],[] def appendTail(self, value: int) -\u0026gt; None: self.A.append(value) def deleteHead(self) -\u0026gt; int: if not self.B: if not self.A: return -1 else: while self.A: self.B.append(self.A.pop()) return self.B.pop() # Your CQueue object will be instantiated and called as such: # obj = CQueue() # obj.appendTail(value) # param_2 = obj.deleteHead()  ②队列实现栈 # 双队列实现栈，等于把一个队列B存放之前的结果，把新的元素放入A后，再把B中之前的元素放入A 执行用时：32 ms, 在所有 Python3 提交中击败了94.87%的用户 内存消耗：15 MB, 在所有 Python3 提交中击败了35.37%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A,self.B = deque(),deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) while self.B: self.A.append(self.B.popleft()) self.A,self.B = self.B,self.A self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.B.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.B[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0 # 单队列实现栈，存储原有元素数量n，把新元素放入队列中后，执行n次pop-append操作，把之前的元素重新放入队列中 执行用时：40 ms, 在所有 Python3 提交中击败了61.83%的用户 内存消耗：14.8 MB, 在所有 Python3 提交中击败了93.24%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A = deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) n = self.size while n \u0026gt; 0: self.A.append(self.A.popleft()) n -= 1 self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.A.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.A[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0  ③字符串匹配——KMP https://leetcode-cn.com/problems/implement-strstr/solution/zhe-ke-neng-shi-quan-wang-zui-xi-de-kmp-8zl57/ class Solution: def strStr(self, haystack: str, needle: str) -\u0026gt; int: a=len(needle) b=len(haystack) if a==0: return 0 next=self.getnext(a,needle) p=-1 for j in range(b): while p\u0026gt;=0 and needle[p+1]!=haystack[j]: p=next[p] if needle[p+1]==haystack[j]: p+=1 if p==a-1: return j-a+1 return -1 def getnext(self,a,needle): next=['' for i in range(a)] k=-1 next[0]=k for i in range(1,len(needle)): while (k\u0026gt;-1 and needle[k+1]!=needle[i]): k=next[k] if needle[k+1]==needle[i]: k+=1 next[i]=k return next  ④摩尔投票——选出数组中超过半数的值 class Solution: def majorityElement(self, nums: List[int]) -\u0026gt; int: # n = len(nums) # dic_nums = Counter(nums) # for key in dic_nums.keys(): # if dic_nums[key] \u0026gt; n/2: # return key # return -1 # 狼人杀归票算法 # 第一轮找到最可能出局的那个人 n = len(nums) ans = -1 count = 0 for num in nums: # 没有票数，暂时认为是当前的人 if not count: ans = num # 有相同的人上票，票数加一；否则票数减一 if num == ans: count += 1 else: count -= 1 # 第二轮确定这个人的票数确实过半 return ans if count and nums.count(ans) \u0026gt; n // 2 else -1  ","id":5,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等 二.刷题沉淀 ①两个栈实现队列 # 类似于负负得正的思想，ap","tags":["“python”","代码沉淀","数据结构","leetcode"],"title":"数据结构-基础结构","uri":"https://biofrostyy.github.io/2021/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84/","year":"2021"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等$n^2$\n二.刷题沉淀 ①花费最少爬楼梯 class Solution: def minCostClimbingStairs(self, cost: List[int]) -\u0026gt; int: # cost.append(0) # def min_cost(i): # # 递归 -- 超时 # if i in (0,1): # return cost[i] # return cost[i] + min(min_cost(i-1),min_cost(i-2)) # return min_cost(len(cost)-1) # # 带记忆的递归 # cost.append(0) # cost_sum = [-1]*len(cost) # def min_cost(i): # if i in (0,1): return cost[i] # elif cost_sum[i] == -1: # cost_sum[i] = cost[i] + min(min_cost(i-1),min_cost(i-2)) # return cost_sum[i] # return min_cost(len(cost)-1) # DP cost.append(0) i, n, cost_num = cost[0],cost[1],0 for k in range(2,len(cost)): cost_num = min(i,n)+cost[k] i,n = n,cost_num return cost_num  ②队列实现栈 # 双队列实现栈，等于把一个队列B存放之前的结果，把新的元素放入A后，再把B中之前的元素放入A 执行用时：32 ms, 在所有 Python3 提交中击败了94.87%的用户 内存消耗：15 MB, 在所有 Python3 提交中击败了35.37%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A,self.B = deque(),deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) while self.B: self.A.append(self.B.popleft()) self.A,self.B = self.B,self.A self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.B.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.B[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0 # 单队列实现栈，存储原有元素数量n，把新元素放入队列中后，执行n次pop-append操作，把之前的元素重新放入队列中 执行用时：40 ms, 在所有 Python3 提交中击败了61.83%的用户 内存消耗：14.8 MB, 在所有 Python3 提交中击败了93.24%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A = deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) n = self.size while n \u0026gt; 0: self.A.append(self.A.popleft()) n -= 1 self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.A.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.A[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0  ","id":6,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等$n^2$ 二.刷题沉淀 ①花费最少爬楼梯 class Solution: def minCostClimbingStairs(self, cost: List[int]) -\u0026gt; int: #","tags":["“python”","代码沉淀","数据结构","leetcode"],"title":"数据结构-基础结构","uri":"https://biofrostyy.github.io/2021/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","year":"2021"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等\n二.刷题沉淀 ①ListNode题 # 可以使用一个空起始点来做开头，这样就不需要制作list保存了，规避了第一个的下一个的悖论  ②str # 实现str倒序，-1表示步长，即每次向前一步 str(x)[::-1]  ③差分数组 差分数组--把区间统一修改转嫁为前后临界点的修改 1109.航班预定统计  ","id":7,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等 二.刷题沉淀 ①ListNode题 # 可以使用一个空起始点来","tags":["“python”","代码沉淀","数据结构","leetcode"],"title":"数据结构-刷题tips","uri":"https://biofrostyy.github.io/2021/06/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%88%B7%E9%A2%98tips/","year":"2021"},{"content":"本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，没有选择自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些相关应用。当然，这本书是一本科普读物，至少在仅有的我熟悉的几章中，书中的介绍还是比较基础的模型（当然这正是现在复杂模型的数学基础）。另外，这是一本让你构建宏观体系的书，它介绍了人工智能运用在工业界的方方面面，让人们对这些问题的解决有一个“道”的思想理解，有趣的是，因为这种宏观的描绘，在读书中会有很多背后蕴藏的深刻理论会吸引你停下读书的脚步，对其进行更深的探索，这正是本书的乐趣所在。\n在此记录读书中的收获和积累，也希望给无暇读书的同行一个十分钟读完本书的可能。\n①统计语言模型 当前词的概率只取决于前面N-1个词，这种假设被称为N-1阶马尔可夫假设，对应的语言模型称为N元模型(N-Gram Model)。N = 2的二元模型就只取决于前一个词，而N = 1的一元模型实际上是一个上下文无关的模型，也就是假定当前词出现的概率与前面的词无关。而在实际中，应用最多的是N=3的三元模型，更高阶的模型就很少使用。\n为什么N取值一般都这么小呢？这里主要有两个原因。首先，N元模型的大小（或者说空间复杂度）几乎是N的指数函数，即O(|V|^N )，这里|V|是一种语言词典的词汇量，一般在几万到几十万个。而使用N元模型的速度（或者说时间复杂度）也几乎是一个指数函数，即O(|V|^(N−1) )。因此，N不能很大。而且当模型从3到4时，效果的提升就不是很显著了，但资源的耗费却增加的非常快，所以，除非是为了做到极致不惜资源，很少有人使用四元以上的模型。Google的罗塞塔翻译系统和语音搜索系统，使用的就是四元模型，该模型存储于500台以上的Google服务器中。\n另外，因为上下文之间的相关性可能跨度非常大，甚至可以从一个段落到另一个段落，所以N无论多大都不能覆盖所有的语言现象。这就需要一些长程的依赖性(Long Distance Dependency)来解决问题了。\n而重新估算概率的估计使用古德-图灵估计(Good-Turing Estimate)，模型的零概率及较小统计值的平滑问题卡茨退避法(Katz Backoff)，一般参数T在8-10之间，频数在T以上的词不用进行古德-图灵估计，同时为了保证总概率为1，所有下调频率总和平均分给未出现的词，内伊(Herman Ney)等人对卡茨退避法进行了一次优化，原理大同小异，参考。PS.二元组的相对频率比三元组更接近概率分布，低阶模型比高阶模型零概率问题轻微，因此用低阶语言模型和高阶语言模型进行线性插值来达到平滑目的，这种方法称为删除差值(Deleted Interpolation)，此方法比卡茨退避法略差，现在已经很少用了。\n需要训练数据和应用数据一致且训练量足够大，所以在语料少的情况下，片面追求高阶的大模型没什么意义。另外，训练语料的噪音高低也会产生影响，因此，一般情况下，对于能找到模式(Pattern)的、量比较大的噪音还是有必要过滤的，比如网页文本中存在的大量制表符。\n②谈谈分词 分词问题属于已经解决的问题，在工业界，只要采用基本的统计语言模型，加上一些业界熟知的技巧就能得到很好的分词结果，提高的空间微乎其微（人工分词也有不同的差异）。另外，在手写体识别中，罗马体系的拼音语言也需要分词方法。\n梁南元教授提出的“查字典”这种最简单的方法可以解决七八成以上的分词问题，1990年前后，郭进博士用统计语言模型成功解决了分词二义性问题，即各种不同的分词方式中，出现概率最高的分词方法。但是，如果穷举所有可能的分词方法并计算出每种可能性下句子的概率，那么计算量是相当大的。因此，可以把它看成是一个（Dynamic Programming）的问题，并利用维特比(Viterbi)算法快速地找到最佳分词。接下来，孙茂松教授解决了没有字典时的分词问题，吴德凯教授最早将中文分词方法用于英文词组的分割，并且将英文词组和中文词组在机器翻译时对应起来。\n词的颗粒度问题，在机器翻译中，颗粒度大效果好，而在搜索中，颗粒度小效果好，因为当用户查询清华时，我们时希望能找到清华大学的。对于上述大小粒度的需求，我们可以构造一个分词器同时支持不同层次的分词\u0026mdash;基于基本词表与复合词表，根据基本词表和复合词表分别建立语言模型\u0026ndash;L1和L2。这就是分词的层次概念。\n分词的不一致性可以分为错误和颗粒度不一致两种，错误又分成两类，一类是越界错误，比如把“北京大学生”分成“北京大学-生”。另一类是覆盖型错误，比如把“贾里克尼”拆成四个字。这些是明显的错误，是改进分词器时要尽可能消除的。接下来是颗粒度的不一致性，人工分词的不一致性大多属于此类。这一类不一致性在衡量分词器的好坏时，可以不作为错误。对于某些应用，需要尽可能地找到各种复合词，而不是将其切分。总之，要继续做数据挖掘，不断完善复合词的词典（它的增长速度较快），这也是近年来中文分词工作的重点。\n③信息的度量和作用 变量的不确定性越大，熵也就越大，所需信息量就越大。一本50万字的中文书平均有多少信息量，常用的前10%的汉字占常用文本的95%以上，那么每个汉字的信息熵约8-9bit，如果考虑上下文，每个汉字的信息熵就只有5bit左右。所以一本50万字的中文书，信息量大约是250万比特，采用较好的算法进行压缩，整本书可以存成一个320kb的文件，而如果直接用两字节的国标编码存储这本书，大约需要1MB大小，是压缩文件的3倍，这两个数量的差距，在信息论中被称作“冗余度”(Redundancy)，需要指出的是，这里的250万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复内容很多，它的信息量就小，冗余度就大。而在不同语言中，汉语是冗余度相对小的。\n一个事物的不确定性U，需要引入信息I来消除，而需要引入的信息量取决于这个不确定性的大小，即I\u0026gt;U才行。在某些时候，不引入新的信息，而在已有的信息上玩数字和公式的游戏，本质上和蒙没有区别。\n说回自然语言的统计模型，其中一元模型就是通过某个词本身的概率分布来消除不确定性，而二元及更高阶的语言模型则还使用了上下文的信息，这些“相关的”信息可以消除不确定性，就像ID3树模型中使用的节点选择依据\u0026ndash;条件熵，也是根据条件熵体现某个特征对目标变量取值的判断是否有消除不确定性的作用。所以说，自然语言处理的大量问题就是寻找相关信息。\n还是使用决策树节点选择作为例子，为了度量一个特征（信息）对目标特征（目标信息）的相关性，或者说消除目标信息不确定性的能力，我们使用互信息，而互信息就是目标特征的不确定性(熵)-条件熵（在已知某特征时目标特征的不确定性），当X和Y完全相关时，它的取值时H（X），同时H（X）=H（Y），当两者完全无关时，它的取值是0。\n信息论中的另一个重要概念，相对熵，又叫交叉熵，是机器学习分类模型中经常使用的损失函数。相对熵也用来度量相关性，但和变量的互信息不同，它用来衡量两个取值为正数的函数的相似性。交叉熵是不对称的，詹森和向明提出了一种新的相对熵的计算方法，将其左右取平均，在Google的自动问答系统中，我们采用了上面的詹森-香农度量来衡量两个答案的相似性。相对熵还可以用来衡量两个常用词（在语义和语法上）在不同文本中的概率分布，看它们是否同义。另外，利用相对熵，还可以得到信息检索中最重要的一个概念：词频率-逆向文档频率(TF-IDF)。\n总的来说，熵、条件熵和相对熵这三个概念与语言模型的关系非常密切。贾里尼克从条件熵和相对熵出发，定义了一个称为语言模型复杂度(Perplexity)的概念来直接衡量语言模型的好坏。复杂度有很清晰的物理含义，它是在给定上下文的条件下，句子中每个位置平均可以选择的单词数量。一个模型的复杂度越小，每个位置的词就越确定，模型越好。\n李开复博士介绍他发明的Sphinx语音识别系统的论文里谈到，如果不用任何语言模型（即零元语言模型），（模型的）复杂度为997，也就是说句子中每个位置有997个可能的单词可以填入。如果（二元）语言模型只考虑前后词的搭配，不考虑搭配的概率，复杂度为60。虽然它比不用语言模型好很多，但与考虑搭配概率的二元语言模型相比要差很多，因为后者的复杂度只有20。\n④贾里尼克和现代语言处理 20世纪70年代的IBM，在其他科学家把语音识别问题当作人工智能和模式匹配问题时，贾里尼克等人在IBM把它当作通信问题，并用两个隐马尔可夫模型（声学模型和语言模型）把语音识别概括得清清楚楚。\n贾里尼克和波尔、库克以及拉维夫的另一大贡献是BCJR算法，这是今天数字通信中应用最广的两个算法之一（另一个是维特比算法）。\n⑤简单之美-布尔代数和搜索引擎 Truth is ever to be found in simplicity, and not in the multiplicity and confusion of things.\n布尔代数对于数学的意义等同于量子力学对于物理学的意义。他们将我们对世界的认知从连续状态扩展到离散状态。\n搜索引擎通过建立索引来在零点零几秒内就找到成千上万甚至上亿的结果。早期的文献检索查询系统，严格要求查询语句复合布尔运算。相比之下，今天的搜索引擎会聪明的自动把用户的查询语句转换成布尔运算的算式，但是基本的原理没有什么不同。\n就像我们理解的ES倒排索引进行搜索的原理一样，如果要找到同时包含“原子能”和“应用”的文章，只要将代表是否有这两个字的二进制数组进行布尔运算AND，就可以得到结果。例如“原子能”的布尔数组为10001，表示第一篇和第五篇文章包含原子能关键字。“应用”的布尔数组为01001，那么同时包含“原子能”和“应用”的文章就是00001，即第五篇文章。\n对于互联网的搜索引擎来说，每一个网页就是一个文献。这个索引是巨大的，在万亿字节这个量级。早期的搜索引擎（比如AltaVista以前的所有搜索引擎），由于受计算机速度和容量的限制，只能对重要、关键的主题词建立索引，但是这样不常见的词和太常见的虚词就找不到了。现在，为了保证对任何搜索都能提供相关的网页，常见的搜索引擎都会对所有的词进行索引。但是，这在工程上却极具挑战性。\n加入互联网上有10^10^个有意义的网页，而词汇表的大小是30万，那么这个索引的大小至少是(10^10^)*30=3000万亿。考虑到大多数词只出现在一部分文本中，压缩为30万亿的量级。为了网页排名方便，索引中还需存有大量附加信息，如这个词出现的位置、次数等。因此，整个索引会通过分布式的方式根据网页的序号将索引分成很多份(Shards)，分别存储在不同服务器中。同时，为了提高效率，需要根据网页的重要性、质量和访问的频率建立常用的非常用等不同级别的索引。常用的索引需要访问速度快，附加信息多，更新也要快；而非常用的要求就低多了。但是不论搜索引擎的索引在工程上如何复杂，原理上依然非常简单，即等价于布尔运算。\n⑥图论和网络爬虫 上文提到搜索引擎，主要由下载、索引和排序组成，布尔代数支撑了索引的逻辑，那么图论中的遍历算法支撑了搜索引擎的下载。\n对于图论相关算法有很多，图的遍历、最短路径、MST、Floyd算法等。而基于图论，还有许多Np-Hard的有趣问题如TSP、VRP问题等。\n假定从一家门户网站的首页出发，先下载这个网站，然后通过分析这个网站，可以找到页面里的所有超链接，也就等于知道了这家门户网站首页所直接链接的全部网页。让计算机不停搜索下去，就能下载整个互联网。在网络爬虫中，人们使用Hash Table来记载哪些网站已经被访问过。现在的互联网非常庞大，一个商业的网络爬虫需要有成千上万个服务器，并且通过高速网络连接起来。如何建立起这样复杂的网络结构，如何协调这些服务器的任务，就是网络设计和程序设计的艺术了。\n网络爬虫对网页遍历的次序不是简单的BFS或DFS，而是有一个相对复杂的下载优先级排序方法，由调度系统(Scheduler)管理。在调度系统里需要存储那些已经发现但尚未下载的URL，它们一般存在一个优先级队列(Priority Queue)，这在工程上与BFS更相似，因此，爬虫中BFS的成分多一些。\n对于页面的分析，过去由HTML书写的网站很容易提取，但是现在很多网页是由一些脚本语言(如JavaScript)生成。打开网页的源代码，URL不是直接可见的文本，而是运行这一段脚本才能得到的结果。因此网络爬虫需要模拟运行这一段脚本。另外，有些脚本写的非常不规范，以至于解析起来非常困难（浏览器可以解析），这就需要解析人员具有非常强大的浏览器内核工程能力。因此，如果一些网页明明存在，但搜索引擎没有收录，一个原因可能是网络爬虫中的解析程序没能成功解析网页中不规范的脚本程序，又或者这个网页搞了反爬（就像GitHub.io的博客）。\n上面提到过，Hash Table被用来记录哪些网站已经被访问过，Hash Table判断一个网页是否在表中只需要1个复杂度，写入某新网站也只需要Hash运算。但在一个成千上万个服务器的爬虫工程中，存储并实时维护一张哈希表就是一个令人头疼的事了。为了各个服务器不做重复的工作，它们需要在下载前和下载后实时访问哈希表，这就造成了通信瓶颈。对于这个问题有两个技术：首先明确各个服务器的分工，也就是一看到某个URL就确定要某一台服务器去做（例如结尾数字为1的URL都要服务器1去下载），这样就可以避免重复下载，在明确分工的基础上，判断URL就可以批处理了，比如每次向Hash Table发送一大批询问，或者每次更新一大批Hash Table的内容。这样通信的次数就大大减少了。\n书中没有提到具体的Hash Table存储的解决方案，个人理解可能参考参数服务器的分布式存储构架进行存储。\n⑦搜索引擎的排序-网页质量度量与相关性度量 搜索引擎中，根据索引返回的网页数有成千上万条，那么该如何排序，把用户最想看到的结果排在前面呢。总的来讲，排名取决于两个信息：网页的质量信息(Quality)，以及这个查询与每个网页的相关性信息(Relevance)。\n网页质量的衡量 最初，一些衡量网络质量的方法或多或少地用到了指向某个网页的链接以及链接上的文本（在搜索技术中成为锚文本，Anchor Text）。真正找到计算网页自身质量的完美的数学模型的是Google创始人拉里·佩奇和谢尔盖·布林。PageRank的高明之处在于把整个互联网当作一个整体来对待，这无意中符合了系统论的观点。核心思想是，如果一个网页被很多其他网页链接，说明它受到普遍的承认和信赖，那么它的排名就高。另外，PageRank也考虑了网页排名高的网站贡献的链接权重大这个因素。现在就出现了一个问题，计算搜索结果的网页排名过程中需要用到网页本身的排名，这不就是\u0026quot;先有鸡还是先有蛋\u0026quot;吗？破解这个怪圈的是布林，他把这个问题变成了一个 二维矩阵相乘的问题，并用迭代的方法解决了这个问题，他们先假定所有网页的排名是相同的，然后根据这个初始值，算出各个网页的第一次迭代排名，然后再根据第一次迭代排名算出第二次迭代排名。而且他们从理论上证明了，不论初始值如何选取，这种算法都能保证网页排名的估计值能收敛到排名的真实值，值得一提的是，这种算法不需要任何人工干预。理论的问题解决了，数量庞大的网页有带来了实际问题，十亿个网页时，这个二维矩阵就是一百亿亿个元素。这么大的矩阵相乘，计算量是非常大的。佩奇和布林利用稀疏矩阵的计算技巧，大大简化了计算量，并实现了这个网页排名算法。\nGoogle早期，并行化是半自动的，这样更新一遍所有网页的周期很长。2003年，Google的工程师迪恩(Jeffrey Dean)和格麦瓦特(Sanjay Ghemawat)发明了二MapReduce（矩阵相乘很容易分解成许多小任务），PageRank的并行计算完全自动化了。\nPageRank的计算方法，结果向量B，为一个$1 \\times N$的向量，初始时为$B_0=(\\cfrac{1}{N},\\cfrac{1}{N},…,\\cfrac{1}{N})$，A为$N \\times N$的矩阵，其中值代表第n个网页指向第m个网页的链接数。通过公式$B_i = A \\cdot B_{i-1}$进行迭代，可以证明$B_i$最终会收敛至$B=B \\cdot A$，一般来讲，只要10次左右的迭代基本上就收敛了，即两次迭代的结果$B_i$和$B_{i-1}$之间的差异非常小。\n由于网页之间链接的数量相比互联网的规模非常稀疏，因此需要对零概率或者小概率事件进行平滑处理。\n$B_i = [\\cfrac{α}{N} \\cdot I + (1-α)A] \\cdot B_{i-1}$\n其中N是互联网网页的数量，α是一个（较小的）常数，I是单位矩阵\n网页与查询的相关性度量 2007年，技术和算法的重要性依然高于数据，因此确定相关性主要依靠算法。但是今天，由于商业搜索引擎已经有了大量的用户点击数据，因此，对搜索相关性贡献最大的是根据用户对常见搜索点击网页的结果得到的概率模型。如今，影响搜索引擎质量的诸多因素，除了用户的点击数据之外，都可以归纳成下面四大因素：\n1.完备的索引。俗话说，巧妇难为无米之炊，如果一个网页不在索引中，那么再好的算法也找不到。\n2.对网页质量的度量。当然现在看来，PageRank的作用比10年前已经小了很多。今天，对网页质量的衡量是全方位的，比如对网页内容权威性的衡量，一些八卦网站的PageRank可能很高，但是他们的内容权威性则很低。\n3.用户偏好。一个好的搜索引擎会针对不同用户，对相同的搜索给出不同的排名。\n4.确定一个网页和某个查询的相关性的方法。\nTF-IDF TF（Term Frequency 单文本词频），词频，关键词的出现次数/总字数。其中停止词，如“的”，“是”，“中”，的权重为0。对于一组关键词，可以得到$TF_1$,$TF_2$,\u0026hellip;,$TF_n$\nIDF（Inverse Document Frequency 逆文本频率指数），对于预测主题能力更强的词，权重越大，这个权重的衡量就是IDF，$log(\\cfrac{D}{D_w})$，其中D为全部文本数，$D_w$为关键词w在$D_w$个文本中出现。即如果一个关键词只在很少的网页中出现，通过它就容易锁定搜索目标，它的权重也就应该大。其实，所谓IDF的概念就是一个特定条件下关键词的概率分布的交叉熵(KL散度)，这样，关于信息检索相关性的度量，又回到了信息论。\n现在，各家搜索引擎对关键词重要性的度量，都在TF-IDF的基础上做了一定的改进和微调，但是，原理上与TF-IDF相差不远。\n⑧搜索引擎反作弊问题和搜索结果的权威性问题 搜索引擎反作弊 搜索引擎中排名靠前的网页不一定就是高质量的、相关的网页，而是商业味儿非常浓的作弊网页，它们采用不正当的手段(SPAM)提高自己网页的排名。\n早期的作弊方法是重复关键词，有时为了不让读者看到过多讨厌的关键词，聪明一点的作弊者常用很小的字体或与背景相同的颜色来掩盖这些关键词，其实这种方法很容易被搜索引擎发现并纠正。有了PageRank后，就有了专门买卖链接的生意，有人会创建成百上千的网站用于链接客户的网页。但是大量地卖链接很容易露出马脚（流通量大后很容易找到源头）。当然，还会有各种各样其他的作弊手段，抓作弊是一种长期的“猫捉老鼠”的游戏。\n作弊者所做的事情，就像是在手机信号中加入噪声，这种人为加入的噪声并不难消除，因为这些作弊方法不是随机的（否则就无法提高排名了），因此，可以在搜集一段时间的作弊信息后，将作弊者抓出来。即针对这些商业相关的搜索，采用一套“刚干扰”强的搜索算法。\n首先，那些卖链接的网站，都有大量的出链(Out Links)，而这些出链的余弦距离如果接近1，则说明这些网站间并无关系！那么很有可能这是一个卖链接的网站。其次，反作弊用到的另一个工具是图论。在图中，如果有几个节点两两互相都连接在一起，它们被称为一个Clique。作弊网站一般需要互相链接以提高自己的排名，图论中有专门发现Clique的方法，可以直接用于找到这些作弊网站的Clique。另外，至于术的方面，方法也很多，例如针对作弊的JavaScript跳转页面，通过解析相应JavaScript内容即可。\n作弊的本质是在网页排名信号中加入了噪声，因此反作弊的关键是去噪声。沿着这个思路可以从根本上提高搜索算法抗作弊的能力，事半功倍，而如果只是根据作弊的具体特征头痛医头，脚痛医脚，则很容易被作弊者牵着鼻子走。\n搜索结果的权威性 当用户问的是一些需要专业人士认真作答的的问题，比如医疗方面的问题，那么如何才能从众多信息源中找到最权威的信息，就成了近年来搜索引擎公司面对的难题。这些网页虽然权威性不高，但是文章常常写的很好看，名气也很大，PageRank也很高，但是它们的内容未必权威。其次，互联网上对同一个问题给出的答案常常互相矛盾。比如奥巴马的出生地，竟然有近百个答案，他的一些政敌说他生于肯尼亚，而官方给出的是夏威夷，虽然大家都知道政敌说的话未必可信，但是互联网又怎么知道谁是政敌呢？\n这就会引出“权威性的度量”，为了引入这一点，我们引入一个概念“提及”(Mention)，如果在各种新闻、学术论文或者其他网络信息页中，讨论到“吸烟危害”这一主题时，国际卫生组织和约翰·霍普金斯大学这两个组织作为信息源被多次提及，那么我们就有理由相信这两个组织是谈论“吸烟危害”这个主题的权威机构。需要指出的是，“提及”不像是超链接那样一目了然，它隐含在文章的自然语句中，需要通过自然语言处理的方式分析出来，即使有了好的算法，计算量也是非常大的。并且权威性与搜索主题是相关的，毕竟每个组织都只在自己的领域有权威性，使得存储量非常大，比如M个网页，N个搜索关键词时，我们要计算和存储$O(M \\cdot N)$个结果，而一般的网页只需要M个结果。因此，只有在今天有了云计算和大数据技术的情况下，计算权威性才成为可能。\n在计算权威度时，我们采用了句法分析、互信息和短语（词组）聚类这三种方法。\n计算权威度的步骤：\n1.对网站的文字进行句法分析，找到涉及主题的短语，以及对信息源的描述，这样大量的运算得益于皮耶尔领导开发的Google句法分析器足够快，而且有大量的服务器可供使用。\n2.利用互信息，找到主题短语和信息源的相关性，即出现主题短语时更容易出现某些信息源。\n3.对主题短语进行聚合，聚合那些字面上不同，但意义相同的主题短语，如“吸烟的危害”、“吸烟是否致癌”等等。这样我们就得到了一些搜索的主题。至于聚类的方法，可以采用前面提过的矩阵运算的方法。\n4.对一个网站中的网页聚合，比如把一个网站下面的网页按照子域(Subdomain)或者子目录(Subdirectory)进行聚类。这一步的目的是，即使一个权威的网站，它下面的一些子域却未必具有权威性。比如约翰·霍普金斯大学的网站，它下面可能有很多子域的内容与医学无关。因此，权威性的度量只能建立在子域或者子目录这一级。\n完成上述四个步骤后，我们就可以得到一个针对不同主题，哪些信息源(网站)具有权威性的关联矩阵。当然，在计算这个关联矩阵时，也可以像计算PageRank那样，对权威度高的网站给出“提及”关系更高的权重，并且通过迭代算法，得到收敛后的权威度关联矩阵。这样便可以在搜索结果中提升那些权威度高的信息源的结果，使得用户对搜索结果更放心。\n⑨有限状态机和动态规划-地图与本地搜索的核心技术 2008年9月23日，Google、T-mobile和HTC宣布了第一款基于开源操作系统Android的3G智能手机，它可以利用全球卫星定位系统实现的全球导航功能。此时这个系统可以媲美任何一个卫星导航仪，加上他的地址识别技术（采用有限状态机）比卫星导航仪严格的地址匹配技术（不能输错一个字母）要好得多。智能手机的定位和导航功能，其实只有三项关键技术：第一，利用卫星定位，这一点传统的导航仪都能做到；第二，地址的识别；第三，根据用户输入的起点和终点，在地图上规划最短线路。\n地址分析和有限状态机 地址的写法各种各样，且有比较复杂的上下有关的文法，例如 上海市北京东路XX号，南京市北京东路XX号，当识别器扫描到“北京东路”时，它和后面的门牌号能否构成一个正确的地址，取决于上下文，即城市名。在统计语言模型中有介绍，上下文有关的文法分析非常复杂又耗时，如果没有好的模型，这个分析器写出来很难看不说，很多情况无法覆盖。所幸的是，地址的文法是上下文有关文法中相对简单的一种，因此有许多识别和分析的方法，其中最有效的是有限状态机。有限状态机还能帮助google对用户输入的查询进行分析，挑出其中描述地址的部分，当然，剩下的关键词就是用户要找的内容。\n如果一个地址能从状态机的开始状态进过状态机的若干中间状态，走到终止状态，那么这条地址有效，否则无效。\n使用有限状态机识别地址，关键要解决两个问题，首先，要通过给定地址建立状态机，然后，给定这个有限状态机后，地址字串的匹配算法。好在这两个问题都有现成的算法。\n在实际情况中，用户输入地址不标准或有错别字时，有限状态机因为只能进行严格匹配，所以无法识别。其实，有限状态机在计算机科学早期的成功主要用于程序语言编译器的设计中。为了实现可以模糊匹配，基于概率的有限状态机被提出，它可以给出一个字串为地址的可能性，原理与离散的马尔科夫链基本等效。\n值得一提的是，有限状态机的用途远不止于对地址这样的状态序列进行分析，在Google新一代的产品中，有限状态机被应用在Google Now，一个在智能手机上的基于个人信息的服务软件。他会根据个人的地理位置信息、日历和一些其他信息（对应于有限状态机里面的状态），以及用户当前语音或者文字输入，回答个人的问题，提供用户查找的信息，或者提供相应服务（如打开地图导航、拨打电话等）。Google Now的引擎和AT\u0026amp;T的有限状态机工具库从功能上讲完全等价。\n有一些领域使用一种特殊的有限状态机——加权的有限状态机(Weighted Finite State Transducer, WFST)，有限状态传感器的特殊性在于，每一个状态由输入和输出符号定义，任何一个词的二元组，都可以对应到WFST的一个状态，WFST是天然的自然语言处理的分析和解码工具。在语音识别中，每个句子都可以用一个WFST表示，WFST中的每一条路径就是一个备选句子，其中概率最大的那条路径就是这个句子的识别结果。而这个算法的原理是动态规划。\n全球导航和动态规划 所有的导航系统都采用了动态规划(Dynamic Programming, DP)的办法，关于动态规划的思想及实现相信在数据结构及算法中已经讲解的很深刻，我在这里根据我在路径规划的研究和工作实践，补充一些其他经验。\n路径规划问题的精确解，都会采用动态规划。而在一些节点数庞大，又对完全精确解需求不高的情况下，解决这种NP-hard问题运筹学中的启发式算法，近年来有很多强化学习被应用在路径规划中，但是由于强化学习的加入带来了时间成本的增加，算法整体效率并没有太大提升。\n⑩余弦定理和新闻分类 所谓新闻的分类，或者更广义地讲文本的分类，无非是要把相似的新闻归入同一类中。这就要求我们先把文字的新闻变成一组可计算的数字，然后再设计一个算法来算出任意两篇新闻的相似性。\n一个新闻可以用一个向量表示，根据TF-IDF，对其中的每一个词在词典中进行标识，例如一个65535个词的词典，那么就会形成一个65535维的向量，称为新闻的特征向量(Feature Vector)。和计算搜索相关性一样，出现在文本不同位置的词再分类时的重要性也不相同，一般，标题\u0026gt;正文开头结尾\u0026gt;正文中间。可以对标题和重要位置的词进行额外加权，以提高文本分类的准确性。\n现在我们有了向量来代表一个新闻的内容，可以发现，同一类新闻用词一定是相似的。因此，可以通过计算两个向量的夹角来判断对应的新闻主题的接近程度，即余弦相似度。\n$cosθ = \\cfrac{\u0026lt;b,c\u0026gt;}{|b| \\cdot |c|} = \\cfrac{x_1y_1+x_2y_2+\u0026hellip;+x_{65535}y_{65535}}{\\sqrt{x_1^2+x_2^2+\u0026hellip;+x_{65535}^2} \\cdot \\sqrt{y_1^2+y_2^2+\u0026hellip;+y_{65535}^2}}$\n在工程中，为了简化余弦相似度的计算复杂度$O(N^2·|a|)$，有以下三种方向，首先，分母部分（向量的长度）不需要重复计算，计算向量a和向量b的余弦时，可以先将它们的长度存起来，等计算向量a和向量c的余弦时，直接取用a的长度即可；其次，在计算分子，即两个向量内积时，只需考虑向量中的非零元素，如果一篇新闻有2000个词，那么非零元素一般只有1000个，这样计算的复杂度可以下降1000/词典总词数，计算时间直接从“天”下降到十几分钟这个量级；第三，可以删除虚词，进一步减少非零元素个数，另外删除虚词，不仅可以提高计算速度，对新闻分类的准确性也大有好处，因为虚词的权重其实是一种噪声。经过这些操作，10万篇新闻两两比较，计算时间也就几分钟而已，如果做几十次迭代，可以在一天内计算完。\n具体的分类算法分为两种情形，第一种，假定我们已经有了已知每一类的特征向量，新的新闻特征向量就可以通过计算它与各类新闻特征向量的余弦相似度，来决定将它划分到哪一类中去。第二种，当我们是类似层次聚类，从第向上，一步一步合并直至类别越累越少，而每个类越来越大，当某一类太大时，这一类里一些新闻之间的相似性就很小了，这是就要停止迭代，这就是自动分类的方法。\n本章介绍的这种新闻归类的方法，准确性好，适用于被分类的文本集合再百万数量级。如果大到亿这个数量级，那么计算时间还是比较长的。对于更大规模的文本处理，将在下一章介绍一种更快速但相对粗糙的方法。\n⑪矩阵运算和文本处理中的两个分类问题 自然语言处理中，最常见的两个分类问题分别是，将文本按主题归类（比如新闻分类）和将词汇表中的字词按意思归类（比如将各种运动的项目名称都归到体育类）。这两个分类问题都可以通过矩阵运算来圆满地、一次性地解决。\n上一章中的文本分类其实是一个聚类问题，关键是计算两篇新闻（向量）的相似度。运用余弦相似度时需要两两计算并且多次迭代，虽然算法漂亮但耗时长。我们希望有一个办法，一次就能把所有新闻相关性计算出来。这个一步到位的方法利用的是矩阵运算中的奇异值分解(Singular Value Decomposition, SVD)。\n$M = UΣV$ $$ M=\\left[ \\begin{matrix} a_{11} \u0026amp; \\cdots \u0026amp; a_{1j} \u0026amp; \\cdots \u0026amp; a_{1N} \\ \\cdots\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\cdots \\a_{i1} \u0026amp; \\cdots\u0026amp;a_{ij} \u0026amp; \\cdots \u0026amp; a_{iN} \\ \\cdots\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\cdots\\ a_{M1} \u0026amp; \\cdots\u0026amp;a_{Mj} \u0026amp; \\cdots \u0026amp; a_{MN} \\ \\end{matrix} \\right] $$ 其中，每一行对应一篇文章，每一列对应一个词，如果有N个词，M篇文章，则得到一个$M \\times N$的矩阵，第i行、第j列的元素$a_{ij}$，是字典中第j个词在第i篇文章中出现的加权词频(比如TF-IDF值)。\n奇异值分解，就是把M，分解成三个小矩阵相乘。这三个矩阵有非常清晰的物理含义。\n由于对角矩阵B对角线上的元素的很多值相对其他的值非常小，或者干脆为0，故可以省略。\n$U$的每一行表示一个词，每一列表示一个语义相近的词类，或者称为语义类，数值越大越相关。\n$V$是对文本的分类结果，它的每一列对应一篇文本，每一行对应一个主题。这一列中的每个元素表示这篇文本在不同主题中的相关性。\n$Σ$表示词的类和文章的类之间的相关性。\n因此只要完成对关联矩阵$M$进行一次奇异值分解，就可以同时完成近义词分类和文章的分类。另外，还能得到每个主题和每个词的语义类之间的相关性。这个结果非常漂亮！\n奇异值分解一般分为两步。首先，将矩阵A变成一个双对角矩阵，这个过程的计算量是$O(MN^2)$，当然这里假设M\u0026gt;N。我们仍然可以利用矩阵$A$的稀疏性大大缩短计算时间。第二步是将双对角矩阵变成奇异值分解的三个矩阵，这一步的计算量只是第一步的零头，可以忽略不计。在文本分类中，$M$对应文本的数量，$N$对应词典大小。奇异值分解的计算复杂度与余弦定理一次迭代的时间复杂度处于一个量级，但是它不需要多次迭代。奇异值分解的另一个大问题是存储量较大，因为整个矩阵都需要存在内存里，而利用余弦定理的聚类则不需要。实际应用中，可以先进行奇异值分解，得到粗分类结果，再利用计算余弦向量的方法，在粗分类的基础上，进行几次迭代，得到比较精确的结果。\n理论的问题解决了，下面就是工程的实际问题，怎么利用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值，以及数值分析的各种算法等就统统派上用场了。对于不大的矩阵，比如几万乘几万的矩阵，用计算机上的数学工具MATLAB就可以计算。但是更大的矩阵，比如上百万乘上百万，奇异值分解的计算量非常大。虽然Google早就有了MapReduce等并行计算工具，但是由于奇异值分解很难拆成不相关的子运算，即使在Google内部以前也无法利用并行计算的优势来进行矩阵分解。直到2007年，Google中国（谷歌）的张智威博士带领几个中国的工程师及实习生实现了奇异值分解的并行算法，这是Google中国对世界的一个贡献。\n⑫信息指纹及其应用 所谓信息指纹，可以简单理解为一段信息（文字、图片、音频、视频等）随机地映射到一个多维二进制空间中的一个点（一个二进制数字）。只要这个随机函数做得足够好，那么不同信息对应的这些点就不会重合，因此，这些二进制的数字就成了原来的信息所具有的独一无二的指纹。\n字符串的信息指纹的计算方法一般分为两步，首先，把这个字符串看成是一个特殊、很长的整数，这一步非常容易，因为在计算机里本来就是这样存储的。接下来就需要用到一个产生信息指纹的关键算法：伪随机数产生器算法（Pseudo-Random Number Generator, PRNG），通过它把任意很长的整数转换成定长的伪随机数，现在常用的算法是梅森旋转算法(Mersenne Twister)。\n另外，信息指纹的不可逆性为网络加密传输提供了很好的支持。一些网站会将用户访问的cookie（本来已经加密，但是如果某一访问所有网站的cookie都一样还是会暴漏行为）通过HTTPS加密。加密的可靠性，取决于是否很难人为地找到具有某一指纹的信息，即解密。从加密的角度来讲，梅森旋转算法还不够好，因为它产生的随机数还有一定的相关性，破解一个就等于破解了一大批。\n在互联网上加密要使用基于加密的伪随机数产生器(Cryptographically Secure Pseudo-Random Number Generator, GSPRNG)。常用的算法有MD5或SHA-1等标准，它们可以将不定长的信息变成定长的128位或160位的二进制随机数。\n除了上述信息指纹可以帮助快速且高效得判断文本相同和互联网加密外，信息指纹在互联网和自然语言处理中还有很多应用。\n集合相同的判定 普通的先将两个集合排序，再进行比较的算法时间复杂度为$O(NlogN)$。或者将第一个表放在哈希表里，再用第二个表比对，这个方法可以达到最佳的$O(N)$时间复杂度，但是额外使用了$O(N)$空间，而且代码复杂。完美的办法是计算这两个集合中元素的指纹和，然后进行比较，加法的交换率，保证了集合的指纹不因元素次序变化而变化。类似如检测网络上某首歌曲是否盗版别人，只需要计算这两个音频的信息指纹即可。\n判定基本相同 判定两个邮箱群发的接收电子邮件地址清单(Email Address List)是否相同，可以用来判断是否群发垃圾邮件。但是如果其中有一点小小的不同，上述信息指纹甚至会产生很大的不同。这个时候可以按照规则，挑选几组电子邮箱地址集，例如尾数为24的地址集与尾数为34的地址集，由于挑选的地址集数量一般为个位数，因此可以得到80%，90%重复这样的结果。又或者对网页中的，刨除常见词如“的”，“是”与只出现一次的噪声词，只需要找出剩下的词中IDF值最大的几个词，并且算出它们的信息指纹即可，为了允许有一定的容错能力，Google采用了一种特定的信息指纹——相似哈希(SimHash)。\n假定一个网页中有若干词$t_1,t_2,\u0026hellip;,t_k$，它们的权重（如TF-IDF）为$w_1,w_2,\u0026hellip;,w_k$。先计算出这些词的信息指纹（假定有8位）。第一步扩展，如$t_1$的信息指纹为10100110，那么对$t_1$的权重$w_1$，逢1相加，逢0相减。这样在k次加减$w$之后，可以获得8个实数值。第二步收缩，这些实数值正数变1，负数变0，就可以重新获得一个8位的二进制数，这个数就是这个网站的SimHash。相似哈希的特点是，相同网页或只有小权重词不同网页的相似哈希必定相同，如果两个网页的相似哈希不同但相差很小，则对应的网页也非常相似。工程上使用的64位相似哈希，如果只差一两位，那么对应网页内容重复的可能性大于80%。\n⑬密码学的数学原理 加密函数不应该通过几个自变量和函数值就能推出函数本身，这样一来，破译一篇密文就可能破译以后全部的密文。\n香农提出的信息论为密码学的发展提供了理论基础，根据信息论，密码的最高境界是敌方在解惑密码后，信息量没有增加。一般来讲，当密码之间均匀分布且统计独立时，提供的信息量最少。均匀分布使地方无从统计，而统计独立可保证敌人即使知道了加密算法，并且看到了一段密码和明码后，无法破解另一段密码。\nDiffie和Hellman在1976年的开创性论文“密码学的新方向”($\\it{New \\ Directions \\ in \\ Cryptography}$)，介绍了公钥和电子签名的方法，这是今天大多数互联网安全协议的基础。\n虽然公开密钥下面有许多不同的具体加密方法，比如早期的RSA算法、Rabin算法、和后来的ElGamal算法、椭圆曲线算法(Elliptic curve)，它们的原理基本一致，并不复杂：\n1.它们都有完全不同的密钥，一个用于加密，一个用于解密\n2.这两个看上去无关的密钥，在数学上是关联的\n以下是一个公开密钥的原理：\n公开密钥的好处：\n1.简单，就是一些乘除而已。\n2.可靠，公开密钥方法保证产生的密文是统计独立而分布均匀的。也就是说，不论给出多少份明文与密文的对应，也无法破解下一份密文。更重要的是N、E可以公开给任何人进行加密，但是只有掌握私钥D的人才可以解密，即使加密者也是无法破解的。\n3.灵活，可以产生很多的公开密钥E和私钥D的组合给不同的加密者。\n最后让我们看看破解这种密码的难度。至今的研究结果表明最彻底的方法还是对大数N进行因数分解，即通过N反过来找到P和Q，这样密码就被破解了。而找到P和Q目前只有一个方法：用计算机把所有可能的数字试一遍，这实际上实在拼计算机的速度，这也就是为什么P和Q都需要非常大。一种加密方法只要保证50年内计算机破解不了，也就算是令人满意了。\n遗憾的是，虽然在原理上非常可靠，但是很多加密系统在工程实现上却留下了不少漏洞。因此，很多攻击者从攻击算法转而攻击实现方法。\n⑭数学模型的重要性 1.一个正确的数学模型应当在形式上是简单的\n2.一个正确的模型一开始可能还不如一个精雕细琢过的错误模型来的准确，但是，如果我们认定大方向是对的，就应该坚持下去\n3.大量准确的数据对研发很重要\n4.正确的模型也可能受噪声干扰，而显得不准确；这时不应该用一种凑合的修正方法加以弥补，而是要找到噪声的根源，这也许能通往重大的发现\n⑮最大熵模型 当需要综合几十甚至上百种不同的信息，最大熵模型可以保留全部的不确定性（除了已知的信息，不做主管假设），将风险降到最小。最初，计算能力还不足时，只能处理计算量相对不太大的自然语言处理问题，拉纳帕体成功将上下文信息、词性（名词、动词和形容词）以及主谓宾等句子成分，通过最大熵模型结合起来，做出了当时世界上最好的词性标识系统和句法分析器。且至今仍然是使用单一方法的系统中效果最好的。在2000年前后，计算机速度的提升以及训练算法的改进，很多复杂的问题，包括句法分析、语言模型和机器翻译都可以采用最大熵模型了。最大熵模型和一些简单组合了特征的模型相比，效果可以提升几个百分点，对于投资这种很少的提升也能带来巨大的利润的行业来说非常有效，所以很多对冲基金开始使用最大熵模型。\n$P(d|x_1,x_2,\u0026hellip;,x_{20}) = \\cfrac{1}{Z(x_1,x_2,\u0026hellip;,x_{20})} e^{λ_1(x_1,d)+λ_2(x_2,d)+\u0026hellip;+λ_{20}(x_{20},d)}$\n其中，归一化因子为：\n$Z(x_1,x_2,\u0026hellip;,x_{20}) = \\sum\\limits_{d}e^{λ_1(x_1,d)+λ_2(x_2,d)+\u0026hellip;+λ_{20}(x_{20},d)}$\n在实现方法上，从GIS(Generalized Iterative Scaling)到IIS(Improved Iterative Scaling)缩短了一到两个数量级，再到吴军找到的数学变换在IIS的基础上减少两个数量级，再加上现代的MapReduce并行计算，现在，再1000台计算机上并行计算一天就可以完成了。\n逻辑回归损失函数的基于伯努利分布的最大似然法，与交叉熵损失函数本质相同（经过log变换后公式相同），可见信息论的思想，变换无穷但又不离本质。\n⑯拼音输入法的数学原理 输入汉字的快慢取决于汉字编码的平均长度诚意寻找这个键所需的时间。单纯地编短编码长度未必能提高输入速度，因为寻找一个键的时间可能变得很长。\n对汉字的编码分为两部分：对拼音的编码和消除歧义性的编码。早期的输入法常常只注重第一部分而忽视第二部分，中国最早的汉字输入微机中华学习机和长城0520采用的都是双拼，即每个字母代表一个或多个声母或韵母，这种输入方法看似节省了编码长度，但是输入一点也不快，一是因为增加了歧义性（很多韵母不得不共享一个字母键），二是双拼比按照拼音的全拼增加了拆字的过程，三是容错性不高，比如an,ang的编码完全没有相似性，一个好的输入法不能要求用户读准每个字的发音。早期的输入法都是这样，一味的追求更少的输入的编码（当然它们都声称自己的编码比其他的输入法更合理，但从信息论的角度上看都是一个水平线上的），而忽视了找到每个键所用的时间，要求普通用户背下这些输入方法里所有汉字的编码是不现实的。\n最终，用户还是选择了拼音输入法。它不需要专门学习，并且输入自然，即找每个键的时间很短，另外因为编码长，有信息冗余，容错性好。于是，拼音输入法要解决的问题只剩下排除一音多字的歧义性了，这也是目前各种拼音输入法的主要工作。接下来我们就分析平均输入一个汉字可以做到最少几次击键。\n按照香农第一定理，对于一个信息，任何编码的长度都不小于它的信息熵。以GB2312，6700个常见字集计算，汉字的信息熵在10个比特以内，如果假定输入法只能用26个字母，那么每个字母可以代表log26≈4.7bit（$2^{4.7}≈26$），也就是输入一个汉字平均需要敲10/4.7≈2.1次键盘。而如果把汉字组成词，再以词为单位统计信息熵，不考虑上下文相关性，那么每个汉字的平均信息熵就会减少为8bit，这也是现在所有输入法都基于词输入的根本原因。如果再考虑上下文相关，可以将每个汉字的信息熵降到6bit，这是输入一个汉字只需要敲6/4.7≈1.3次键盘。但事实上没有一种输入法能接近这个效率，有两个原因，首先，需要根据词频进行特殊编码，而上一段说过特殊编码会增加找到每个键的思考时间，其次上，在个人电脑上，很难安装非常大的语言模型。\n事实上，现在汉语全拼的平均长度为2.98，如果能利用上下文解决一音多字的问题，全拼输入法的平均击键次数应该小于3次。而解决上下文问题，10年前的拼音输入法（以紫光为代表）解决这个问题的办法是建立大词库，但是这种方法多少根据经验和直觉，一直是在打补丁。而利用上下文最好的方法是借助语言模型，目前，各家输入法（Google、腾讯和搜狗）基本处在同一个量级，将来技术上进一步提升的关键在于看谁能准确而有效地建立语言模型。\n拼音转汉字的算法和在导航中寻找最短路径的算法相同，都是动态规划。\n每一条路，都是一个可选结果，找到概率最高的一条就是最优的句子：\n$w_!,w_2,\u0026hellip;,w_N = \\mathop{ArgMax}\\limits_{w∈W}P(W_1,W_2,\u0026hellip;,w_n|y_1,y_2,\u0026hellip;,y_N)$\n上述公式的简化方法：\n$w_!,w_2,\u0026hellip;,w_N = \\mathop{ArgMax}\\limits_{w∈W}P(y_1,y_2,\u0026hellip;,y_N|W_1,W_2,\u0026hellip;,w_n)\\ \\cdot \\ P(W_1,W_2,\u0026hellip;,w_n)≈ \\mathop{ArgMax}\\limits_{w∈W}\\prod\\limits_{i=1}^{N}P(w_i|w_{i-1})·P(y_i|w_i)$\n如果对上述简化公式中的概率取对数同时取反，即定义$d(W_{i-1},W_i)=-logP(w_i|w_{i-1})·P(y_i|w_i)$，上面的连乘关系，就变成了加法关系，寻找最大概率的问题，就变成了最短路径问题。\n当输入速度超过一定阈值后，用户的体验可能更重要，客户端上虽然不能放置太大的语言模型，但是可以建立个性化的语言模型。那么就有两个问题：如何训练一个个性化的语言模型，其次是怎样处理好它和通用语言模型的关系。为某个人训练一个特定的词汇量在几万的二元语言模型需要几千万的语料，这对一个人来说可能一辈子也达不到，没有足够多的训练数据，高阶的语言模型根本没用，一些输入法就找到了一种经验做法：用户词典，这实际上是一个小规模的一元模型加上非常小量的元组（比如一个用户定义的词ABC，实际是一个三元组）。\n更好的办法是找到大量符合用户经常输入的内容和用语习惯的语料，训练一个用户特定的语言模型。这就要用到余弦定理和文本分类的技术了，即找到某个人输入文本的特征向量Y和已有文本$X_1,X_2,\u0026hellip;,X_{1000}$，选择前K个和Y距离最近的类对应的文本，作为这个特定用户语言模型的训练数据。\n大部分情况下，$M_1$对这个特定用户的输入比通用模型$M_0$好，但是对于相对冷僻的内容，$M_1$的效果就远不如$M_0$了，所以我们需要综合两个模型。把各种特征综合在一起的最好方法是采用最大熵模型，当然，如果为每个人都建立这样一个模型，成本较高。因此可以采用一个简化的模型：线性插值模型，能得到最大熵模型约80%的收益，Google拼音输入法的个性化语言模型就是这么实现的。\n⑰布隆过滤器 判断一个元素是否在一个集合中，按12章信息指纹所描述的，可以通过8个字节的信息指纹哈希表，但由于哈希表的存储效率一般只有50%，因此一个元素需要占用16个字节，一亿个地址大约要1.6GB，即16亿字节的内容，那么存储几十亿个邮件地址需要上百GB，除非是超级计算机，一般服务器是存不下的。\n而我们现在介绍的一种称作布隆过滤器的数学工具，它只需要哈希表$\\cfrac{1}{8}$到$\\cfrac{1}{4}$的大小就能解决同样的问题。布隆过滤器(Bloom Filter)是由伯顿·布隆(Burton Bloom)于1970年提出的，它实际上是一个很长的二进制向量和一系列随机映射函数。\n假定存储一个亿个电子邮件，那么我们使用16亿个比特位，每个比特位初始值为0。对于每个电子邮件地址X，用8个不同的随机数产生器（$F_1,F_2,\u0026hellip;,F_8$）产生8个信息指纹，再用一个随机数产生器G把这8个信息指纹映射到1-16亿中的8个自然数$g_1,g_2,\u0026hellip;,g_8$。然后把这8个位置的比特位全部设置为1。在把所有邮件都处理后，一个针对这些电子邮件的布隆过滤器就建成了。当有一个新的电子邮件地址Y时，就将Y重复上述的处理结果，然后检查这8个位置是否都是1就行了。当然根据这个逻辑，布隆过滤器决不会漏掉任何一个可以地址，但是，当某个好的电子邮件对应的8个位置都恰好被设置成1时，就会被误判为黑地址。好在这种可能性很小，对于一个元素的集合，这种误识别率在万分之一以下。常见的补救方法是建立一个小的白名单，存储那些可能被误判的地址。\n误判率的计算，先计算某个比特被设为1个概率，根据伯努利分布，$1-(1-\\cfrac{1}{m})^{kn}$。而对于一个新的不在集合中的元素，它的k个bit每个都命中的概率为$(1-(1-\\cfrac{1}{m})^{kn})^{k}$，化简后为\n$P = (1-e^{-\\cfrac{ln{(\\cfrac{m}{n}ln2)n}}{m}})^{ln(\\cfrac{m}{n}ln2)}$\n如果n比较大，可以近似为\n$(1-e^{-k(n+0.5)/(m-1)})^k ≈ (1-e^{\\cfrac{kn}{m}})^k$\n假定一个元素用16bit，即k=8，那么假阳性的概率为万分之五。当然这根据m(布隆过滤器的总bit)，n(集合中元素数)，k(判定一个元素使用的bit数)的不同有所差别。根据直觉，m越大误判率越小，n越大误判率越大，而k从1开始误判率会经过一个先减小再增加的过程。\n⑱马尔可夫\u0026quot;网络\u0026quot;——贝叶斯网络 马尔可夫链描述了一种状态序列，其每一个状态值取决于前面有限个状态，但实际问题中，很多事物相互的关系并不能用一条链串起来，很可能是交叉的、错综复杂的，如图：\n每一个edge都表示一个量化的可信度(belief)，马尔可夫假设保证了贝叶斯网络便于计算，在网络中，每个节点的概率，都可以用贝叶斯公式来计算，贝叶斯网络因此得名，又因为每个edge都有一个可信度，贝叶斯网络也被称作信念网络(Belief Networks)。\n使用贝叶斯网络必须先确定这个网络的拓扑结构，然后还要知道各个状态之间相关的概率，得到拓扑结构和这些参数的过程分别叫做结构训练和参数训练，统称训练。和训练马尔可夫模型一样，训练贝叶斯网络要用一些已知的数据，并且贝叶斯网络的训练比较复杂，从理论上讲，它是一个NP-hard问题。但是对于某些应用，这个训练过程可以简化，并在计算机上实现。\n优化的贝叶斯网络要保证它产生的序列从头走到尾的可能性最大，即后验概率最大。当然产生一个序列可以有多条路径，从理论上讲，需要完备的穷举搜索（Exhaustive Search），但这就是Np-Hard的了，因此一般采用贪心算法（Greedy Algorithm），也就是在每一步时，沿着箭头的方法寻找有限步，为了防止陷入局部最优，可以采用蒙特卡洛（Monte Carlo），用许多随机数在贝叶斯网络中试一试，看看是否陷入局部最优，这个方法计算量比较大。最近，新的方法是利用信息论，计算节点之间两两的互信息，只保留互信息较大的节点直接的连接，然后再对简化了的网络进行完备的搜索，找到全局优化的结构。\n找到结构后就要对参数进行训练了，假定这些权重用条件概率来度量，我们需要优化贝叶斯网络的参数，使得观察到的这些数据的概率（即后验概率）$P(D|θ)$达到最大，这个过程就是EM过程。在计算后验概率时，计算的是条件X和结果Y之间的联合概率$P(X,Y)$，我们的训练数据会提供一些$P(X,Y)$之间的限制条件，而训练出来的模型要满足这些条件。并且这个模型应该是满足最大熵模型的，因此涉及最大熵模型的训练方法在这里都能使用。\n值得一提的是，结构的训练和参数的额训练通常是交替进行的。IBM华生实验室的茨威格博士(Geoffrey Zweig)和西雅图华盛顿大学的比尔默教授（Jeff Bilmer）完成了一个通用的贝叶斯网络的工具包，提供给贝叶斯网络有兴趣的研究者免费使用。\n贝叶斯网络再图像处理、文字处理、支持决策等方面有很多应用。在文字处理方面，语义相近的词之间的关系可以用一个贝叶斯网络来描述，我们利用贝叶斯网络，可以找出近义词和相关的词，因而在Google搜索和Google广告中都有直接的应用。\n贝叶斯网络可用于分析文本，抽取概念，分析主题，这是一种主题模型（Topic Model）。前面提到的余弦相似性和奇异值分解方法都是主题模型的一种。这里介绍通过贝叶斯网络建立的另一种模型——Google的Rephil。在主题模型中，一个主题可以包含多个词，一个词也可以属于多个主题，那么就可以用贝叶斯网络建立一个文章、概念和关键词之间的联系：\n其中，文章和关键词本身有直接的关联，它们两者都还和概念有直接关联，同时它们通过主题还有间接的关联。\n其他参考资料：\n比尔默于茨威格共同发表的论文：http://people.ece.uw.edu/~bilmes/pgs/sort_Date.html\n斯坦福大学科勒（Daphne Koller）教授的巨著：$\\it{Probabilistic \\ Graphical \\ Models \\ :Principles \\ and \\ Techniques}$\n⑲条件随机场、文法分析与其他 句法分析（Sentence Parsing），指根据文法对一个句子进行分析，建立这个句子的语法树，即句法分析（Syntactic Parsing）。20世纪80年代前，人们受形式语言学的影响，采用基于规则的方法，这种方法规则非常多，并且一旦某一步走岔了，需要回溯很多步，计算复杂度大的不得了。20世纪80年代后，布朗大学计算机系的计算语言学家尤金·查尼阿克（Eugene Charniack）统计出文法规则的概率，在选择文法规则时坚持一个原则——让被分析的句子的语法树概率达到最大。马库斯的学生拉纳帕提把文法分析看成是一个括括号的过程，每次从左到右扫描句子的每个词（或句子成分）时，只需要判断是否属于以下三个操作之一：\nA1.是否开始一个新的左括号，比如在“美联储”是新括号的开始。\nA2.是否继续留在这个括号中，比如在“保险公司”的位置，是否继续留在括号中。\nA3.是否结束一个括号，即标上右括号，比如“资金”的位置是一个括号的结束。\n为了判断采用哪个操作，拉纳帕提建立了一个统计模型$P(A|prefix)$，其中A表示行动，句子前缀$prefix$是指句子从开头到目前为止所有的词和语法成分。拉纳帕提用最大熵模型实现了这个模型，当然，拉纳帕提还用了一个统计模型来预测句子成分的种类。但是这种方法对于非常“规矩“的语句，分析正确率在80%以上，但是2000年后，随着互联网的普及，非严谨的句子中这个模型的正确率连50%也达不到，所幸在很多自然语言处理的应用中，并不需要对语句做深入的分析，只要做浅层的分析（Shallow Parsing），比如找出句子中主要的词语以及它们之间的关系即可。\n到了20世纪90年代以后，随着计算机计算能力的增强，科学家们采用了一种新的数学模型工具——条件随机场，大大提高了句子浅层分析的正确率，可达到95%，使文法分析得以应用到很多产品，比如机器翻译上。\n⑳维特比和他的维特比算法 维特比算法是一个特殊但应用最广的动态规划算法，维特比算法是针对一个特殊的图——篱笆网络（Lattice）的有向图最短路径问题而提出的。它之所以重要，是因为凡是使用隐马尔可夫模型描述的问题都可以用它来解码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词等。\n以输入法拼音转汉字为例，输入可见的序列$y_1,y_2,\u0026hellip;,y_N$，而产生他们的隐含序列使$x_1,x_2,\u0026hellip;,x_N$，$P(x_i|x_{i-1})$是状态之间的转移概率，$P(y_i|x_i)$是每个状态的产生概率。这个马尔可夫链的每个状态的输出是固定的，但是每个状态的值可以变化，比如输出读音”zhong“的字可以是”中“”钟“等多个字。\n用符号$X_{ij}$表示状态$x_i$的第$j$个可能的值，如果把每个状态按照不同的值展开，就得到下图的篱笆网络\n每一条路径都能产生我们观察到的输出序列Y，我们要找到的就是最可能（P最大）的一条路径。但是这样的路径组合数非常多，假定句长为10个字，那么这个组合数为$13^5~5 \\times10^{14}$。假定计算每条路径需要20次乘法，就是$10^{16}$次计算，而今天的处理器每秒计算$10^{11}$次的话，也需要一天时间。因此需要一个最好能和状态数目成正比的算法，这就是维特比算法。\n维特比算法的基础可以概括成以下三点：\n1.如果概率最大的路径$P$（或者说最短路径）经过某个点，那么这条路径上从起始点$S$到这个点的子路径$Q$一定是$S$到这个点的最短距离，否则，用$S$到这个点的最短路径$R$替代$Q$，便构成了一条比P更短的路径，这显然是矛盾的\n2.从$S$到$E$路径必定经过第$i$时刻的某个状态，即对于每个$x$，都要经过它的一个状态，不能跳过其中一个时刻。假定第$i$时刻有$k$个状态，那么如果记录了从$S$到第$i$个状态的所有$k$个节点的最短路径，那么最终的最短路径一定经过其中一条。这样，在任何时刻，只要考虑非常有限条候选路径即可\n3.结合上述两点，假定当我们从状态$i$进入状态$i+1$时，从$S$到状态$i$上各个节点的最短路径已经找到，那么在计算从起点$S$到第$i+1$状态的某个节点的最短路径时，只要考虑从S到钱一个状态$i$所有的$k$个节点的最短路径，以及从这$k$个节点到$x_{i+1},j$的距离即可\n基于上述三点基础，维特比算法过程如下：\nstep1.从S出发，对于第一个状态$x_1$的各个节点，不妨假定有$n_1$个，计算出$S$到它们的距离$d(S,x_{1i})$，其中$x_{1i}$代表任意状态1的节点。因为只有一步，所以这些距离都是S到它们各自的最短距离\nstep2.对于第二个状态$x_2$的所有节点，要计算出从S到它们的最短距离。路径长度$d(S,x_{2i}) = d(S,x_{1j})+d(x_{ij},x_{2i})$，由于$j$有$n_1$种可能，我们要一一计算，然后找到最小值，即\n$d(S,x_{2i})=min_{I=1,n_1}d(S,x_{1j})+d(x_{1j},x_{2i})$\n此时，第二个状态有多少个个节点（假设每个节点只有一条最短路径），就有多少个可能路径，与第一个状态的节点数就无关了，这样一直走到最后一个状态，就得到了整个网格从头到尾的最短路径，每一步计算的复杂度都和相邻两个状态$S_I$和$S_{i+1}$各自的节点数$n_i,n_{i+1}$成正比，即$O(n_i \\cdot n_{i+1})$。如果假定在这个隐马尔可夫链中节点最多的状态有D个节点，也就是说整个网格的宽度为D，那么任何一步的复杂度不超过$O(D^2)$，由于网格长度是$N$，所以整个维特比算法的复杂度是$O(N \\cdot D^2)$。回到上述那个输入法问题，计算量基本上是$13 \\times 13 \\times 10 = 1690≈10^3$，这和原来的$10^{16}$有天壤之别。更重要的是，维特比算法与长度$N$成正比，无论是在通信中，还是在语音识别、打字中，输入都是按照流（Stream）的方式进行的，制药处理每个状态的时间比讲话或者打字速度快（这点很容易做到），那么无论输入有多长，解码过程永远是实时的。\n不知道有没有同学对这个算法的原理有非常似曾相识的感觉，其实这个算法与最短路径算法中的Dijkstra算法很相似——它们的基础理论是相似的，但是具体实现及应用场景有些许不同。首先，维特比只能用于有向无环图求最长最短路径，因为有环的时候 维特比的动态规划的递推公式会相互依赖形成类似于死锁的结构，没法解出来。其次，Dijkstra不适用于有负权值的场景。再次，Dijkstra是找一个节点到其他所有节点到最短路径，viterbi则是在篱笆网络里找一条从起点到终点（可能有多个起点，多个终点）的最短路径。HMM，CRF等算法中使用Viterbi算法，是因为Viterbi算法的效率比Dijsktra算法高。\n㉑CDMA技术——3G移动通信的基础 最早，海蒂·拉玛尔（Hedy Lammarr）与她的邻居乔治·安泰尔（George Antheil）一道发明了一种称为”保密通信系统“的调频通信技术。这种传输方式是在一个较宽的扩展频带上进行的，因此它称为扩频传输(Spread-Spectrum Transmission)。和固定频率的传输相比，它有三点明显的好处：\n1.它的抗干扰能力强。当有人想用噪音干扰固定的广播频率时，对于扩频传输来说基本不可能，因为不能把所有的频带都干扰了，否则整个国家的通信就中断了。\n2.扩频传输的信号很难被截获。以极低的功率在很宽的频带上发送加密信号，对于试图截获者来讲，这些信号能量非常低，很难获取。\n3.扩频传输利用带宽更充分。固定频率的通信由于邻近的频率相互干扰，载波频率的频点不能分布得太密集，两个频点之间的频带就浪费了。扩频通信由于抗干扰能力强，浪费的频带较少。\n虽然这种扩频技术和调频技术早在20世纪60年代就应用于军事，但是转为民用则是20世纪80年代以后的事情，在CDMA以前，移动通信使用过两种技术：频分多址(FDMA)和时分多址(TDMA)。\n频分多址，是对频率进行切分，每一路通信使用一个不同的频率，对讲机采用的就是这个原理。由于相邻频率会互相干扰，因此每个信道要有足够的带宽。如果用户数量增加，总带宽就必须增加。我们知道空中的频带资源是有限的，因此要么必须限制通信人数，要么降低话音质量。\n时分多址是将同一频带按时间分成很多份。每个人的（语音）通信数量在压缩后只占用这个频带传输的$\\cfrac{1}{N}$时间，这样同一个频带可以被多个人同时使用了。第二代移动通信的标准都是基于TMDA的。\n前面讲了，扩频传输对频带的利用率比固定频率传输高，因此，如果把很多细分的频带合在一起，很多路信息同时传输，那么应该可以提高带宽的利用率。\n由于每个发送者有不同的密码，接收者在接到不同信号时，通过密码过滤掉自己无法解码的信号，就可以避免相邻频率干扰的问题。由于这种方法是根据不同的密码区分发送的，因此称为码分多址(CDMA)。\n㉒上帝的算法——期望最大化算法 在一般性问题中，如果有非常多的观测数据，定义一个最大化函数，经过若干次迭代，我们需要的模型就训练好了。这实在是太美妙了。\n前面介绍过的很多算法，其实都是EM算法，比如隐马尔可夫模型的训练方法Baum-Welch算法，以及最大熵模型的训练方法GIS算法。在Baum-Welch算法中，E过程就是根据现有的模型计算每个状态之间转移的次数（可以是分数值）以及每个状态产生它们输出的次数，M过程就是根据这些次数重新估计隐马尔可夫的参数。这里最大化的目标函数就是观测值的概率。在最大熵模型的通用迭代算法GIS中，E过程就是跟着现有的模型计算每一个特征的数学期望值，M过程就是根据这些特征的数学期望值和实际观测值的比值，调整模型参数。这里，最大化的目标函数是熵函数。\n值得一提的是，在凸函数中，EM算法一定能达到最优解，但是在非凸函数中，EM算法可能陷于局部最优解。\n㉓逻辑回归和搜索广告 搜索广告基本上走过了三个阶段。第一个阶段以早期的Overture和百度的广告系统为代表，按广告主出价高低来排名。第二个阶段，Google按照预测哪个广告可能被点击，综合出价和点击率(Click Through Rate, CTR)等因素决定广告的投放。第三个阶段是进一步的全局优化。这章主要介绍第二阶段。\n预估点击率有以下几点问题，首先，对于新广告的冷启动问题，第二，点击数据少，统计数据不足的问题，第三，广告的点击量显然与展示位置有关，因此，在预估点击率时，必须消除这些噪音。最后还要指出，影响点击率的因素非常多，这些都是在预估点击率时要考虑的。\n需要整合这些特征，工业界普遍采用了逻辑回归模型（Logistic Regression Model）。逻辑回归作为最基础的广义线性模型，这里就不多介绍了。一个广告系统中，点击率预估机制的好坏决定了能否城北提高单位搜索的广告收入。而目前Google和腾讯的广告系统在预估点击率都采用了逻辑回归函数。\n另外提一句，CTR在推荐中的实现，现在工业上会使用DeepFM、Wide\u0026amp;Deep等深度学习模型。\n㉔分布式和Google云计算的基础 MapReduce实Google云计算的基础，将一个大任务拆分成小的子任务，并且完成了子任务的计算，这个过程叫做Map，将中间结果合并成最终结果，这个过程叫做Reduce。\n㉕Google大脑和人工神经网络 无论是在计算机科学、通信、生物统计和医学，还是在金融和经济学（包括股市预测）中，大多数与“智能”有点关系的问题，都可以归结为一个在多维空间进行模式分类的问题。模式分类的任务就是要在空间里切一刀，将多个类分开，如语音识别中，就是把韵母a和e分开，有了深度学习，我们就可以实现特征工程自动化了。\n除了神经元之间的线性关系之外，毕竟线性关系上的线性关系还是线性关系，并无意义，所以为了实现复杂的弯曲的世界，我们会增加非线性映射。为了通用性，一般在人工神经网络中，规定神经元函数只能对输入变量线性组合后的结果进行一次非线性变换。请注意是组合后的结果，将每一个输入值都进行非线性变换后再线性组合在一起是不被允许的（这样的计算量太大了）。\n从理论上讲，人工神经网络只要设计得当，就可以实现任何复杂曲线（在高维空间里是曲面）的边界。\n总的来说，人工神经网络是一个分层的有向图，第一层输入节点$X_1,X_2,\u0026hellip;,X_n$接受输入的信息，也成为输入层。来自这些点的数值$x_1,x_2,\u0026hellip;,x_n$按照它们输出的弧的权重($w_0,w_1,w_2,\u0026hellip;,w_n$)进行加权，然后再做一次函数变换$f(G)$，赋给第二层的节点Y。\n第二层的节点照此将数值向后传递，直到最后一层，最后一层又被称为输出层，输出层哪个节点的数值最大，输入的模式就被分在了哪一层。\n在人工神经网络中，需要设计的部分只有两个，一个是它的结构，即网络分几层，每层几个节点，节点之间如何连接等等；第二就是非线性函数$f(·)$的设计，常用的函数有指数函数，sigmoid等，在指数函数时，它的模式分类能力等价于最大熵模型。\n值得指出的是，如果我们把不同输出节点上得到的值看成是一种概率分布，那么实际上人工神经网络就等价于一个概率模型了，比如前面提到的统计语言模型。\n关于人工神经网络的训练，一般情况下，在拥有大量训练数据时，我们定义一个cost function，然后按照梯度下降法找到让成本达到最小的参数，当然除了书中提到的外，还有很多值得我们去深入学习的内容，例如，反向传播导致梯度消失和爆炸，以及衍生的用于不同场景的深度学习模型，例如用于图像的CNN、用于推荐的DeepFM等。\n上述都是在有大量训练数据时的选择，当我们无法获得大量标注好的数据时，就需要借助无监督的训练得到人工神经网络的参数。此时，我们需要定义一种新的成本函数，它能够在不知道正确的输出值的情况下，确定（或者预估）训练出的模型是好是坏。最简单的，我们希望分完类后，同一类样本应该靠的比较近，而不同类比较远，这样就可以把每一个样本点到训练出来的聚类中心（Centroid）的欧氏距离作为成本函数。对于估计语言模型的条件概率，就可以用熵作为成本函数。定义了成本函数后，就可以用梯度下降法进行无监督的参数训练了。对于结构复杂的人工神经网络，它的训练计算量非常大，而且是个Np-Hard问题（梯度下降可能会陷入局部最优），因此有很多机器学习的专家在寻找各种好的近似方法。\n需要指出的是，人工神经网络的规模决定了它能做多大的事情，但是要想要人工神经网络上规模并非易事，因为网络节点之间是连接的，它的复杂度会随着网络规模的扩大呈指数上升。幸运的是，从20世纪90年代到2010年，计算机处理能力的增长和云计算的兴起让人工神经网络的用处大大增加，并且因为并行计算，过去训练人工神经网络的方法就必须改变，以适应云计算的要求。Google大脑就是在这样的前提下诞生的，其创新之处也在于利用了云计算的并行处理技术。\nGoogle大脑采用人工神经网络主要有以下三点原因：首先，人工神经网络理论上可以在多维空间“画出”各种形状的模式分类边界，有很好的通用性。其次，人工神经网络的算法比较稳定且通用，可以一次设计长期并多场景使用。第三，人工神经网络容易并行化，被贝叶斯网络则不然。\nGoogle大脑的实现与MapReduce的分治思想相似，但是更加复杂。它会把一整个神经网络模型切成数千块，与MapReduce不同，每一块的计算并不是完全独立的，而是要考虑上下左右很多块，相互的关联总数大致和块数的平方成正比，虽然会让这一部分关联的计算变得复杂，但是达到了分解一个大任务成多个小任务的目的。\n除了并发训练，Google大脑在减少计算量方面做了两个改进。首先，降低了每次迭代的计算量，Google大脑采用了随机梯度下降（Stochastic Gradient Descent）,这种算法秩序随机抽取少量数据来计算成本函数，可以大大降低计算量，虽然会牺牲一点点准确性。第二个改进是减少迭代次数，Google大脑采用比一般梯度下降法收敛的更快的L-BFGS方法（Limited-memory Broyden Fletcher Goldfard Shanno Method），其原理和随机梯度法相似，但是略微复杂一些。它的好处是可以根据离最后目标的“远近”调整每次迭代的步长，这样经过很少次的迭代就能收敛，但是它每一次迭代的计算量也会增加一点（因为要计算二阶导数）。另外，L-BFGS方法更容易并行化计算。借助这两点，Google大脑才能完成外界认为计算量大的难以承受的人工神经网络的训练任务。\n㉖人工神经网络与贝叶斯网络 共同点：\n1.它们都是有向图，每一个节点的取值只取决于前一级的节点，而与更前面的节点无关，也就是说遵从马尔可夫假设\n2.它们的训练方法相似\n3.对于很多模式分类问题，这两种方法在效果上相似，也就是说很多用人工神经网络解决的问题，也能用贝叶斯网络解决，反之亦然，但是它们的效率可能会不同。如果把人工神经网络和贝叶斯网络都看成事统计模型，那么这两种模型的准确性也是类似的\n4.它们的训练计算量都特别大，大家在使用人工神经网络时要有心理准备\n不过，人工神经网络与贝叶斯网络还是有不少差别的：\n1.人工神经网络在结构上完全是标准化的，而贝叶斯网络更灵活。Google大脑选用人工神经网络，就是因为看中了它的标准化这一特点\n2.虽然神经元函数为非线性函数，但是各个变量智能先进性线性组合，然后对一个变量（即前面组合出来的结果）进行非线性变换，因此用计算机实现起来比较容易。而在贝叶斯网络中，变量可以组合成任意的函数，毫无限制，在获得灵活性的同时，也增加了复杂性\n3.贝叶斯网络更容易考虑（上下文）前后的相关性，因此可以解码一个输入的序列，比如将一段语音识别成问题，或者将一个英语句子翻译成中文。而人工神经网络的输出相对孤立，它可以识别一个个字，但是很难处理一个序列，因此它主要的应用常常是估计一个概率模型的参数，比如语音识别中声学模型参数的训练、机器翻译中语言模型参数的训练等，而不是作为解码器\n㉗区块链——椭圆曲线加密原理 想要保护私有信息，特别是隐私，必须有一套比对称的机制，做到在特定授权的情况下，不需要拥有信息也能使用信息；在不授予访问信息的权限时，也能验证信息。比特币的意义就在于，它证实了利用区块链能够做到上述这两件事。\n㉘量子密钥分发——随机性带来的好处 数据泄露无外乎两种可能：在数据存储的地方被盗取，或者在数据传输的过程中被解惑。要解决这个问题，最好的方法就是对数据进行加密。那么是否存在一种无法破解的密码呢？其实信息论的发明人香农早就指出了，一次性密码从理论上讲永远是安全的。而近年来非常热门的量子通信，便是试图实现加密密钥的安全传输，确保保密通信不被识破。这就是量子密钥分发技术。量子通信并不是像很多媒体曲解的那样——靠量子纠缠实现通信，而是靠光量子的偏振特性承载信息，靠数学和信息论的基本原理保证它的保密性。\n光子既是一种粒子，又是一种波，其传播方向与振动方向垂直，这就是爱因斯坦指出的光的波粒二象性。光子的振动频率和偏振的方向都可以人为控制，激光震动的频率已应用到激光通信中，而量子密钥分发利用了光子的偏振特性。即，我们可以在发送方调整光的振动方向来传递信息，比如把光偏振的方向调成水平的，代表0；调成垂直的，代表1。在接收端，我们放置一个垂直的偏振镜就能检测到所传递来的垂直信息，我们收到信号，就认为发送方送来的信息是1。当然，这么做不是很可靠，因为没有收到信号时，不容易确认是对方没有发送，还是发送过来的是0，因此，更好的办法就是在接收方用一个十字交叉的光栅，让垂直和水平的信号都通过，这样就不会把信息0与没有发送信息这件事混淆了。\n当然，（激光）光子的偏振方向可以有各种角度，未必一定要是水平或者垂直的，而如果偏振的方向是其他角度，经过一个水平的光栅，它是否能通过就是随机的了。例如，如果发送方发射了一个偏振方向是45°的光子，它经过垂直光栅的概率是50%，即被检测为1的概率为50%、这个信息就是随机的了。\n利用这个特性，我们就可以来分发密钥了，具体做法是这样的。首先，发送方和接收方约定好有两组信息编码方式，一组用垂直的偏振光代表1，水平的代表0，另一组则分别用45°和135°代表1和0。其次，发送方采用哪种编码方式完全是随机的，而且是交替进行的，它并不告诉接收方。接收方也随机调整偏振镜（光栅）的方向。此时的期望值如下，有50%的可能接收方和发送方的解调和调制方法一致，另外50%不一致的情况下，又有50%的几率随机蒙对，也就是说，不论接收方如何设置偏振镜解调的方向，最后得到的信息大约有75%是一致的，或者说误码率为25%。\n如果在传输过程中，信息被中间的窃听者截获了。由于光子在经过被错误放置的光栅中，光子的偏振方向就无从得知了，得到的是0还是1是完全随机的（可以被认为是噪声）。如果窃听者再将这些信息转发给原本的接收者，接收者得到的信息只有$75% \\times 75% = 56.25%$。接下来，如果接收方再将自己的信息发还给发送者确认，发送者就会发现只有56.25%的一致性，这时他们就知道信息被截获了。\n解决了信息安全问题后，我们就需要消除信息的不确定性，来确定以下双方通信的密钥。这一步其实非常简单，发送方只要用明码将它调制偏振方向的基传给接收方即可。这样，接收方就知道在哪些信息位它设置对了（其实是蒙对了），然后再用明码把它设置对的信息位告诉发送方即可。此时这个密钥一般有一半传送信息的长度（偏振器设置对的概率为50%），且完全是随机的。上述通信都是明码进行的，但是因为窃听者并不知道原本的信息，所以获知哪些位置应该设置什么样的解调器并没有意义，并没有破坏密钥的安全性。\n上述这种通信协议被称为BB84协议，时查理斯·贝内特(Charles Bennett)和吉勒·布拉萨(Gills Brassard)在1984年发表的。后来，人们又在这个协议的基础上进行改进，有了其他的协议，但是其加密和通信原理并没有本质的变化。\n在使用上述协议通信的过程中，发送方和接收方需要通过几次通信彼此确认密钥，而这个密钥只使用一次。如果需要继续通信，就需要再产生和确认新的密钥。因此，这种做法实际上是用时间换取通信的安全性。\n值得一提的是，量子通信绝不像很多媒体讲的是万能的。加入通信卫星真的被黑客攻击了，或者通信的光纤在半途被破坏了，虽然通信的双方知道有人在偷听，能够中断通信，不丢失保密信息，但是于此同时，它就无法保证正常的信息能送出去了，就如同情报机关虽然抓不到对方的信使，却能把对方围堵在家里，不让消息发出。此外，虽然今天已经实现了上千千米量级的量子密钥分发，但是量子通信从实验到工程，再到商用，还有很长的路要走。\n㉙数学的极限——希尔伯特第十问题和机器智能的极限 就如物理学上无法超越的光速极限或绝对零度的极限一样，人工智能的能力有数学的边界。这一边界与技术无关，仅取决于数学本身的限制。具体到今天大家使用的计算机，它有着两条不可逾越的边界，分别是由图灵和希尔伯特划定的。\n图灵划定计算机可计算问题的边界：机器智能显得极为强大，靠的是人们找到了让及其拥有智能的方法，即大数据、摩尔定律和数学模型这三个支柱。我们在前面各个章节中所介绍的内容，其实依然只是一部分数学模型而已。这些数学模型将各种形形色色的实际问题变成了计算问题，当然，这里面有一个前提，就是那些问题本质上就是数学问题，而且是可以用计算机计算的数学问题。但是，当计算机科学家们揭开了一个又一个这样的问题的数学本质之后，人们自然会贪心地以为这样的进步是没有极限的，一致浪费时间去解决根本解决不了、可能也没有必要解决的问题。图灵思考问题的方式恰恰和常人一步步进步的方式相反，他会先划定计算这件事情的边界，在他眼中边界内的问题都是可以通过计算来解决的，当然在边界外可能还有更多的问题，它们与计算无关，无法通过计算来解决，因此图灵并不打算考虑它们。图灵就划定了这样一条边界，后人就不必再浪费时间纠结没有意义的事情，也就不必试图超越边界或极限做事情。\n计算对应于确定性的机械运动，这保证了在相同条件下计算出来的结果是可重复的，而人的意识则可能来自于测不准原理（量子力学中一个微粒的位置与速度无法同时被准确测量，即越精确的知道速度，就越测不准位置，而人则是由无数的测不准堆积出来的意识）。所以，关于人的意识，图灵认为是不确定的，不属于计算的范畴，如果真是像图灵想的那样，那么宇宙本身就存在着大量数学问题之外的问题。事实上，与图灵同时代的数学家哥德尔在1930年便证明了数学不可能既是完备的，又是一致的，也就是说，一些命题即使是对的，我们也无法用数学证明它们。这被称为哥德尔不完全性定理，它说明数学的方法不是万能的。事实上除了意识上的不确定性，一些数学问题也是无法用数学证明的，今天很多数学问题并没有数学答案，比如不存在一组正整数$x,y,z$，让它们满足$x^3+y^3=z^3$。这个问题实际上是著名的费尔马大定理的一个特例，而这个定理本身已于1994年由英国著名的数学家怀尔斯（Andrew Wiles）证明了，也就是说，它是无解的。当然，不管怎么说，知道一个问题无解也算是有了答案。但是，是否还存在一些问题，我们根本无法判定答案存在与否呢？如果有，那么这类问题显然是无法通过计算来解决的。在这一方面给了图灵启发的是希尔伯特。\n1900年，希尔伯特在国际数学大会上提出了23个（当时还无解的）著名的数学问题，其中第十个问题讲的是：\n“任何一个（多项式）不定方程，能否通过有限步的运算，判定它是否存在整数解”\n所谓不定方程（也被称为丢番图方程，Diophantine equation），就是指有两个或更多未知数的方程，它们的解可能有无穷多个。为了对这个问题有根性认知，我们来看三个特例：\n例一 $x^2+y^2=z^2$\n这个方程有三个未知数，它有很多正整数解，每一组解其实就是一组勾股数，构成直角三角形的三边。\n例二 $x^N+y^N=z^N$\n这些方程都没有正整数解，这就是著名的费尔马大定理。\n例三 $x^3+5y^3=4z^3$\n这个方程是否有正整数解，就不那么直观了。更糟糕的是，我们没有办法一步一步地判定它是否存在整数解。此外，需要指出的是，即使我们能判定它有整数解，也未必找得出来。\n希尔伯特第十问题在1970年被苏联数学家尤里·马蒂亚塞维奇（Yuri Matiyasevich）严格地证明了，除了极少数特例，在一般情况下，无法通过有限步的运算，判定一个不定方程是否存在整数解，那么就说明很多数学问题其实上帝也不知道答案是否存在。对于连答案存在都无法判定的问题，答案自然是找不到的，我们也就不用费心去解决这一类问题了。正是希尔伯特对数学问题边界的思考，让图灵明白了计算的极限所在。\n第十问题向世人宣告了很多问题我们无从得知是否有解。如果连是否有解都不知道，就更不可能通过计算来解决它们了。更重要的是，这种无法判定是否有解的问题，要远比有答案的问题多得多。即，有答案的问题\u0026mdash;\u0026gt;可判定的问题\u0026mdash;\u0026gt;数学问题\u0026mdash;\u0026gt;所有问题。\n那么对于可判定且有答案的数学问题是否都能够用计算机解决吗？那就要看看计算机是怎么设计的了，1963年，图灵提出了一种抽象的计算机的数学模型，这就是后来人们常说的图灵机，图灵机的核心思想就是用机器来模拟人进行数学运算的过程，这个过程其实是在不断重复两个动作：在纸上写或擦掉一些符号，用笔在纸上不断移动书写位置。图灵机这种数学模型在逻辑上非常强大，任何可以通过有限步逻辑和数学运算完成的问题，从理论上讲都可以遵循一个设定的过程，在图灵机上完成。今天的各种计算机也不过是图灵机这种模型的一种具体实现方式。不仅如此，今天那些还没有实现的假想计算机，比如量子计算机，在逻辑上也并没有超出图灵机的范畴，因此，在计算机科学领域，人们就把能够用图灵机计算的问题称为可计算的问题。\n是否所有有答案问题都是可计算问题，这一问题依旧有争议，一方面，人们总可以构建出一些类似悖论的数学问题，显然无法用图灵机来解决；另一方面，在现实世界里是否有这样的问题存在，或者说这些构建出来的我呢提是否有意义，很多人觉得暂时没有必要去考虑。但不管怎么讲，依据丘奇和图灵这两位数学家对可计算问题的描述（也就是所谓的丘奇-图灵论题），有明确算法的任何问题都是可计算的，至于没有明确算法的问题，计算也无从谈起。另外，对于理论上可计算的问题，在工程上未必能实现，比如NP-Hard问题，可能需要算到宇宙的尽头，并且图灵机没有存储内容的限制，这在现实中也是不可能的。\n联系一下上述的子集关系，即所有问题\u0026mdash;\u0026gt;数学问题\u0026mdash;\u0026gt;可判定的问题\u0026mdash;\u0026gt;有答案的问题\u0026mdash;\u0026gt;可计算问题\u0026mdash;\u0026gt;工程可解问题\u0026mdash;\u0026gt;人工智能问题。这一种层层递进的包含关系，就可以清楚的看到人工智能的边界了。可以看到，理想状态的图灵机可以解决的问题，只是有答案问题的一部分，而在今天和未来，在工程上可以解决的问题都不会超出这个可计算的范畴。\n完结撒花✨✨✨\n","id":8,"section":"posts","summary":"本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，没有选择自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些","tags":["“数学之美”","“读书笔记”"],"title":"十分钟读完数学之美","uri":"https://biofrostyy.github.io/2021/06/10%E5%88%86%E9%92%9F%E8%AF%BB%E5%AE%8C%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/","year":"2021"},{"content":" 评前碎碎念\n 翻译的好坏总是能左右我对一本书的兴趣，仿佛我试图入坑克苏鲁神话被译本晦涩仿佛机翻的语言劝退的那天，一直以来我对译本总是抱着怀疑的态度，我觉得翻译过就缺失了原作者的最初想法和语言造诣。尤其是日文译本，包括早些年看过村上春树，我不懂日文，不知道是不是日文本身的原因会造成译本晦涩难懂，还好这本书除了开头的几章有些晦涩，后面也没什么阅读障碍，我得以顺利的读完。另外，本书更多聚焦在社会派的推理，直到中间部分才开始出现紧张而烧脑的案件推进情节，对于本格推理爱好者可能吸引不大，对死刑相关及当下社会的思考才是这本小说最注重且可贵的地方。\n 读后感\n 说回这本书的本身，关于刑法的正义性和是否废除死刑的喧嚣从来没有停止过，\u0026lt;大卫·戈尔的一生\u0026gt;，\u0026lt;一级恐惧\u0026gt;等，都让我印象深刻。我也渐渐对于各种故事（尤其是日式推理）里面所有被害人都是罪有应得的设定，开始审美疲劳起来。这本书带来了思考，我也庆幸作者的立场大体上与我的并没有相悖，不至于看得难受。从理性角度私刑永远不该被选择，从感情角度我只关注受害者的故事，无论犯罪者有多么凄惨的过去，只要他伤害的是无辜者，那么他永远得不到我感情上的同情。如果感情上总是站在加害者的立场上，为他找各种理由得到宽恕，遵纪守法的公民们又怎么能感到安全呢？毕竟我真的有一头牛。\n作为这本书的读后感，我将分别从两个主角-纯一和南乡-的角度观察他们的行为与立场。\n因为纯一是个杀人犯，所以他一直在把自己代入罪犯的立场，他会在翻案时犹豫，会认为他们查案去抓住真正的凶手只不过是把一个人解救出来，又把另一个人送上绞刑台吗？他这种想法这在我看来是无法理解的。但是他确实是我感情上的同情者，在这起案件中，佐村恭介是“罪有应得”的受害人，他对于杀害佐村恭介没有懊悔，他也对于佐村恭介的父亲想要杀死自己没有任何责备\u0026ndash;“就像我杀死了佐村恭介那样，他的父亲也可以杀死我”。他体会到了私刑会引起一次又一次的复仇，为了避免这种情况的发生，就必须有人来代替他们做这件事，他安慰了南乡，至少给不是被冤枉的死刑犯实施死刑确确是正确的事。\n再说到审判官南乡，最让我震撼也是感同身受的就是南乡的想法变化，最开始他认为自己在做正义的替受害者惩罚犯罪者的事，后来他看到了一封家属宽恕犯罪者的信，开始怀疑如果受害者家属都希望宽恕犯罪者，自己忍着人类对\u0026quot;杀人\u0026quot;的厌恶实施死刑是为了谁，但其实这里有两个逻辑问题：一是家属可以代替受害者宽恕吗，如果可以那会不会有人伙同罪犯杀掉家人然后再作为家属为罪犯脱罪？二是文中这个家属所说的宽恕是希望施暴者永生都在监狱中，但事实上，按书中所说，如果不是死刑，那么如果考虑表现优异的减刑政策，无期徒刑的平均服刑时间是十八年。南乡一直在挣扎，这些事情本就像是一万个哈姆雷特，我认为或者纯一认为南乡是正义的都没有用，最重要的是南乡可以走出自己心里的牢，毕竟就算是大慈大悲的佛教也为那些无法挽救的愚昧众生准备了破坏神\u0026ndash;不动明王。\n说到本书表达的社会性问题。首先，对死刑问题持有疑问，其中一个原因人们把死刑与杀人的不快感混同在一起导致的，正如南乡在亲自执行死刑之前，他一直是支持死刑制度的，而为了规避私刑，执行死刑必须有第三者，也就是国家机器行使刑罚。这是很多支持废除死刑的人的论据之一，他们认为这对行刑者是一种折磨。有些工作肯定是大家都不想去做的，例如随时会死的矿工工人，但是当他的社会责任感或是朴素的钱给够了，总是会有一些人愿意去做，请注意，是愿意去做，他们有选择权且他们选择了去做，我们尊敬他们，但为了他们废除一种刑罚则大可不必。再一点，有人认为，人生而平等，人命也是平等的，那么法律剥夺罪犯的生命，不和罪犯剥夺被害者的生命一样吗。这种想法我实在是无法苟同。首先罪犯剥夺被害者的生命是一个自然人对另一个自然人生命权力的剥夺，是主动的，而死刑则是报复式刑法的一种，是第三方对一个自然人生命的剥夺。另外，作为社会中拥有权利的公民，首先他是要遵守社会的秩序的，那么在罪犯剥夺受害人生命的时候，就已经放弃了自身作为合法公民的权利，那么法律刑罚应用在他的身上一点都不令人奇怪。\n第二点令我震撼的是，整个故事中反映出的对受害者的恶意，性侵是亲告罪，纯一与友里不敢声张，害怕受到检方和社会更大的而侮辱；南乡是属于自卫杀人，但是在他杀人的一瞬间，他的south wind面包店和家人就已经成为遥不可及的梦。身体上的伤害尚且可愈，心灵上的伤害对于受害者可能是永远的无期徒刑了。\n最后，对于十三级台阶，就像挑水的三个和尚。虽然着墨不多，但是也几次描写了死刑审判员们，因为怕麻烦或担心支持率，虽然案件存疑，也推动了树原亮的死刑进程。作为替受害者”审判\u0026quot;的第三者，也需要有良好的制度与责任感，才能承担法务国家机器的责任。\nPS.十三级台阶，指对于一个死刑犯的判决，共经过13个人的审核。第一步刑事局3人审核；第二步矫正局3人审核；第三步保护局3人审核；第四步法务大臣事务局秘书科科长审核；第五步法务大臣事务局局长审核；第六步法务大臣的次官审核；最后第七步法务大臣审核。一共13人。\n","id":9,"section":"posts","summary":"评前碎碎念 翻译的好坏总是能左右我对一本书的兴趣，仿佛我试图入坑克苏鲁神话被译本晦涩仿佛机翻的语言劝退的那天，一直以来我对译本总是抱着怀疑的态","tags":["“消失的第13级台阶”","“推理悬疑”","读后感"],"title":"消失的第13级台阶","uri":"https://biofrostyy.github.io/2021/05/%E8%AF%BB%E5%90%8E%E6%84%9F-%E6%B6%88%E5%A4%B1%E7%9A%84%E7%AC%AC13%E7%BA%A7%E5%8F%B0%E9%98%B6/","year":"2021"},{"content":"一.问题介绍 工作中遇到一个问题，一个列表中，存在多个以\u0026quot;-\u0026ldquo;连接的范围string，希望可以把规则集压缩，即融合的范围融合\nI：[10-30kg,3-10kg,3-10kg,1-3kg,3-10kg,3-10kg,50-100kg]\nO：[1-30kg,50-100kg]\n注：范围string无序且有重复\n二.python实现 ①去重并排序 def takeFirst(elem): return int(elem.split('-')[0]) input = list(set(input.split(\u0026quot;,\u0026quot;))) # 去重 input.sort(key=takeFirst) # 排序 '''此时input为[3-10kg,10-30kg,50-100kg]'''  ②规则融合 def merge(elem1,elem2): elem1 = elem1.split('-') # 将元素基于-分割，次数为没有后缀的情况，如果像我上面的实例有kg作为后缀，可以先对string将进行切片，如elem1[0:-2] elem2 = elem2.split('-') # 如果前一个元素的最大值等于后一个元素的最小值，则进行融合，返回融合值，否则返回空 if elem1[1] == elem2[0]: return ''.join([elem1[0],'-',elem2[1]]) def mergelist(list): res = [] # 如果此input列表只有一个元素，则返回原列表 if len(list) == 1: return list # 从列表后端遍历所有相邻元素对 a = len(list)-2 while a \u0026gt;= 0: new_elem = merge(list[a],list[a+1]) # 如果可以融合，删除两元素，并在原位置插入新元素 if merge(list[a],list[a+1]) is not None: list[a] = new_elem del list[a+1] a -= 1 return list # 如果存在'\u0026gt;100kg'这样的情况，可以先转换成'100-infikg'进行处理，结束后再统一反转 mergedinput = mergelist(input) # 融合  三.你可能还需要\u0026hellip; ①融合前整合数据 我们拿到手的数据，都是松散的，所以需要我们先整合再进行上述融合操作，pandas库的groupby函数可以帮我们做到这一点\nI：\n   name classify weight     小王 手表 0-1kg   小王 日用品 3-10kg   小李 手机 1-3kg   小王 日用品 1-3kg   小李 手机 \u0026lt;1kg   小李 手机 10-30kg   小李 手机 1-3kg    O：\n   name classify weight     小王 手表 0-1kg   小王 日用品 3-10kg,1-3kg   小李 手机 1-3kg,\u0026lt;1kg,10-30kg,1-3kg    outputDataframe = inputDataframe.groupby(['name','classify'])['weight'].apply(lambda x:x.str.cat(sep=',')).reset_index()  ②融合后，无法融合的部分松散数据 对于无法融合成一条的多条规则，因为dataframe后续处理问题，我们也不能将这些规则继续挤在一行用逗号隔开，我们需要将他们拆成多行，保证每一行只有一个规则\nI：\n   name classify weight     小王 手表 0-1kg   小王 日用品 1-10kg   小李 手机 0-3kg,10-30kg    O：\n   name classify weight     小王 手表 0-1kg   小王 日用品 1-10kg   小李 手机 0-3kg   小李 手机 10-30kg    ''' 本质上，使用numpy来存储累加数据，再重新转为dataframe ''' newvalues=np.dstack((np.repeat(inputDataframe.name.values,list(map(len,inputDataframe.weight.values))),np.repeat(inputDataframe.classify.values,list(map(len,inputDataframe.weight.values))),np.concatenate(inputDataframe.weight.values))) outputDataframe = pd.DataFrame(data=newvalues[0],columns=inputDataframe.columns)  ","id":10,"section":"posts","summary":"一.问题介绍 工作中遇到一个问题，一个列表中，存在多个以\u0026quot;-\u0026ldquo;连接的范围string，希望可以把规则集压缩，即融合的范围","tags":["“python”","代码沉淀"],"title":"python 实现规则集分类并融合压缩","uri":"https://biofrostyy.github.io/2021/05/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80/","year":"2021"},{"content":"并行计算编程技术 CPU并行计算，从体系层次上来讲，①CPU单核通过向量化技术提升单核的处理能力，②多核CPU通过多线程技术来充分利用多核处理性能，③GPU异构计算来扩充单机的处理能力，④多机并行把多机串联起来组成计算集群\n①CPU单核通过向量化技术提升单核的处理能力 在X86体系架构的CPU上，主要的向量化编程技术是SSE和AVX。主要用于对不同数据执行同一（批）命令。Intel公司的单指令多数据流式扩展(Streaming SIMD Extension, SSE)支持的处理器有16个128位寄存器，每一个寄存器可以存放4个（32位）单精度的浮点数。即理论上可以提升4倍计算加速，不过在实际运行中，由于加载数据到寄存器有时间消耗，加速比略低于这个理论值。\n②多核CPU通过多线程技术来充分利用多核处理性能 为了充分利用多核CPU的计算能力，各个操作系统和编程语言都提供了多线程编程库，例如UNIX/Linux中Pthread、Windows环境下的WinThread。但是相对于机器学习并行来说，一方面多线程编程技术的开发成本较高，而且此技术需要妥善处理同步互斥等问题；另一方面，不同平台使用的多线程编程库是不一样的，这样也会造成移植性问题。OpenMP是一个支持共享存储并行设计的库，特别适宜多核CPU上的并行程序设计，它可以有效解决上面两个问题， 具有如下几个特点： 1，OpenMP 是基于共享存储体系结构的一个并行编程标准。目前主流编译器GCC和Visual Studio都支持它。 2，OpenMP通过在源代码(串行程序)中添加 OpenMP指令和调用OpenMP库函数来实现在共享内存系统上的并行执行，使得很方便对传统的程序进行并行化改造。 3，OpenMP为共享内存并行程序员提供了一种简单灵活的开发并行应用的接口模型 ，使程序既可以在台式机上IA行，也可以在超级计算机上执行，具有良好的可移植性。\n例如下方对for循环语句进行并行化改造： ③GPU异构计算来扩充单机的处理能力 CPU vs. GPU 1.CPU主要是为串行指令而优化，而GPU是为大规模并行运算而优化 2.GPU相对CPU来说，在同样的芯片面积上， GPU拥有更多的计算单元，这也使得GPU计算性能更加强大，而CPU则拥有更多的缓存和相关的控制部件 3.GPU相对CPU来说拥有更高的带宽 接下来，我们来看看CUDA相关的几个概念:线程 线程块、线程网格 、线程束、流处理器、流多处理器 口流处理器：它是最基本的处理单元，最后具体的指令和任务都是在其上处理的。GPu进行并行计算 ，也就是很多个流处理器同时做计算 口流多处理器：多个流处理器加上其他的一些资源组成一个流多处理器。资源也就是存储资源、共享内存、寄储器等。流多处理器就相当于 CPU 中的核，负责线程束的执行。同一时刻只能有一个线程束执行。 口线程束：它是GPU执行程序时的调度单位，目前CUDA的Warp 的大小为32 ，同在 Warp的线程以不同数据资源执行相同的指令。 口网格、线程块、线程：在利用CUDA进行编程时，一个网格分为多个线程块，而一个线程块分为多个线程。其中任务划分到是否影响最后的执行效果。划分的依据是任务特性和GPU本身的硬件特性。\nCUDA通过函数名前缀来指明函数类型 口_device_函数在设备端执行，并且也只能从设备端调用， 即作为设备端的子函数来使用 口_global_函数即kernel 函数，它在设备上执行，但是要从Host端调用 口_host_函数在Host端执行，也只能从Host端调用，与一般的C函数相同\n更多关于CUDA编程的知识，请参考其官方网站\n④多机并行把多机串联起来组成计算集群 多机并行技术发展起来，消息传递接口(Massage Passing Interface, MPI)就是其中一个典型代表。MPI是消息传递函数库的标准规范，由MPI论坛开发，支持Fortran和C/C++，它具有如下特点： 1，MPI是一种新的库描述，不是一种语言 2，MPI共有上百个函数调用接口， Fortran和C++ 可以直接对这些函数进行调用 3，MPI是一种标准或规范，而不是特指某一个对它的具体实现 4，MPI 种消息传递编程模型，并成为这种编程模型的代表和事实上的标准\n目前比较主流的MPI实现有MPICH2和OpenMPI。MPI从本质上解决了多机并行中的数据通信问题，从而使得多机并行开发变得容易。MPI程序执行流程如下图\nMPI是一种多进程编程技术。根据经验，MPI的学习曲线和开发成本都是比较低的，下面是6个最基本也是最常用的MPI函数 MPI_Init()初始化环境 MPI_Comm_size() 获取进程数 MPI_Comm_rank() 获取进程序号 MPI_Send() 送消息 MPI_Recv() 接收消息 MPI_Finalize() 并行结束函数\nMPI主要解决了进程之间的通信问题，包括跨主机的进程通信 数据移动 数据归约 并行编程技术小结 上述四种技术在机器学习领域得到广泛应用 1，向量化化和OpenMP，在单机执行机器学习训练的情况下，主要采用该技术进行加速，使用该技术常用的开源机器学习包有XGBoost、Multi-core Liblinear、Libffm和ANN 2，GPU编程（CUDA) ：几乎所有的深度学习包都会使用该技术， 比如Theano、MXNet、TensorFlow和Caffe 3，MPI：有些大公司也开发了自己的消息通信系统，其功能上与MPI大同小异， Graphlab、Distributed MPI LIBLINEAR、Paracel都是采用MPI的代表\n我们可以根据自己的硬件条件和应用场景来选择合适的并行计算编程技术 并行计算模型 随着大数据技术的发展，分布式系统越来越多。我们熟悉的MapReduce系统Hadoop、基于内存的MapReduce的Spark系统是Job中间输出结果、可以保存在内存中的实时流处理系统Storm，以及后起之秀面向分布式数据流处理和批量数据处理的开源计算平台Flink。而对于机器学习的分布式系统，基于Hadoop的Mahout因为效果不理想已经停止更新。Spark的MLlib也提供了大量的机器学习算法的分布式实现。\n但是，为什么一些公司会研发自己的分布式机器学习平台？常见的分布式深度学习包如TensorFlow、MXNet、PaddlePaddle等并没有直接基于Spark等平台实现？\n答案其实就是术业有专攻，第一段提到的Hadoop和spark等系统是通用的分布式任务处理解决方案，当然也可以用来处理分布式机器学习任务，而互联网巨头研发的分布式机器学习平台是专门为机器学习任务处理而设计的。TensorFlow等甚至是专门为分布式深度学习设计的。\n传统分布式任务于分布式机器学习任务的区别 1，传统 MapReduce 模型计算过程中一旦发生错误，错误是会一直传播而不会得到任何修正的，而对于机器学习程序来说，中间结果的错误是可以容忍的，有多条路径都可以收敛到最优 2，机器学习算法比传统的MapReduce程序拥有更加复杂的结构依赖，就是说机器学习模型中的参数通常不是不独立的 3，机器学习算法还有一个特性是参数收敛速度的不均匀，MapReduce排序把数据分发到不同节点执行，各节点的任务和负载基本上是均衡的\n根据以上的区别，分布式机器学习系统需要解决以下三个问题： 1，如何更好地切分多个任务 2，如何调度子任务 3，均衡各节点的负载\n机器学习中重要的两个问题：建模和求解。分布式机器学习主要需要解决求解的并行化问题，具体来说其实主要是梯度下降求解的并行化问题，并针对最优化求解并行化提出了很多并行模型。 BSP 一个较早的并行计算模型，也是当前主流的并行计算模型之一 BSP模型简单说就是切分好的并行任务同时计算(线程或进程)， 算完之后统一进行通信，对各自的计算结果进行同步，然后再开始新一轮的计算和同步。BSP模型中，计算由一系列用全局同步分开的周期为L的计算组成，这些计算称为超级步( SuperStep ) 这个性能公式对于算法和程序分析是很简单方便的。\nBSP具有如下优点： 1，它将处理器和路由器分开，强调了计算任务和通信任务的分开，而路由器仅仅完成点到点的消息传递，不提供组合、复制和广播等功能，这样做既掩盖具体的互连网络拓扑，又简化了通信协议 2，采用障碍同步的方式以硬件实现的全局同步是在可控的粗粒度级，从而提供了执行紧耦合同步式并行算法的有效方式，而程序员并无过分的负担 BSP模型的这些特点使它成为并行计算主流模型之一，开源的Mahout、ApacheHuma、Spark MLlib、Google Pregel、Graphlab和XGBoost等的并行实现都是基于BSP模型的。\nSSP BSP在每一轮结论都需要一次同步，这就会造成木桶效应，导致每一轮迭代的效率由最慢的计算任务来决定，为了缓解这个现象，SSP模型被提出来了 当最快的worker比最慢的worker超过一个预设的bound时，所有的worker进行一次参数同步。这个Bound可以根据迭代的次数，也可以根据参数更新的差值来确定。SSP协议的好处在于，Faster worker会遇到参数版本过于陈旧的问题，导致每一步迭代都需要网络通信，从而达到了平衡计算和网络通信时间开销的效果。\n可能有人由疑问，这样不会导致参数不同步的问题吗？答案是肯定的，但是在传统分布式任务于分布式机器学习任务的区别第一点中，我们提到机器学习可以容忍错误，对于机器学习程序来说，中间结果的错误是可以容忍的，有多条路径都可以收敛到最优，因此少量的错误可类似于随机噪声，但不影响最终的收敛结果。尽管每一次迭代可能存在误差，但是经过多轮迭代后，平均误差趋近于零。尽管每次求解路径可能不是最优的，但是最终还是找到了一条通往最优解的整体路径。尽管这条路径不是最快的路径，但是由于其在通信方面的优势，整体的求解速度相对于BSP来说还是更快一些，特别是在数据规模和参数规模非常大的情况下、多机并行的环境下。\nASP 如果我们把SSP的同步时机约束无限放宽，SSP模型就变成了ASP模型。也就是说在ASP模型下，Worker之间没有同步操作了。 这种看似非常大胆的举动，？却会在特定的情况下发挥很重要的作用。这里我们对比下BSP、SSP、ASP三种模型 ①SSP协议可以有效平衡计算和网络通信的开销。解决Faster Worker参数版本过于陈旧的问题。 ②？SSP有收敛性保障，异步是没有的。异步的问题在于，整体对参数的更新量de!ta w = delta w 1+ delta w2+… (de!ta wi表示单个Worker 根据部分数据计算的参数更新量)， delta wi之间 应该是不能跨迭代次数的(而SSP则放宽了这种约束)，因此异步并没有收敛的保证。而SSP是有收敛保证的，有的论文提供了 Bound③对于非凸问题， BSP和SSP收敛的最优解可能不一样。非凸优化问题(比如说神经网络)有大量局部最优解，随机梯度下降(可以跳出局部最优解)比批量梯度下降效果要更好。\n基于上述紫色文字部分疑问的补充知识 在Angel中这三种设置方式非常简单，如图中只要设置不同的staleness，就能以不同的异步模型运行。但是，同步限制放宽之后可能导致收敛质量下降甚至任务不收敛的情况，这需要在实际算法中，需要指标的变化情况，调整同步协议以及相关的参数，以达到收敛性和计算速度的平衡。在Angel中，我们是通过向量时钟的方式，来实现异步模型控制。 来源：https://edgue.github.io/angel/docs/design/sync_controller.html#2-ssp\n参数服务器 参数服务器是近年来在分布式机器学习领域非常火的一种技术。参数服务器是个编程框架，用于方便分布式并行程序的编写，其重点是对大规模参数的分布式存储和协同的支持。在参数服务器的机器学习训练集中，集群中的节点可以分为计算节点和参数服务节点两种。\n其中，计算节点负责对分配到自己本地的训练数据进行计算学习，并更新对应的参数；参数服务节点采用分布式存储的方式，各自存储全局参数的一部分，并作为服务方接受计算节点的参数查询和更新请求。\n①参数的获取与提交 计算节点从参数服务节点上获取当前的梯度，然后根据本地分配的训练样本进行梯度计算，通过几轮迭代后将更新后的梯度推送给参数服务节点。参数在参数服务器中需要高效地进行分布式存储，同时计算节点和参数服务节点之间的通信要足够高效。 ②参数值的同步问题 根据参数服务的设计和运行原理，我们可以得知在每一个时刻计算节点的梯度和当前参数服务节点上存储的梯度可能是不一致的。 一般我们设定，每个计算节点Worker迭代这个轮数后和参数服务节点做一次同步。\n③非凸问题能否收敛到最优解 本身梯度下降严格来说是无法得到非凸问题的全局最优解的。参数服务器的异步随机梯度方法正好可以跳出局部最优解，最终的训练效果还不错。\n梯度更新方法 梯度更新方法主要有参数平均法和基于更新方法两种\n①参数平均法 将每个计算节点获取的参数值平均后作为全局参数值，可以证明参数平均法的结果在数学意义上等同于用单个机器进行训练 ②基于异步梯度下的更新方法，相对于参数平均法在工作节点于参数服务器之间传递参数，我们在这里只传递梯度更新信息 梯度更新时机 Bounded Delay：它是Sequential与Eventual之间的折中，当最快计算任务比最慢计算任务快于一定阀值时，最快的计算任务进行等待，也可以当计算任务对梯度的累计更新值大于一定阔值时，最快计算任务进行等待。可以设置 τ 为最大的延时时间，也就是说，只有大于 τ 阈值之前的任务都被完成了，才能开始一个新的任务。极端的情况有 τ= 0，Bounded Delay 等价于 Sequential； τ =∞， Bounded Delay 等价于Eventual BSP、SSP、ASP与参数服务器 参数服务器的这些优点使之成为最近最流行的机器学习分布式机器解决方案\n并行计算案例 现在有很多开源的分布式机器学习包，这里我们对DMLC的XGBoost和MXNet两个开源的机器学习包实现原理进行剖析。XGBoost是目前GBDT模型最好的实现，在工业界和Kaggle比赛中都发挥了非常重要作用，取得不错的效果。MXNet 也是目前最优秀的分布式深度学习框架之一，并被亚马逊云计算选为官方深度学习包。\nXGBoost并行库Rabit XGBoost的分布式实现有如下几个特点 ①OpenMP支持多核并行 ②CUDA支持GPU加速 ③Rabit支持分布式\nXGBoost的核心就是Rabit，XGBoost将其分布式核心功能抽象出来，Rabit是基于BSP模型的，通过两个基本原语Broadcast和AIIReduce来实现其分布式功能。Broadcase和AIIReduce与MPI的功能基本上一致，设计思想类似。那么，为什么不直接使用MPI? 原因就是Rabit在MPI的AIIReduce和Broadcast操作原语这个基础上提供了更好的容错处理功能，弥补了MPI的不足。\n为什么传统的MapReduce模型在机器学习并行化中的作用有限呢？首先，MapReduce每一轮迭代计算后其中间结果都放入到存储系统，有的甚至是在硬盘(比如Hadoop) 中，这就降低了迭代效率。其次，在每-轮迭代后数据都需要重新洗牌分发，这无疑又增加了系统的通信开销。 XGBoost Rabit针对这个缺陷，在两个地方都做了优化，其一，每一轮迭代结束后计算结果不需要放入到存储系统，而是直接保留在内存；其二，每一轮迭代后没有数据重新分发的过程，直接进行下一轮迭代，这使得计算效率大大提升。 Rabit通过检查点机制来进行容错恢复，Rabit在一轮AllReduce通信结束后会在各节点内存中将模型结果缓存为检查点，并增加检查点版本号。同步结果后，各节点会继续各自的计算直到下一次AllReduce通信同步。在这个过程中，如果其中有节点发生故障计算失败，该故障节点会从集群中找到最近的节点，拿到上一轮的模型文件，然后重新开始计算。其他无故障节点等待故障节点计算完成后，各节点间才再一次进行AllReduce通信同步。\nXGBoost Rabit对分布式操作的封装非常好，可以很方便移植到其他系统中去，我们可以基于Rabit来开发我们的分布式机器学习程序： MXNet并行库PS-Lite PS-Lite是MXNet分布式现实的核心，它是基于参数服务器模型的。它的实现具有如下特点。 ①高效的通信 异步通信不会拖慢计算 ②弹性一致 将模型一致这个条件放宽松，允许在算法收敛速度和系统性能之间做平衡。通过之前提到的Bound来解决计算节点与参数服务器的参数同步时机 ③扩展性强 增加节点无须重启集群。在数据一致性上，使用的是传统的一致性散列算法。这是分布式计算和存储中一种非常有名的算法\u0026mdash;当数据太大而无法存储在一个节点或机器上时，将对象和服务器都放置到同一个哈希环后，在哈希环上顺时针查找距离这个对象的 hash 值最近的机器，即是这个对象所属的机器。 当添加新服务器时，如果使用简单的取模方法，当新添加服务器时可能会导致大部分缓存失效，而使用一致性哈希算法后，这种情况得到了较大的改善，因为只有少部分对象需要重新分配。 ④错误容忍 机器错误恢复时间短，向量时钟( Vector Clock) 容许网络错误。使用向量时钟来记录每个节点中参数的时间戳，分布式系统能够用来跟踪状态或避免数据的重复发送，推或拉的时候，需要更新的参数都是Rang-Based。这带来了一个好处是，这个Range里面的参数共享的是同一个时间戳，这显然可以大大降低空间复杂度 ⑤易用性 全局共享的参数使用向量和矩阵表示，而这些又可以用高性能多线程库进行优化\n传统的基于随机梯度下降及其优化变种Momentum、Nesterov Momentum、AdaGrad、RMSProp、 Adam的机器学习算法能很容易移植至PS-Lite框架下。 PS-Lite的使用很简单，开发人员可以很方便对现有的机器学习程序进行分布式改造。PS-Lite的核心是KVStore。PS-Lite提供一个分布式的键值对存储来进行数据交换，它主要有两个函数。 ①push 它将键值对从一个设备push进存储，用于计算节点将更新后的参数值推送到参数服务器上 ②pull 它将某个键上的值从存储中pull 出来，用于计算节点从参数服务器上获取相关的参数值\nKVStore还接受自定义的更新函数来控制收到的值如何写入到存储中。最后，KVStore提供数种包含最终一致性模型和顺序一致性模型在内的数据一致性模型 美团并行计算机器学习平台 - Ginger Ginger是美团结合自己的业务场景和应用特点自主研发的大规模并行深度学习平台。 Ginger属于第一代深度学习框架( 比如Caffe-vl和Paddle等) 封装深度学习算法的前向、后向等操作为Layer ，基于Layer 组装成网络，相比第二代深度学习框架( 对运算的封装)，它的抽象粒度更大，它对开发者要求更高，需要开发者实现Layer梯度计算，但运行效率可以做得更高。因为Ginger框架目标是\u0026quot;运行更快，使用更灵活\u0026quot;，框架会充分考虑效率，因此借鉴第一代深度学习框架思想设计 Ginger架构的组成包括存储部分（Stores、Tensor） 、模型组建部分( Layer、Net、Model)、控 制部分（Engine、Updator ）、并行库( Parallel Lib )和在线预测（Online Pred）。 ①存储部分 它包括深度学习算法在运行时输入数据、 训练参数、 中间状态的存储和操作。使用的是Tensor (看成多维数组)， 比如 Mini-Batch x Hl x H2 , TensorOp就是对Tensor的运算，加减乘除使用的是MKL高性能库。Stores依赖的Tensor和lTensorOp与具体Layer有关，比如全连接层的参数及梯度等。\n②模型组建部分 它是整个框架最主要部分，模型是Ginger每个进程生成的整体，可以包括网络、引擎或者一些特殊功能，而网络 (Net) 由层 (Layer) 组合而成，其中层包含数据读取层、功能层、Loss层和组件层。 \u0026ndash;数据读取层：它要求数据是二进制格式，对海量数据实现多线程异步读取，支持LIBSVM格式和NLP的词ID格式 \u0026ndash;功能层：它支持多入( Concat、Eltwise) 或者多出( Split) 的功能层以组合复杂的网络 \u0026ndash;Loss层：它支持多分类的Softmax 、二分类的LogLoss、Pairwise的HingeLoss等 \u0026ndash;组件层：它包含各种非线性变化 (Sigmoid、SoftSign、Relu、Elu、Polynominal等)、全连接 (Fully Connected , FC) 层、LookUp层、卷积层等\n③控制部分 它包含Engine和Updator。其中Engine会控制数据在网络中的流动、参数的变化，比如FeedFoward的操作。而Updator是Engine所管的一个实施者，负责参数更新、隐层状态的清空、梯度的清空或累计，对应包括各种更新算法， 比如SGD及SGD的变形\n④并行库 它基于ZMQ实现的多服务器的参数服务器主要面向大规模机器学习，设计的通信机制能做到通信耗时与节点数目无关，因此扩展性很好。ZMQ也是著名的分布式流式计算系统Storm的消息通信组件，在这里我们用ZMQ取代了MPI 的通信功能\n⑤在线预测 线上预测可以生成库嵌入到C++线上代码中，或者生成服务可通过网络访问。在线预测部分的数据输入是在线的数据流，因此和计算线下的Mini-Batch不一样，并且考虑到线上性能和内存要求，Ginger对有些计算和存在特殊优化过\nGinger采用业界主流的参数服务器架构，模型参数分布在服务器节点上，Worker为计算节点，训练样本分布在每个Worker节点上。Worker只和服务器通信， Worker/Worker和Server/Server之间没有通信，Worker与所有服务器都保持通信。训练时，Worker 通过消息系统从所有服务器中获取完整的模型参数信息，然后根据本地训练样本进行梯度计算。 服务器和worker启动，根据配置文件中远程节点的IP和端口建立收发数据的连接。 开始训练，Worker根据指定的通信间隔与服务器通信，服务器根据指定的策略（Async、BSP、SSP）更新模型。 Worker训练结束后给服务器发送结束信息并退出，服务器收到所有Worker的结束信息之后退出。 随机梯度下降作为深度学习的核心求优算法，Ginger对其进行了很好的支持。支持随机梯度下降的优化算法有Momentum、Adagrad、RMSprop和Adam。\nGinger有非常不错的性能，这里我们对主流的机器学习框架XGBoost、Tensorflow以及Keras作了对比测试的实验。 ","id":11,"section":"posts","summary":"并行计算编程技术 CPU并行计算，从体系层次上来讲，①CPU单核通过向量化技术提升单核的处理能力，②多核CPU通过多线程技术来充分利用多核处理","tags":["“美团机器学习实践”","“读书笔记”"],"title":"美团机器学习实践 - 读书笔记 - Chapter 16 大规模机器学习","uri":"https://biofrostyy.github.io/2021/04/%E7%BE%8E%E5%9B%A216/","year":"2021"},{"content":"UGC：User Generated Content 评论的真实性和针对性比较强，不会像游记文本出现多个实体。而且其对单一实体的描述更深刻、更直观，且获取成本更低。\n评论挖掘的背景 对用户主观做出的评价性内容进行核心思想的挖掘，其包括多维度的主观体验及不同情感倾向的感受反馈。我们要对评论进行挖掘，提取其中的标签及标签情感，可让用户以最快速最直观的方式看到最优质的评论内容。\n本书通过以下三个维度，来评论挖掘的背景进行深入解析。 ①评论挖掘的粒度\u0026ndash;POI粒度及其下的商品粒度 与电商领域不同，在在线业务场景中，以美团酒旅业务为例，某条评价的主体描述虽然可能针对某一类 房型或某一处景点，但是，其核心评价集中于对商家整体服务的评价。 对于在线商品来说，评论的应用方，不仅仅限于用户，而且对业务方的销售人员和运营人员也有着指导作用，例如对房间卫生的负面评价对酒店方改善店内设施有指导作用。也就是说基于单个商品的评论标签，更适合公司内部使用，而从线上展示层面来说，POI粒度的评论挖掘才是重中之重 ②评论挖掘的维度 与用户画像中的标签是对C端用户进行属性挖掘和行为构建不同，评论挖掘中的标签则是对B端商家进行硬件层面的挖掘和服务层面的判定。 ③整体的角度重新审视评论挖掘 从实践中发现，海外酒店评论的平均长度高于国内酒店，也就意味着， 同样的算法需求，在应用于不同业务线时，需要采用不同的模型策略以及差异化的处理流程。从自然语言处理的深层语义角度来看，评论是一种多情感并存的内容类文本。它的明显特征在于，上下文的情感差异可能存在极端情况，如前一句话正向描述服务态度，而后一句话负向描述卫生条件 ，评论挖掘的整体流程的设计要考虑到不同评论的不同预处理方式，具体就是，什么情况下需要断句、什么情况下可以直接端到端\n标签需求提取 标签提取意味着从评论中提取核心关键词。 ①有监督学习 ②无监督学习 ③在风控领域被广泛使用的基于规则（Rule-Based）的专家系统，也不失为一类能够通过整合其他模型、明显提高召回率的规则类方法\n数据的获取及预处理 在做标签提取之前，需要对评论进行表示层的预处理，也就是将评论从纯自然的中文形式，转换为模型适用的词、字符或向量形式 无监督的标签提取方法 关键词抽取的常规算法主要包括各类主题模型，如广泛用于文章主题提取的LDA、基于TF-IDF的各种变种算法，以及部分学者提出的基于统计机器翻译的新兴方法论。但评论具有随意性强、更新快和无规律的特点，因此，这些方法无法通过预先训练大量语料来对后续评论持续保持高效的抽取结果。于是，我们将TextRank作为主要的标签提取算法。\n⭐️TextRank 思想来自于PageRank，将文本内容拆分成若干单元，每个单元由句子或单词组成，据此建立一个有向有权图G = (V,E)，其中V(vertices)代表组成单元的集合，E(edges)代表组成单元间关系的集合，对每个单元计算重要度得分的公式： 其中d是是阻尼系数，取值为[0，1 ]，代表从计算单元中的某个特定单元指向其他单元的概率值，经验取值一般为0.8，ln(Vi)代表图中某单元的入度 ，即指向该单元的单元集合，Out(Vi)代表图中某单元的出度，即从该单元指向其他单元的单元集合。在具体开发实现中， 一般会对所有单元初始化一个初值，由此持续迭代，最终收敛到图中任一单元的误差值均小于给定阈值，一般阈值设定为0.0001。\n下面将以旅游侧通过对差评评论的关键词提取进而对差评原因进行分析的案例，来说明该算法在业务场景中的具体应用。其中，评论总数为 17万多条，选定几种词性分别为名词、形容词、动名词等，概率初值定为 0.8 5，收敛阔值定为 0.0001，最终选取重要度排名前 1500 的词作为基准生成词云。 从图中可以明显看出，旅游评论差评中服务、设施、排队、态度、门票等是差评的主要诱因，还有收费、工作人员、环境等次级诱因，整体差评诱因一目了然。但是该算法存在一个致命缺陷，就是其无法提取文本中未出现的词。若想提取用户是否会推荐该POI，如果用户没有显示提到’推荐‘，则’推荐‘关键词的提取无法进行。因此，下一节将对有监督的神经网络算法进行原理和效果层面的表述，从而满足业务方的多元化需求。\n基于深度学习的标签提取方法 ①输入：评论是经过分词和去停用词处理过后的评论文本，每个文件只包含一个评论文本，标签以'\\n\u0026rsquo;分隔，每行只包含一个标签，且标签数量不固定，按照真实标签数量存放（即N个标签就存N行数据） ②目标变量数据标注：数据标注的成本较高，因为需要人工阅读和理解。除了电梯、电脑等名词性标签可以通过关键词过滤来达到筛选的目的。较为复杂的推荐、位置、满意等主观性标签都需要进行人工标注。本美图案例所用标注标签样本量为21079个，其中总标签数为23，标注样本中平均标签数为4.3。 ③模型：看作CNN多分类问题，输出为N个标签的概率，即N维向量，取TopK作为最终输出的评论标签。 ④模型完善： 多模型融合，将基于不同长度的N-Gram的模型分别训练，以针对不同长度的评论，并且最终的输出通过人工规则的方式进行纠正，强召回被CNN标签模型忽略的标签，同时丢弃CNN标签模型判错的标签，由此将最总TOP5准确率控制在93.4%左右。 另外本节中对TopK中K的选择，采用了K-means，将N维标签概率值向量，进行K分类（实验证明K一般选取2或3比较好），由于实现动态TopK的标签提取。（我理解为：例如上述23维结果向量，做二分类，取整体结果较高的一类个数作为TopK的K值。） 最后还应用了一个基于问题理解方面的小技巧：对于评论长度过短，但提取出的标签过多的情况，只取前2个标签作为最终标签输出，该技巧在最终的应用上，能够在部分情况下将标签提取的Top5 准确率提升到95% 右，已经相当可观。\n标签情感分析 传统意义上的情感分析是指，根据文本的含义和其中的情感信息将文本划分成某个情感的积极或消极类别。情感分析是一个领域相关或者说是文本类别相关的问题，本节所述评论标签情感分析的文本类别为长短相结合的文本形式，其情感信息表达的多义性更加难以判 ，且由于评论文本过于口语化，它的信息表征的隐蔽性更加突出，甚至很多情感维度的极性显得模棱两可。接下来，本节将会通过剖析评论标签情感分析的领域特殊性，来进一步讨论探度学习算法在该方向的解决方案，并分析后续应该进行哪些优化和改进，从而进行一个概述性的整合。\n评论标签情感分析的特殊性 ①评论的情感非常主观，同样的客观事实会导致截然不同的情感。这些情况常常伴随着很多实时性事件出现，实时性事件与客观环境相关性不强，但与主体所处时间点的事件性内容强相关，如孩子苦恼等。我们可以通过一些方法过滤掉实时性事件，比如实时性事件大多出现在长文本中，若连续两句及两句以上没有出现任何评价词语，即认定此评论90%左右（实验发现某些POI下比例更高）的内容是实时性事件，进而将该评论从训练语料中删除。\n②长短不一的内容如何提取关键情感信息\n③保证在不同标签间可能存在重叠性情感信息的时候，能够把情感信息的极性表达精确分配给不同标签。从算法层面来说，这类多标签情感信息重叠的情况可以归结为情感分析中的多标签正负样本杂糅的问题。幸运的是，大多数标签的情感信息处于互斥状态，因为基本不会存在一句话同时描述电视或电梯或泳池或早餐，那么这句话的情感就可以精确分配给这些（电视或电梯）标签。但服务态度和满意程度标签的整体性较高，很多局部情感信息可能会重叠甚至相悖，这样就会使得正负样本中同时存在某句话，但是这句话对于不同标签的情感倾向不同，这类标签的情感分析就需要严格控制多标签间正负样本中的重复样本占比，进而增强不同标签间情感分析模型的容错能力④正负样本不均衡，一般情况下，情感类二分类问题正负样本会在多标签下基本保持在5：5。但对于其他标签，如有无热水壶（1：9），这会让模型结果向大比例样本方向偏移。对于一些多分类问题，如有游泳池、无游泳池、有无边泳池（10：9：1），首先样本比例中\u0026quot;有无边游泳池\u0026quot;的比例极低，其次因为其中两个类别存在包含关系，遂可以先进行高准确率的二分类，再对其中的\u0026quot;有游泳池\u0026quot;类别进行二次分类，从而挖掘提取出最后一个三分类类别。\n","id":12,"section":"posts","summary":"UGC：User Generated Content 评论的真实性和针对性比较强，不会像游记文本出现多个实体。而且其对单一实体的描述更深刻、更直观，且获取成本更低。 评论挖掘的","tags":["“美团机器学习实践”","“读书笔记”"],"title":"美团机器学习实践 - 读书笔记 - Chapter 7 评论挖掘","uri":"https://biofrostyy.github.io/2021/04/%E7%BE%8E%E5%9B%A27/","year":"2021"},{"content":"POI points of interest 基本的业务单元，在酒店领域就是酒店。在上一章用户画像领域，就是一个自然人。一个优质的POI库：POI信息完整性、POI重复情况、POI信息的准确性等。\n问题的背景与难点 在酒店领域，美团自有POI库中的实体与待选POI库中的实体建立一对一或者多对一的关系。在实际应用中，业务方会提供一批需要进行实体链接的POI，并需要将这些POI被系统以极低的误判率自动处理掉，能够减少多少的运营人力。所以除了算法层面改进，在流程中介入一些业务规则也可以高效提高准确率和自动处理率。 将POI建立链接关系，除去文字名称外，还有一些信息可以引入进行比较，这些信息需要覆盖率高且准确性高。 但是一一比对显然复杂度O(MN)较高，以下两种思路用于解决这个问题。 ①通过聚类的方式聚合 类似自下而上的层次聚类，也类似自然人合并中使用的最小连通域（相似度大于阈值的进行联通）。 ②通过建立倒排索引缩小比较候选集 对自有POI库建立倒排索引，待选POI库搜索倒排索引，查出多个ID簇，对所有查取的ID进行去重并进行相似度排序，选择相似度最高的POI。此方法可以很好的将整体流程分成子任务，并且一个单独待选POI进入模型时可以实时计算。 对于此方法，存在‘索引粒度’问题，如果设定的搜索粒度太小，本应该链接的实体没有被查找，就会降低召回率。而如果设定的搜索粒度太大，例如设定同一个城市代替同一个区，那么返回的候选集会变大，工程实现的压力会增大。合理选择粒度，也是一种分词问题，可以在可接受的召回率损失下达到工程实现的优化。\n国内酒店POI实体链接解决方案 通过覆盖率高、准确性高、双方POI库都具有的特征，我们认为，同时拥有名称、地址、电话、经度、维度5个字段，我们可以全方位地定位一家酒店。\n相似度计算 在机器学习没有兴起之前，基于规则的判断（类似决策树）和各特征相似度基于权重的相似度加和（类似于线性回归），这些参数和规则都是基于经验的人为设定。 但是渐渐各种拍脑袋的参数设置、规则补丁的存在，维护规则库将变得特别复杂，导致新案例的泛化能力差。而这时监督模型就可以用来训练这种分类模型（是/不是一个实例）。 ①数据清洗：对特征数据预处理，可以有效提高算法性能。例如，统一所有样本中符号、数字格式、坐标系等。 ②特征生成：一共分为四个模块：名称解析模块、地址解析模块、电话解析模块和经纬度解析模块。除此四部分特征外，缺失值导致无法比较的情况，可以通过为每一个特征配备了一个辅助特征，表示这个维度的特征是真实有比较过的结果还是维度缺失的结果。这个策略的收益非常大，大概F1值提高2%，此外在特征处理中，加入一些简单的统计特征，例如”名称中含有相同字数的比例“等，效果也有明显提升，约F1提升1%。\n名称解析模块 根据观察，名称主要包含了酒店所在城市、酒店品牌名、酒店类型、酒店分店名。因此，名称解析模块的主要任务是从酒店全程中提取各个对应的部分。 一些具体的策略：加载各类词典（城市/品牌/酒店类型），使用分词工具对酒店名全称分词，根据分词结果和词典提取城市、品牌和类型信息。如果没有匹配成功，则需要具体讨论情况（例如北京大饭店为城市+品牌）。\n地址解析模块 我们的目的是希望解析后的两个POI可以比较，那么在实际的技术解决方案中，采用“街”或“道”前面的两个字表示街名。譬如\u0026quot;和平区宝鸡西道1号(总医院后门)\u0026ldquo;中的街道名为\u0026quot;鸡西道\u0026rdquo;，\u0026ldquo;天津市武清区徐官屯大街北头(近徐官屯中学)\u0026ldquo;中的街道名为\u0026quot;屯大街\u0026rdquo;。这种方式虽然不能正确解析街道名，却能在街道名比较中得到很准确的结果。此外，地址中可能有多个街道名出现，所有的地址都需要解析出来。譬如两个POI的地址分别为\u0026quot;河北路与福州道交口\u0026quot;和\u0026quot;福州道1号\u0026rdquo;.如果第一个POI地址的街道名称只解析到\u0026quot;河北路\u0026quot;就停止的话，表示\u0026quot;街道\u0026quot;相似程度的特征分数会很低。\n电话解析模块 电话号码可以视作由国家码 、区域码 、号码本体以及分机号码(一般显示在括号当中)等组成。分析上述案例，通过规则和正则匹配的方式可以很好地提取出对应的部分。这部分的主要难点在于如何尽可能列举电话写法的场景。在实际应用中，如果该模块能正确解析99% 以上的POI电话，那么该模块就可以对电话进行解析。\n经纬度解析模块 是否有经纬写反的情况需要修正。 计算两个经纬度在空间上的直线距离，将计算好的经纬度作为特征输入到特征向量中即可。\n索引粒度配置 哪些POI一定不会和待实体链接的POI链接成功？ 不同城市的POI不用参与比较，但对于北京，酒店POI有数万之多，我们可以用目标酒店POI的坐标以一定距离画一个圈子，这种策略可以用ES直接实现，也可以通过GeoHash算法编程实现。GeoHash算法将地理空间进行网格划分，并将每一个网格进行散列编码。在实际操作中先计算目标POI的GeoHash算法的散列编码，返回目标POI所在空间网格及其相邻8个空间内所有的POI作为候选POI。另外在名称上也可以设置一些规则，例如存在至少一个除‘酒’和‘店’外相同的字。 美团将已有的所有POI通过策略进行候选集推荐，在保证(a)本应实体链接的POI是否在推荐出的候选集中，计算召回率。在保证召回率的同时，尽量减小(b)一个目标POI的候选集的POI个数的平均值。\n其他场景 除了上述酒店POI，也有旅游POI与海外POI实体链接的需求。美团作为一个平台，不同供应商提供的POI在汇总到平台时不可避免地出现大量重复。用户不希望在客户端看到这些重复的推荐结果（虽然是不同供应商），所以针对旅游POI的实体链接和海外酒店POI的实体链接需求的优先级会很高。然而，这两种场景并不能直接复用国内酒店POI实体链接的解决方案。\n旅游POI 首先，电话覆盖率会很低，而且很多电话是供应商电话。因为景点占地面积问题，经纬度需要索引粒度较大。所以，在线上的实际系统中，采用了更多的统计类特征代替了一些复杂、低准确率的解析特征，算法主要的判断依据还是景点的名称。\n海外POI 涉及到英文名称与不同翻译的于其对应的中文名称。所以海外POI实体链接场景只能利用国内POI实体链接的算法框架。而核心工作，对各个字段的解析部分都需要重新开发。针对新的场景，这本书重点介绍如下核心改进策略： ①整理替换词词表，减少同义词对算法的干扰，例如3rd\u0026ndash;\u0026gt;third，corner\u0026ndash;\u0026gt;cor. , S\u0026ndash;\u0026gt;South. ②使用拼音代替文字进行处理，可以解决不同音译匹配问题。例如”施密特酒店“和”斯密特酒店“。 ③对于中文名称和英文名称（各占50%左右），直接训练多组模型，根据是否含有中文名称，选择不同的模型进行预测。\n总结 应用场景 ①业务方会提供一批需要进行实体链接的POI，并需要将这些POI被系统以极低的误判率自动处理掉，能够减少多少的运营人力。这里需要注重准确率。尽管有些找不到实体链接，但是希望找到的实体链接不要误判。 ②不同供应商提供的POI在汇总到平台时不可避免地出现大量重复。用户不希望在客户端看到这些重复的推荐结果（虽然是不同供应商） ③解决已有POI库中的重复问题，找到疑似重复POI对，提交给运营进行人工处理，相对于①中情况，更注重召回率，即可以忍受计算量大，但是不能容忍重复的POI不被找到。所以在索引粒度上，我们需要将索引粒度调大，降低推荐POI相似度的阈值。\n在美团酒旅的应用场景中，60%以上的POI实体链接任务已经能完成自动处理化，剩下的POI实体链接会以高人效的方式进行人工处理。\n","id":13,"section":"posts","summary":"POI points of interest 基本的业务单元，在酒店领域就是酒店。在上一章用户画像领域，就是一个自然人。一个优质的POI库：POI信息完整性、POI重复情况、PO","tags":["“美团机器学习实践”","“读书笔记”"],"title":"美团机器学习实践 - 读书笔记 - Chapter 6 POI实体链接","uri":"https://biofrostyy.github.io/2021/04/%E7%BE%8E%E5%9B%A26/","year":"2021"},{"content":"企业聚焦于利用大数据来提升各种商业行为的效率，例如精准营销、个性化搜索和推荐、征信判断、风险控制、体验优化以及商业分析等。以深入理解用户为出发点的‘用户画像’，作为上述策略实施的基础也就应运而生。\n什么是用户画像\u0026mdash;数据标签化 因为个人工作与用户画像密切相关，私认为用户画像最具有挑战性的工作之一就是将海量繁杂的用户行为数据转化为用户属性体系，例如根据用户参与活动的行为，我们可以得到用户最后一次参与活动日期或用户仅一个月参与活动次数，而在模型中我们使用哪个特征，就取决于业务更关注时间还是次数，这是个长期活动还是短期活动等信息。而我们从这些基于统计的数据中，还可以通过模型提取出例如用户喜好品类、金融风险等标签。而这些标签因为基于模型，所以如果想真正投入业务模型使用，覆盖率、准确率都是我们衡量一个基于模型的标签是否优秀的标准。\n以下是本书中介绍的美团用户画像体系\n用户画像数据挖掘 有一些标签需要通过机器学习训练模型，然后基于模型预测得到。即首先收集样本集合建立机器学习模型，然后在全量数据上做预测计算。建模涉及各种机器学习/数据挖掘方法，譬如比较传统的分类、回归、聚类、关联挖掘等浅层学习方法，当然也在不断尝试当前比较流行的深度神经网络/卷积神经网络/长短期记忆等深度学习乃至增强学习方法。\n画像数据挖掘整体架构 用户画像数据的生产和消费是一个比较复杂的过程，通常涉及数据收集、数据清洗、特征生成、标签建模、预测计算、效果评估、线上应用、效果反馈等多个环节，这里忽略掉部分不太重要的环节，按逻辑上的大致分层，给出美团用户画像的系统结构图。\n1.数据收集：日志数据Logs（包含埋点数据等)、互联网爬取公开数据、第三方数据\n2.特征计算：数据处理（异常值过滤、异构转同构等）、数据加工（统计、平滑、归一等）。本书介绍了美团特征过滤的一个模型\u0026ndash;特征爬虫，给出数据样本后，自动扫描这些结构化的数据表，根据一些相关性指标（如相关系数、卡方、p值等），找到跟样本标签强相关的数据列，稍做处理后可以加入特征库作为后续建模特征使用。\n3.特征库维护：特征生成后，需要一个统一管理的地方，收录新特征并下线老特征，以及可视化展示特征的各种统计指标（如特征的最小值、最大值、均值、方差等各种统计值）。最重要的一点是，为了保证特征质量，这里有一个旁路系统，它用于监控特征的各种波动情况，在有质量风险的时候做预警。\n4.机器学习模型：在拥有完备特征库后，我们就可以使用算法工具完成：特征选择、模型训练、效果评估、例行预测。特征选择通常采用高效的Filter方法，直接通过卡方、信息增益等指标度量特征的重要程度。模 型训练我们会用到各种机器学习和数据挖掘相关的工具，例如 Spark MLlib、Sklearn、XGBoost、TensorFlow等。 为了方便统一预测，我们通常会使用预测模型标记语言（PMML）做模型的表达，从而减少模型预测时大量的适配工作。\n5.应用接口：将预测标签应用到线上，拿到收益反馈。除了常规的直接同步流转使用离线Hive表数据之外，我们也提供了几种通用的平台级工具来管理使用这些标签。针对标签的收录管理，我们开发了TCS (标签统一收录于管理平台)来负责各种标签基于层级划分的收录， 以及标签的数量和质量监控 ;对于数据的使用，其一是对于给定客户ID查询客户属性标签，例如对于某下单用户的服务骑手，会在平台上提前知晓此用户的投诉属性，如经常进行时效投诉，那么可以考虑对于其优先服务。其二是通过属性规则圈定客群，可以应用于精准营销。\n6.画像应用：最后就是将用户画像的标签应用到各个业务线， 如酒店、旅游、美食、外卖、电影、 休闲娱乐等。而应用方向主要有：精细化运营、 个性化排序、个性化推荐、商业分析、金融征信建模、反作弊风控、个性化展现等。用户画像搭建中的关键问题及解决方案 1.用户标识 问题：user_id只能识别内部信息，而外部信息无法联通/ user_id只能记录用户登录状态下的行为，而非登录状态下的浏览信息对于用户增长、营收提升也非常重要/ 很多人拥有多个user_id，例如用多个账号获取优化补贴套。\n解决方案：围绕userID,deviceID等维度建立自然人概念，在此自然人维度上将用户画像统一起来。自然人标识利用最大连通子图中的所有节点为一个自然人ID。\n大数据问题：在美团这样面向C端的业务环境中，用户各种ID 的数量通常高达几十亿量级，计算这种量级的连通图，无论是使用普通的并查集算法，还是简单地做 MapReduce数据迭代，它们都会有一些硬伤， 比如并查集算法难以做多机器并行化，而单机承载数据总量有限无法适应数据日增的环境。使用普通MapReduce求连通图方法 （O(N) 时间复杂度，N为最大子图半径)，面对超级簇问题(半径非常大的连通子图)会有迭代轮次过多无法及时收敛的问题。实际上我们采用了一种优化的MapReduce算法，称为Hash-To-Min ，该方法可以将时间复杂度做到O(log(N))。\n2.特征数据 问题：特征数据稀疏 解决方案：特征库的规划\n3.样本数据 问题：样本缺失、样本少、单样本（只有正样本没有负样本） 解决方案：第一条路还是通过各种途径找样本，第二条路是转，如用户偏好类问题转换成用现在预测未来的问题，用点击/购买等行为作为标注，这样就可以直接按时间段标注样本；第三条路对于小样本、单样本问题，主要是试验学术论文中实验效果比较好的一些学习方法，如自学习算法( Self-Train/Co-Train/Tri-Train)、直推式学习 (Transductive SVM) 及PU学习( One-Class-SVM/Biased-SVM/SPY技术/NB技术等)方法；小样本、单样本问题中， D. J. Miller、H. S. Uyar、T. Zhang 、F. J. Oles等从不同的角度，解释了未标注样本对机器学习有效的问题。其中， D. J. Miller和H. S. Uyar从数据分布估计的角度给出了一个直观的分析，假设所有数据服从于某个由L个高斯分布混合而成的分布。\n标签建模 建模方法-系统工程 标签建模方法除了涉及算法模型的选择调优还涉及大量的系统工程方面的问题。基于Hadoop生态衍生出的一系列大数据处理的工具和框架已经在互联网领域内得到了广泛的应用。以HDFS、Hbase、MangoDB、RocksDB为代表的大数据存储能够提供水平可扩展的海量存储。以MapReduce、Hive、Spark为代表的离线批量计算框架让海量数据的计算成为可能。 ⭐美团用户画像标签建模系统架构：\n离线标签生产平台 \u0026ndash; DMSPA 基于Spark、Hive\nFlame实时标签建模系统 \u0026ndash; 基于Storm Bolt的实时标签处理框架，使用DMSPA离线特征库，实时事件作为kafka消息队列中的信息，以便开发出实时特征\n实时计算从生产的角度看分为两种：用户行为触发的标签（异步）\u0026amp;用户请求触发的标签（同步）\n①其中用户行为触发的标签，与用户实时特征的计算非常类似。由此，我们可以采用和实时特征类似的构架，基于storm来实现这类标签的开发。标签开发者需要为每一个标签开发计算逻辑，实现如下接口并封装成一个类Jar包。开发者只需将该Jar包加入部署到Storm处理的拓扑（代码中添加标签计算的Jar包依赖)中，消费对应的Kafka数据即可产生相应的标签。灌库的Bolt会自动把标签数据写入到数据库中，以备查询。 ②对于用户请求触发的标签，它们需要在请求时进行同步计算得出。比如，用户对于美食品类的实时偏好标签是根据用户在短期内浏览点击的美食品类信息计算得出的，时间越久的点击记录，权重越低，对偏好结果的影响就越小。而这个随时间衰减的效果是和用户请求的时间戳直接相关的，用户在不同时间请求查询到的标签值是不一样的。\n建模方法-算法经验 ①特征工程：特征选择上，filter与embedded用的比较多，wrapper因计算代价偏高和实际效果一般，用的比较少。在用GBDT、RF、DNN、FM等模型进行特征组合的时候，大部分标签都有效果提升，其中提升比较明显的大都在召回率上。\n②模型使用经验：\n一些标签的归类往往不唯一，例如\u0026rsquo;用户品类偏好'，可以用策略打分按简单的统计来处理，也可以按高维偏好来进行处理，还可以按分类来处理。面对多样的标签与多重的问题，扣紧实际的需求场景，进行场景设定和具体问题限制，往往能将问题简化很多并且收获不错的效果。此外简单有效原理也适用。\n不同的模型会有不同的假设条件和适合的场景，如适合高稀疏场景的因子分解机/场感知因子分解机，适合图像语音场景的卷积神经网络/循环神经网络。其中，XGBoost在多个标签挖掘过程中效果不错，LR+RF/LR+GBDT在整体上表现不错，对召回的提升比较明显，LR/SVM等简单线性模型在性价比上比较高，FM/FFM在公司别的场景下效果不错，在画像上效果表现不太理想(与特征数据的实际情况相关)。DNN在标签挖掘的过程中效果不太理想。\n建模实例 首先本书介绍了两种常见的情况，第一种是基于规则的标签挖掘，主要用于样本很少但先验知识较多的情况，即便样本有了之后，这些规则也能作为人工特征加入模型提升预测模型的效果，并举了一个用户品类偏好预测，个性化预测后，购买与浏览top3准确率约提升5%。第二种是有明确样本时的预测，下图是常用的几个模型在性别标签中的准确率。\n第三种是小样本与单样本问题的标签挖掘。在公司的精准营销场景下，我们通常只需要针对一批特定的用户进行推广，比如汽车类产品只对有车的用户广告比较好。因此在挖掘这些特定人群标签时，准确率是首要指标，而召回率就会放低要求。这种场景和数据规模，刚好为小样本、单样本的算法提供了实验场景。从少量已标注和无标注数据学习称为Lu Learning。从正例和无标注数据中学习称为Pu Learning。 第四种是区别于前面提前一天计算标签的-实时标签。实时标签需要谨慎进行特征选择，因为实时预测低延迟是最重要的指标之一。例如实时品类偏好，其中一种方法是基于规则，另一种是将用户接下来一段时间的点击和购买行为作为正的目标信号让模型来预测。\n其中λ(u)【用户偏置：例如冲动型消费者通常比犹豫型消费者更低的购买率】和μ(c)【品类偏置：例如品类本身的热门程度】在线性模型时，只分别对用户相关特征和品类相关特征打分。而在实际建模目标时，一般不会对每一时刻进行建模，而是以用户的实际点击作为分界点，这样产生的样本与实际数据分布一致。同时业务应用逻辑更加契合。\n本书介绍了一个具体的标签挖掘例子-常驻城市预测\n用户画像应用 用户画像实时查询系统 数据量大、响应时间要求极短、系统可用性要求极高 ①构架设计 美团用户画像数据在十亿量级，数据量达到几个TB。并且用户画像查询服务作为公司级基础数据服务，往往在一条后台调用链的最低端，对响应时间要求非常高，一般要求平均耗时在3ms以内，TP99耗时在10ms以内。由于被众多业务依赖，任何服务性能的波动或短时不可用都会直接引发线上故障，因此，画像查询系统的可用性等级也要求达到4个9以上，接近5个9。 离线标签数据可以通过离线批量处理方式，在业务低峰期灌入KV存储系统。然后通过构建后台API服务对外提供数据查询，但是对于实时标签来说，我们该如何调整系统构架，使之满足实时标签的需求呢\u0026mdash;-Lambda架构\n根据场景需求，自由选择每一层由哪些组件来实现，以下是美团画像查询系统\n②存储选型 一般来说，一次内存随机读取的耗时为几十纳秒，一次SSD硬盘随机读取的耗时为几十微秒，一次SATA机械硬盘随机读取的 时为几十毫秒。显然，将所有数据全部存储在全内存的KV存储系统中，会得到最好的性能效果。但内存的成本代价也是极高的。SSD硬盘和SATA机械硬盘的选择相对要容易很多，前者的性能比后者高出三个数量级，而价格只比后者高大约一倍(硬件价格会随时间有波动)。 显然，相比SATA机械硬盘，耗费两倍的成本采用SSD硬盘来获得三个数量级的性能提升，是非常划算的选择。 对于单条数据读取场景，美团采用了自研的NoSQL存储Cellar（基于RocksDB+MDB），与我司使用的Hbase类似，都是基于LSM-Tree存储结构对数据进行持久化，具备强悍的写入性能和优秀的数据查询性能，同时为了提升查询性能，Cellar在RocksDB之外引入MDB全内存数据库引擎来存储热点数据，根据测评，Cellar热点数据的读写性能与Redis相当。 在实际应用中，为了进一步提升系统的吞吐量和极限负载能力，通过压力测试我们发现，如果整个存储集群性能达到极限，是由于Lambda三层架构中的实时数据视图的读取最先达到瓶颈，这是因为由于实时数据视图的覆盖率很低(时间跨度短)，导致大量请求无法命中MDB ，进而击穿缓存访问RocksDB磁盘数据。磁盘的访问量显然更容易达到瓶颈。为了解决这个问题，我们引 入了 Squirrel来存储实时数据视图。Squirrel是美团自研的基于Redis的集群化解决方案，数据全部存储于内存中，不管命中率高低都能有极高的吞吐量。 在实践中，这套存储系统每日承担了50亿+次访问请求，请求平均耗时在 3ms以内，99线耗时保持在 6ms 以内。 用户画像查询作为基础数据服务，往往处于后台调用链的最底层。这就意味着一旦服务质量出现问题，影响面非常大。因此对于此存储的可靠性要求非常高。为保证服务高可靠，我们采取支持跨IDC多活备份、支持依赖熔断、支持多种降级策略等措施。\n人群画像分析系统 主要应用于客群筛选，可以广泛应用在数据分析、定向营销、定向推送等业务中。 基于Hadoop/Spark批处理框架的客群筛选，通过Spark任务扫描全量用户的方式，筛选出符合条件的用户列表，但是这种方式仍旧需要一定的时间。 基于倒排索引的ES的的人群检索可以实现秒级的实时检索人群。\n然而，当用户数据量很大、标签数目很多时，Elasticsearch集群也有可能无法满足我们对于性能的要求，请求耗时可能在几秒甚至几十秒以上，这时候我们就需要根据实际情况对标签数据、 Elasticsearch节点数、分片数、索引方式等各个方面进行迭代优化，寻求最佳的性能体验。限于篇幅和实际场景的复杂性，这里不再对性能优化部分做详细的阐述。\n其它系统 除了以上的实时查询和客群搜索系统，美团还开发了其他工具和系统，进一步提升数据生产效率。\n数据收录系统 为了方便将各业务线数据接入到用户画像标签库，我们制定一套接入协议，来规定数据的提供方式、更新周期以及访问接口，以此打造平台化的标签管理系统一-集标签收录、标签存储、标签监控、标签服务于一体的标签收录系统。标签收录系统可以用来收集散落在各个Hive库里面的标签数据，生成约定好的用户画像实时查询系统和人群用户画像分析系统所需要的格式数据。 目前，标签收录系统支撑了线上所有1700+标签数据的管理和更新操作，日更新数据5亿+条。\n线上应用效果 通过一系列服务和系统的设计、构建和演化，发挥了用户画像在O2O行业里巨大的应用价值，真真切切地将用户画像从理论中的设想变成了实际工作中的价值收益。\n","id":14,"section":"posts","summary":"企业聚焦于利用大数据来提升各种商业行为的效率，例如精准营销、个性化搜索和推荐、征信判断、风险控制、体验优化以及商业分析等。以深入理解用户为出","tags":["“美团机器学习实践”","“读书笔记”"],"title":"美团机器学习实践 - 读书笔记 - Chapter 4 用户画像","uri":"https://biofrostyy.github.io/2021/03/%E7%BE%8E%E5%9B%A24/","year":"2021"},{"content":"机器学习的算法有很多种，在工业界最常用的算法有以下几种：逻辑回归、场感知因子分解、梯度提升树、随机森林、神经网络。在Kaggle 的众多机器学习比赛中，图像语音领域问题通用解决方案都是神经网络的相关模型，比如卷积神经网络、循环神经网络等，其他监督学习问题主要用逻辑回归、梯度提升树、场感知因子分解机和神经网络。\n逻辑回归 由于逻辑回归的理论比较基础，这里不做记录，主要记录应用领域知识\n逻辑回归算法 实际上，为了提高算法收敛速度和节省内存，实际应用在迭代求解时往往会使用高效的优化算法， LBFGS、信赖域算法等(著名的工具包LibLinear就是基于信赖域现的，Spark MLlib里的逻辑回归是基于LBFGS实现的）。但这些求解方法是基于批量处理的，批处理算法无法高效处理超大规模的数据集，也无法对线上模型进行快速实时更新。 随机梯度下降是相对于批处理的另外一种优化方法，它每次只用一个样本来更新模型的权重，这样就可以更快地进行模型迭代。对于广告和新闻推荐这种数据和样本更新比较频繁的场景，快速的模型更新能够更早捕捉到新数据的律进而提升业务指标。谷歌的FTRL就是基于随机梯度下降的一种逻辑回归优化算法。\nFTRL 现在很多公司线上使用的是FTRL算法。FTRL算法是谷歌基于很多前人工作总结出来的。本书的介绍了此算法的迭代过程，表示FTRL比较复杂，但是优化后实现却十分简单。其中λ1和λ2分别是控制L1和L2正则化强度的，而α和β是控制学习速率的。根据建议，β通常取1，而α的值可以根据数据和特征的取值进行调整。 下面是一篇对FTRL非常详细的总结，同时给出了相关论文。 PS. 又看到了熟悉的KDD论坛，不久前研究运筹优化时看到18年一篇滴滴通过强化学习解决司乘匹配问题记忆犹新，想要多了解机器学习实践领域前沿可以多关注KDD🤠。 https://blog.csdn.net/dengxing1234/article/details/73277251\n逻辑回归应用 逻辑回归常用于疾病自动诊断、经济预测、点击率预测等领域 由于其处理速度快且容易并行，逻辑回归适合用来学习需要大规模训练的样本和特征，对于广告十亿量级的特征和亿量级的特征来说，逻辑回归有着天然的优势，因而逻辑回归在工业界获得了广泛的应用 而逻辑回归的缺点是，需要大量的特征组合和离散的工作来增加特征的表达性，模型表达能力弱，比较容易欠拟合。 业界对逻辑回归的研究热点主要集中在稀疏性、准确性和大规模计算上，实际应用逻辑回归前，经常会对特征进行One Hot编码，比如广告点击率应用中的用户 ID 、广告ID。为了实现计算效率和性能的优化，逻辑回归求解有很多种优化方法，比如BFGS、LBFGS 、共辄梯度法、信赖域法，其中前两个方法是牛顿法的变种， LBFGS算法是BFGS算法在受限内存限制下的近似优化。针对逻辑回归在线学习时遇到的稀疏性和准确性问题，谷歌和伯克利分校提出了稀疏性比较好的FOBOS算法、微软提出了 RDA算法。谷歌综合了精度比较好的RDA和稀疏性比较好的FOBOS提出了FTRL ，但在L1范数或者非光滑的正则项下， FTRL的效果会更好。 在实际应用中，逻辑回归也需要注意正则化的问题。L1正则(也称LASSO) 假设模型参数取值满足拉普拉斯分布，L2正则(也称RIDGE) 假设模型参数取值满足高斯分布。\n","id":15,"section":"posts","summary":"机器学习的算法有很多种，在工业界最常用的算法有以下几种：逻辑回归、场感知因子分解、梯度提升树、随机森林、神经网络。在Kaggle 的众多机器学","tags":["“美团机器学习实践”","“读书笔记”"],"title":"美团机器学习实践 - 读书笔记 - Chapter 3 常用模型","uri":"https://biofrostyy.github.io/2021/03/%E7%BE%8E%E5%9B%A23/","year":"2021"},{"content":"#1. 通用流程 问题建模 （评估指标-\u0026gt;样本选择-\u0026gt;交叉验证 ）\u0026mdash;\u0026gt; 特征工程 \u0026mdash;\u0026gt; 模型选择 \u0026mdash;\u0026gt; 模型融合 \u0026mdash;\u0026gt; 模型应用\n①评估指标 线下使用的机器学习评估指标需要尽可能的与线上使用的业务指标变化趋势一致。 并且在实际问题中，我们需要考虑我们对不同误差的容忍程度。 比如在癌症检验、贷款风险的评估中，我们对FN不敏感，但是对FP敏感，即宁可错杀也不放过。又如RMSE与MAE相比，对大误差有更大的惩罚，而MAPE对负值误差的惩罚大于正值误差，而RMSLE对预测值偏小的惩罚会比偏大大（例如，一个人20岁，预测成15岁的惩罚会比25岁大） 分类指标： 1.在阈值两端坐跷跷板的精确率和召回率 2.综合考虑精确率与召回率的F1及带加权的Fα 3.ROC\u0026amp;AUC 4.KS衡量正负样本的区分程度（多用于风控） 5.IV\n回归指标： 1.MAE与加权平均绝对误差（对不同样本赋予不同权重）WMAE 2.与量纲无关，不同问题可比平均绝对百分误差MAPE，缺点：对yi取值敏感，对负值误差惩罚大于正值误差，改进：MASE、sMAPE、MDA.\n②样本选择 从全部样本中选择子集使模型获得的有效信息不会减少，训练效果不变。样本选择不仅可以筛选冗余数据提取出重要的信息，精准打击使模型训练时长减少，以适应计算量大的复杂模型，更可以通过去除噪声数据提高训练集质量，改善模型效果。\n1.去除噪声，最重要的我们可以通过一些业务本身的特性做一些数据过滤工作，例如缺失值是否有含义、清洗爬虫数据或用户最后一次点击（用户跳转或离开）后的曝光。当然处理噪声数据时，需要权衡模型的健壮性和模型的效果。处理误标注实例，常见的有集成过滤法EF、交叉验证委员会过滤法CVCF和迭代分割过滤法IPF，这些方法都是基于融合或者投票的思想进行数据过滤的。\n2.样本选择-采样，采样不仅可以降低训练成本，还可以在不平衡样本中平衡比例。一个好的样本子集应该具有无偏性和很小的样本方差。\n3.样本选择-原型选择PS，根据选择的样本，原型选择相关算法可以分为如下三类：Condensation(保留决策边界处样本)、Edition（删除边界的异常点，使得边界更平滑）和Hybrid(尝试找到最小的样本集S，能够保持甚至提升测试集上的泛化精度)。也可以类似特征选择，根据选择样本的策略进行分类：包装器（根据模型的目标函数）和过滤器（样本的选择标准不基于模型）。\n4.样本选择-训练集选择，构建预测模型来进行样本选择，如决策树、ANN和SVM等\n③交叉验证 测试集可以帮助防止过拟合，也可以帮助调参。我们将划分训练集和测试集的方法统称为交叉验证。一般数据量足够时，一般采用留出法和交叉验证法，而在数据量较小，并且难以有效区分训练集和测试集时，自助法很有用。\n1.留出法，简单划分成两部分，不能充分利用数据并且训练集和测试集的划分严重影响最终结果，但实际工作中有一种普遍的应用场景广泛使用留出法：具有明显时间序列因素的数据，即线上数据的时间都在离线数据之后，这种情况下，应该使测试集时间分布在训练集时间之后。\n2.K折交叉验证，K的选取是一个Bias和Variance的trade-off。K越大，每次投入的训练集的数据越多，模型的Bias越小。但是K越大，又意味着每一次选取的训练集之前的相关性越大（考虑最极端的例子，当k=N，也就是在LOOCV里，每次的训练数据几乎是一样的）。而这种大相关性会导致最终的test error具有更大的Variance。一般来说，根据经验我们一般选择k=5或10。书中有提到，数据稀疏时，留一法也就是K=N很适用，个人没有很理解原理（这里面的稀疏指样本数小还是稀疏矩阵中的稀疏？），如果有缘人看到可以Email我探讨。3.Bootstrapping，有放回的重复采样，由于n次没被采到的概率取n极限得0.368，即测试集约有0.368n条样本。自助法解决了训练集测试集大小所导致的偏差，但是由于改变了初始数据集的分布引入了估计偏差。\n","id":16,"section":"posts","summary":"#1. 通用流程 问题建模 （评估指标-\u0026gt;样本选择-\u0026gt;交叉验证 ）\u0026mdash;\u0026gt; 特征工程 \u0026mdash;\u0026gt; 模型选择 \u0026mdash;\u0026gt; 模型融合 \u0026mdash;\u0026gt; 模型应用 ①评估指标 线下","tags":["“美团机器学习实践”","“读书笔记”"],"title":"美团机器学习实践 - 读书笔记 - Chapter 1 问题建模","uri":"https://biofrostyy.github.io/2021/03/%E7%BE%8E%E5%9B%A21/","year":"2021"},{"content":"特征工程不仅操作困难、耗时，而且需要专业领域知识。应用机器学习基本上就是特征工程。 \u0026ndash;吴恩达🤠 这篇笔记来自于《美团机器学习实践》特征工程部分，我们都知道特征工程在算法的工业应用中有着举足轻重的作用，甚至作为校招生，算法岗位的工作重点就是通过分析研究基于不同的业务场景下的不同特征情况，找到新特征或优化现有特征来提升模型性能，而这些不同场景下的算法本身并不具有特征这样的区别性，例如不同场景下的推荐模型多是应用相似的算法构架。\n特征工程指将原始数据空间变换为模型输入向量空间的过程。好的特征工程不仅需要我们对模型和算法有深入的理解，而需要较强的专业领域知识。特征工程不仅需要我们对模型和算法有深入的理解，更需要较强的专业领域知识(问题理解/业务理解)。特征工程和模型二者此消彼长。例如，对于线性模型，我们需要one-hot编码处理类别变量，要处理特征间的线性关系。但对于随机森林，就可以处理很高维度的数据并且不需要one-hot编码。而对于更复杂的神经网络，模型可以自动进行特征表示，但同时需要很深度的调参才能获得比较好的效果。\n特征工程可以说是业务逻辑的一种数据层面的表示。特征提取的目标是对业务进行精确、全面的描写。例如在对用户推荐排序时，考虑用户维度的现有因素哪些与用户点击相关，可以影响用户的选择（完全可以理解用户ID是完全无关的特征）。另外对于用户行为数据，可能是下单维度或点击行为维度，我们需要从中抽取相关因素，用数值变量进行表示，例如浏览次数、点击时间、以及价格均值等统计量特征。在我工作中，就出现过推荐模型中仅一个特征的增加就使CTR显著增加。当然如果你并不是特别了解业务。探索性数据分析也许可以帮到你哦。如果你特别了解业务，也先做一下数据分析验证你的想法吧。\n不同类型特征的处理方法 数值特征 对于数值特征，我们主要考虑它的大小和分布。例如线性模型对特征大小很敏感，所以需要数值归一化。又例如对于使用平方损失函数的线性回归和最大似然逻辑回归分别假设目标变量服从高斯分布与伯努利分布，那么我们需要对目标变量进行变换使其满足假设。除了对单一特征进行变换，我们还可以对特征进行交叉组合。特征交叉提升了模型的表达能力，让线性模型具有非线性模型的性质，而树模型天然有这种性质。下面是8种常见数值特征的处理方法。\n1.截断：对于连续型数值特征，有时候太大的精度可能只是噪声。因此，可以在保留重要信息的前提下对特征进行截断，截断后的特征也可以看作是类别特征。另外，至于长尾数据，可以先进行对数转换，然后截断。\n2.二值化：对于一些用户行为特征，一些计数特征，如一首歌的播放量可以很快的累加。我们需要考虑具体的次数重要还是是否存在（二值化：0-不存在，1-存在）更重要，甚至最后一次行为距今的时间更重要。\n3.分桶：如果年龄或点击次数一类的数值特征，跨越了不同的数量级，则它不是一个好特征，例如对于逻辑回归这种对数值大小敏感的模型模型，一个特征又只对应一个系数，从而模型往往只对比较大的特征值敏感。通常这种情况的解决方式是分桶（均匀分桶、分位数分桶、聚类分桶），分桶是一种特征的离散化。\n4.缩放：标准化缩放；最大最小值缩放；最大绝对值缩放；基于某种范数的归一化，如L1范数、L2范数将数值变量的范数变为1；平方根缩放或对数缩放（对数缩放对于处理长尾分布且数值为正数的数值变量非常有效，它将大端长尾压缩为短尾，并将小端进行延伸，平方根或者对数变换是幂变换的特例，在统计学中都称为方差稳定的变换，其中Box-Cox变换是简化的幂变换，Box-Cox转换仅对取值为正数的数值变量起作用）；对于有异常点的数据，可以使用更加健壮的缩放，与一般的标准化基于标准差进行缩放不同的是，健壮的缩放使用中位数而不是均值，基于分位数而不是方差。\n5.缺失值处理：大部分模型不能处理特征缺失。对于特征缺失，第一种方法是补均值/中位数/模型预测，另一种方法是将缺失作为一种信息训练模型，一些模型可以处理缺失特征，例如XGBoost。\n6.特征交叉：将特征两两非线性组合（二阶特征），或多特征非线性组合（多阶特征），然后通过特征选择（统计检验或模型的特征重要性）来选择有用的交叉特征。有些特征交叉组合，虽然没有直观的解释，但有可能对于模型效果有很大提升。除了手动构造交叉特征外，有些模型可以自动进行特征交叉组合，例如FM\\FFM和深度多阶特征的DeepFM。特征交叉可以在线性模型中引入非线性特征，提升模型的表达能力。\n7.非线性编码：除了特征交叉，非线性编码也可以在线性模型中学习非线性关系。例如使用多项式核、高斯核等。但选择合适的核函数并不容易。另外一种方法是将随机森林模型的叶节点进行编码喂给线性模型，这样线性模型的特征就包含了复杂的非线性信息。还有基因算法以及局部线性嵌入、谱嵌入、tSNE等。\n8.行统计量：除了对原始数据变量进行处理之外，直接对行向量进行统计也可以作为一类特征，如统计行向量中空值的个数、0的个数、正值或负值的个数，以及均值、方差、最大值、最小值、偏度、峰度等。 具体采取哪一种方式不仅依赖于业务和数据本身，还依赖于所选择的模型，因此首先要理解数据和业务逻辑以及模型特点，才能更好地进行特征工程。\n类别特征 1.one-hot编码：自然数编码下简单模型容易欠拟合，复杂模型容易过拟合。而对于one-hot编码，得到的 特征矩阵太稀疏。one-hot 编码不仅会为数据集增加大量维度，而且实际上并没有太多信息，很多时候 1 散落在众多零之中，即有用的信息零散地分布在大量数据中。这会导致结果异常稀疏，使其难以进行优化，对于神经网络来说尤其如此。 更糟糕的是，每个信息稀疏列之间都具有线性关系。这意味着一个变量可以很容易地使用其他变量进行预测，导致高维度中出现并行性和多重共线性的问题。\n2.基于统计的类别编码：即将类别特征变成关于它的一种统计量，如将该类别变成该类别在样本中出现的次数，这在有些问题中是有效的，比如纽约和新泽西都是大城市，出现次数都会很多，通过计数的类别编码，模型可以从数值里接受“都是大城市”这个信息。\n3.目标编码：基于目标变量对类别特征进行编码，即有监督的编码方法。也被称为似然编码（likelihood encoding）或平均数编码（mean encoding），而在回归问题中，由于Target是连续的，因此我们可以更灵活地生成新的目标编码功能。例如，我们可以采用均值，模式，标准差或百分位数来创建新特征。但是，当训练集特征和测试数据集中分类变量的分布有很大差异时，此方法可能会出现严重过拟合的糟糕情况😰，即在训练集中A类概率更高，B类概率更小，但测试集中相反，或者A本身概率差距太大，例如训练集中0.8，而测试集中0.5。\nX_target = df_train.copy() # 这两特征不能当数值处理，不妨转为object类型同其它非数值特征一起做目标编码 X_target['day'] = X_target['day'].astype('object') X_target['month'] = X_target['month'].astype('object') for col in X_target.columns: if (X_target[col].dtype=='object'): # target非1即0，对类别分组后把target全加起来也就是1的次数；统计分组后的数目即是该类别样本数目 target = dict(X_target.groupby(col)['target'].agg('sum')/X_target.groupby(col)['target'].agg('count')) # 上一步得到了映射的字典，据此将类别替换为目标编码后的值 X_target[col] = X_target[col].replace(target).values  4.贝叶斯目标编码 贝叶斯目标编码（Bayesian Target Encoding）是一种使用目标作为编码方法的数学方法。仅使用均值可能是一种欺骗性度量标准，因此贝叶斯目标编码试图结合目标变量分布的其他统计度量。例如其方差或偏度（称为高阶矩「higher moments」）。 然后通过贝叶斯模型合并这些分布的属性，从而产生一种编码，该编码更清楚类别目标分布的各个方面，但是结果的可解释性比较差。 $P(Y = 1|X = C_j) = \\frac{n|{X=C_j \u0026amp; Y=1}}{n|{X=C_j}} \\quad (1)$ 高基数变量的取值分布往往是不均衡的，有些取值的出现次数会很低，这将造成上式给出的结果非常不稳定。解决这个问题的一种方法是使用下式获得调整后的结果作为新的编码(Micci-Barreca 2001)： $P_j = \\lambda(n|{X=C_j}) \\times P(Y=1|X=C_j) + (1 - \\lambda(n|{X=C_j})) \\times P(Y=1) \\quad(2)$\n5.K折交叉验证目标编码 本书中介绍的是这种目标编码方式(Viacheslav Prokopev 2018；Zumel and Mount 2016) ，注意，K折正则化只对训练集数据进行（完成后，原分类变量的每个分类最多可以与K个编码对应）；而在对测试集数据进行目标编码时，只要直接使用全部训练集数据进行计算即可（完成后，原分类变量的每个分类只与1个编码对应）。也即是说，K折正则化只影响训练集的目标编码和模型拟合结果，不影响测试集的目标编码结果。另外，有关研究提示K的取值在3-6之间时，目标编码的效果最好 (Viacheslav Prokopev 2018) 。\nfrom sklearn.model_selection import KFold X_fold = X.copy() # 数值形式的类别数据-\u0026gt;字符串形式 X_fold[['ord_0','day','month']] = X_fold[['ord_0','day','month']].astype('object') # 二进制数据-\u0026gt;0/1数值 X_fold[['bin_3','bin_4']] = X_fold[['bin_3','bin_4']].replace({'Y':1,'N':0,'T':1,\u0026quot;F\u0026quot;:0}) # 分成K=5折 kf = KFold(n_splits = 5, shuffle = False, random_state=2019) for train_ind,val_ind in kf.split(X): # val_ind是K中的1块数据的索引，而train_ind是剩下的K-1块数据的索引 for col in cols: if(X_fold[col].dtype=='object'): # 用K-1块数据计算Target encoding，记录到字典 replaced = dict(X.iloc[train_ind][[col,'target']].groupby(col)['target'].mean()) # 用刚刚计算出的映射对这1块内容做Target encoding X_fold.loc[val_ind,col] = X_fold.iloc[val_ind][col].replace(replaced).values  K折正则化方法实现起来比较复杂；在数据量比较大时，也可以考虑使用基于校正集的方法 (Zumel and Mount 2016) ：从数据集中划分出一个额外的子集（称校正集），然后使用该子集计算变量的目标编码，并将结果应用到训练集和测试集中去。\n6.对循环特征的编码：有些特征（如星期、月份）具备循环性质，一月到十二月，紧接着又回到一月。均匀的循环特征可以视为对圆的均匀分割： 可以用极坐标系上的角度来描述循环特征的每个位置，而角度可以用sin值和cos值唯一确定。\nX_train_cyclic = X.copy() columns = ['day','month'] # 要操作的两列循环特征 for col in columns: # 对每个循环特征派生两个特征列，如对于4月，是2pi*4/12，取sin和cos即可 X_train_cyclic[col+'_sin'] = np.sin((2*np.pi*X_train_cyclic[col])/max(X_train_cyclic[col])) X_train_cyclic[col+'_cos'] = np.cos((2*np.pi*X_train_cyclic[col])/max(X_train_cyclic[col])) X_train_cyclic=X_train_cyclic.drop(columns,axis=1) # 删除原来的循环特征列  文本特征 自然语言要处理的对象是文本特征，实际的推荐场景中如商品的名称和类别名称。类别特征的处理方法同样适用，基于深度学习的自动特征工程效果变得越来越好。我们可以从以下几个方面对文本特征进行预处理：\n特征选择 与特征提取是从原始数据中构造新的特征不同，特征选择是尽量去除冗余和无关特征，从而从这些特征集合中选出一个子集。特征选择对于机器学习应用来说非常重要。特征选择也成为属性选择或变量选择，是指为了构建模型而选择相关特征子集的过程。特征选择的目的有如下三个：简化模型（提高可解释性）、改善性能（节省存储和计算开销）和改善通用性（降低泛化性和增加过拟合风险）。 特征选择一般包含 产生过程、评价函数、停止准则、验证过程。为了进行特征选择，我们首先需要产生特征或特征子集候选集和，其次需要衡量特征或特征子集的重要性或者好坏程度，因此需要量化特征变量和目标变量以及特征之间的相互联系。为了避免过拟合，我们一般采用交叉验证的方式来评估特征的好坏，为了减少计算复杂度，我们可能还需要设定一个阈值，当评价函数值达到阈值后搜索停止，最后我们需要在验证数据集上验证选出来的特征子集的有效性。过滤方法 过滤方法其实是更广泛的结构学习的一种特例，特征选择旨在找到跟具体的目标变量相关的特征集合，结构学习需要找到所有变量之间的相互联系，结构学习通常将这些联系表示为一个图。最常见的结构学习算法假设数据由一个贝叶斯网络生成，这时结构为一个有向图模型。特征选择中过滤方法的最优解是目标变量节点的马尔可夫毯，在贝叶斯网络中，每一个节点有且仅有一个马尔可夫毯。\n单变量：特征与目标变量的相关性或互信息，计算效率高不易过拟合，但不能排除冗余的变量，主要用于预处理，常用的单变量过滤方法：覆盖率、皮尔森相关系数（线性相关）、Fisher得分、假设检验、KL散度（也是一种树模型中使用的节点选择方法）、最小冗余最大相关性 多变量： 考虑特征变量之间的相互关系（mRMR），特征集合S和目标变量c之间的相关性可以定义为，特征集合中所有特征与目标变量相关性的均值，冗余性为所有特征之间的相关性的均值，而mRMR准则定义为 但mRMR算法没有考虑到特征之间组合的情况，例如目标变量由特征变量进行XOR运算得到。mRMR是一种增量贪心策略，某个特征一旦被选择了，在后续的步骤不会删除。mRMR可以改写为全局的二次规划的优化问题（即特征集合为特征全集的情况）：QPFS-RCDM算法在特征数目相同的情况下能够挑选出质量最佳的特征子集。 相关特征选择（CFS）：好的特征集合包含和目标变量非常相关的特征，但这些特征之间彼此不想关。对于包含K个特征的集合。\n封装方法 封装方法是特征子集搜索和评估指标相结合的方法，前者提供候选的新特征子集，后者基于新特征子集训练一个新模型，并用验证集进行评估，为每一组特征子集进行打分。 特征子集搜索：完全搜索(分支定界在广度优先搜索的基础上加入了分支限界)、启发式搜索（序列向前、序列向后、双向搜索同时向前向后直到搜索到相同的特征子集时停止、增L去R从空集开始每轮先添加L个特征再删除R个特征）、随机搜索\n嵌入方法 嵌入方法将特征选择嵌入到模型的构建过程中。\n工具介绍 过滤：若数据量较小，可以使用Sklearn里的feature_selection模块，若数据量较大，可以使用Spark MLlib。 嵌入：一般机器学习包的线性模型都支持L1正则，如Spark MLlib和Sklearn等。除此之外，在实际应用中比较常用的特征选择方法还有基于树模型的算法包，如Sklearn中的随即森林以及目前在工业界广泛应用的XGBoost，它们都支持根据不同指标（如增益或者分裂次数等）对特征进行排序。针对XGBoost，Xgbfi提供了多种指标对特征组合进行排序。 References: 网络博客 https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b https://blog.csdn.net/SHU15121856/article/details/102100689 期刊 [1] 印晶. 多标签特征选择算法研究及应用[D].南京师范大学,2016.\n","id":17,"section":"posts","summary":"特征工程不仅操作困难、耗时，而且需要专业领域知识。应用机器学习基本上就是特征工程。 \u0026ndash;吴恩达🤠 这篇笔记来自于《美团机器学习实践》特","tags":["“美团机器学习实践”","“读书笔记”"],"title":"美团机器学习实践 - 读书笔记 - Chapter 2 特征工程","uri":"https://biofrostyy.github.io/2021/03/%E7%BE%8E%E5%9B%A22/","year":"2021"}],"tags":[{"title":"leetcode","uri":"https://biofrostyy.github.io/tags/leetcode/"},{"title":"“MyEveryday'","uri":"https://biofrostyy.github.io/tags/myeveryday/"},{"title":"“python”","uri":"https://biofrostyy.github.io/tags/python/"},{"title":"代码沉淀","uri":"https://biofrostyy.github.io/tags/%E4%BB%A3%E7%A0%81%E6%B2%89%E6%B7%80/"},{"title":"“推理悬疑”","uri":"https://biofrostyy.github.io/tags/%E6%8E%A8%E7%90%86%E6%82%AC%E7%96%91/"},{"title":"“推荐系统”","uri":"https://biofrostyy.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"title":"“数学之美”","uri":"https://biofrostyy.github.io/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"},{"title":"数据结构","uri":"https://biofrostyy.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"title":"“消失的第13级台阶”","uri":"https://biofrostyy.github.io/tags/%E6%B6%88%E5%A4%B1%E7%9A%84%E7%AC%AC13%E7%BA%A7%E5%8F%B0%E9%98%B6/"},{"title":"“美团机器学习实践”","uri":"https://biofrostyy.github.io/tags/%E7%BE%8E%E5%9B%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"},{"title":"“读书笔记”","uri":"https://biofrostyy.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"title":"读后感","uri":"https://biofrostyy.github.io/tags/%E8%AF%BB%E5%90%8E%E6%84%9F/"}]}