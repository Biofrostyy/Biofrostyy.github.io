{"categories":[{"title":"AB测试","uri":"https://biofrostyy.github.io/categories/ab%E6%B5%8B%E8%AF%95/"},{"title":"“刷题\"","uri":"https://biofrostyy.github.io/categories/%E5%88%B7%E9%A2%98/"},{"title":"因果推断","uri":"https://biofrostyy.github.io/categories/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"},{"title":"多任务学习","uri":"https://biofrostyy.github.io/categories/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"title":"多模型融合","uri":"https://biofrostyy.github.io/categories/%E5%A4%9A%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/"},{"title":"大师课","uri":"https://biofrostyy.github.io/categories/%E5%A4%A7%E5%B8%88%E8%AF%BE/"},{"title":"推荐系统","uri":"https://biofrostyy.github.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"title":"模块沉淀","uri":"https://biofrostyy.github.io/categories/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80/"},{"title":"模型应用","uri":"https://biofrostyy.github.io/categories/%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/"},{"title":"知识网络","uri":"https://biofrostyy.github.io/categories/%E7%9F%A5%E8%AF%86%E7%BD%91%E7%BB%9C/"},{"title":"“组件沉淀\"","uri":"https://biofrostyy.github.io/categories/%E7%BB%84%E4%BB%B6%E6%B2%89%E6%B7%80/"},{"title":"“论文复现练习\"","uri":"https://biofrostyy.github.io/categories/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E7%BB%83%E4%B9%A0/"},{"title":"“读书笔记”","uri":"https://biofrostyy.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"title":"钢琴","uri":"https://biofrostyy.github.io/categories/%E9%92%A2%E7%90%B4/"},{"title":"音乐原理","uri":"https://biofrostyy.github.io/categories/%E9%9F%B3%E4%B9%90%E5%8E%9F%E7%90%86/"},{"title":"预测","uri":"https://biofrostyy.github.io/categories/%E9%A2%84%E6%B5%8B/"}],"posts":[{"content":"记录读书历程，喜欢在书店畅销书货架及微信读书推荐栏目淘书，看的书大都比较大众。有想过把专业工具书放在paper list中，但最后因为界限模糊还是一并放在了书单里。\n视频也算做“书”记录在此，包括Bilibili、Youtube、MasterClass等。\n第一部分为Book List，记录每个领域我精心挑选的，推荐阅读的书单\n第二部分为Reading Notes，以日记形式，按时间顺序，记录读的所有书的读书感想\n工具书或科普类书籍都有“xx分钟读完”的文章，小说类书籍读后感不多，但也有几篇有感而发记录在读后感分类中，都可以在读书笔记文件夹中查看。  Book List: Machine Learning 基础篇    书名 作者 简介     机器学习基石 台大林轩田 数学基石   机器学习/深度学习 周志华 西瓜书，花书   机器学习/深度学习课程 吴恩达 说实话没看过，但如此广受好评的课程，还是要写进来推荐给大家的   算法4 Robert Sedgewick/ Kevin Wayne 联系具体应用场景的例子真不错，应用场景可以用来反映很多算法特性   Machine Learning Yearning 吴恩达 调参与算法优化   labuladong的算法小抄 labuladong leetcode刷题可以看看   matrix cookbook  矩阵计算，工具书，需要的时候查一下就行，不需要系统阅读（大佬除外😁）   Pytorch深度学习实践 刘二大人 Bilibili的视频，虽然是讲pytorch的，但我没有把它放在应用篇，主要是此课程中包的深度学习理论基础的讲解，教会我在脑中构建出图形化的张量flow的过程，妥妥的入门神课    算法应用篇    书名 作者 简介     深度学习推荐系统 王喆 推荐系统的宏观知识结构，读了两遍，每次都有新收获   美团机器学习实践 美团技术团队 干货很多   论文复现 论文 考虑如何复现，参考论文提供代码，掌握编程套路   pytorch文档 pytorch官网     人工智能知识宽度拓展篇    书名 作者 简介     数学之美 吴恩达 宽度（非深度）拓展，涉及领域很多   浪潮之巅 吴恩达     小说    书名 作者 简介     星星是冰冷的玩具 谢尔盖·卢基扬年科，肖楚舟[译] 软科幻，讨论社会与自由   盲视  科幻作品好看的基本是既不脱离地面，有完整的逻辑和世界观。又飞跃空中，有丰富的想象力和现实价值。   呼吸/降临 特德姜 无话可说，短篇独有的，无需人物的立体刻画，没有缓慢的剧情铺垫，只是在讲一个故事，一个天才作家的天马行空的故事，这种短平快的震撼感太绝了    科普\u0026amp;自然科学 人文\u0026amp;艺术\u0026amp;历史 生活\u0026amp;娱乐  Reading Notes:    书名 作者 读书日期 心得\u0026amp;读书笔记标签     上帝掷色子吗 曹天元 2021.3.5\u0026mdash;\u0026gt;2021.4.11 作者花费了许多笔墨做比喻和感慨，信息密度稍低，但一点都没有减少对我的震撼，对于量子实验所提供的信息，各位勇士们各显神通提出各种对于宇宙奥秘的猜想，从我的角度希望一个客观的绝对的简单而完美的定理可以解答世界吧。   强化学习精要 核心算法与TensorFlow实现  2021.3.20 \u0026mdash;\u0026gt; 暂停于2021.4.5 记录于2021.4.5晚 p.61/308因为关注的VRP问题最新很多paper都开始应用强化学习（强化学习天然适合于这种输出与loss没有直接关系（不可导）的情况），所以选取了这本书了解强化学习。在工业应用中，强化学习还是更多应用于游戏行业，在VRP问题中的尝试应用暂时没有提升太多时间上的优化（找到更优的算子往往会伴随更长的学习时间），所以我在总体了解了强化学习的思想后暂停了对更深入的变体、调优等部分的了解，后续如果有其他相关工作实践，再重新开启吧 :)   幽灵塔 江户川乱步 2021.3.30\u0026mdash;\u0026gt; 2022.4.1 凑单买的一本日系推理小说，没注意是20世纪的出品，读的时候有仿佛在看聊斋志异的轻微“幼稚感”，坏人居然是被打雷吓到心脏病死的（从来没有提到过这个人有心脏病），推理线索和进展也是突然遇到某个人口述出来的，铺垫很少，人物形象很平面，另外男主对女主感情真是令人疑惑   美团机器学习实践 美团技术团队 2021.3.22\u0026mdash;\u0026gt;2021.7.01 干货很多，配合美团技术团队的博客，各个场景都有系统而详细的解决方案，也发现一些设计和构架去应用到现在工作的模型中，个人认为美团技术算是各大厂中真的想要去分享点干货的团队了，很是敬佩   给未来人类的终极12问 洛朗·亚历山大、让-米歇尔·贝尼耶、张芳[译] 2021.4.11\u0026mdash;\u0026gt;2021.5.3 杜海涛同款😂记录于2021.5.3劳动节清晨早上起来把最后一个topic看完了，怎么说呢，简直就是抱着我就看看到底哪里好的想法看完了整本迷你书，整本书就是两个大佬的对话，有举例争辩也有相互支持，但是可能是因为按出版时间算此书已不算前沿，或者只有两个人主观的辩论信息量太少，总体来说体验并不是很好   labuladong的算法小抄 labuladong 2021.4.11\u0026mdash;\u0026gt; 对于一个大数据从业者来说，工程能力很重要，此算法小抄配leetcode，药到病除;)   数学之美 吴军 2021.5.9\u0026mdash;\u0026gt;2021.6.10 本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，工作领域为智能营销推荐算法，而非自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些相关应用。这是一本让你构建宏观体系的书，介绍人工智能运用在工业界的方方面面，让人们对这些问题的解决有一个“道”的框架理解，有趣的是，因为这种宏观的描绘，在读书中会有很多句子背后蕴藏的深刻理论会吸引你停下读书的脚步，对其进行更深的探索，这正是本书的乐趣所在。   消失的第13级台阶 高野和明、赵建勋[译] 2021.5.22\u0026mdash;-\u0026gt;2021.5.23 一本非常有人气的日系悬疑推理小说，本书更多聚焦在社会派的推理，直到中间部分才开始出现紧张而烧脑的案件推进情节，对于本格推理爱好者可能吸引不大，对死刑相关及当下社会的思考才是这本小说最注重且可贵的地方。   金田一十大佳作 横沟正史 2021.5.29\u0026mdash;\u0026gt;2021.6.5 我永远爱变格派推理！   时间的形状·相对论史话[听书] 汪洁 2021.5.25\u0026mdash;\u0026gt;2021.6.25 是《上帝掷色子吗》的完美补充，一起代表物理界的两朵乌云\u0026ndash;相对论与量子力学。在上下班的路上听作者的有声书讲解，还是很有趣的。   果壳中的宇宙[听书] 霍金，于浩[播] 2021.8.1\u0026mdash;\u0026gt;2021.8.7 上下班的路上的小快乐   给忙碌者的天体物理 尼尔·德格拉斯·泰森、孙正凡[译] 2021.6.3\u0026mdash;\u0026gt; 向⭐出发   深度学习推荐系统 王喆 2021.7.26\u0026mdash;\u0026gt; 王喆大佬是我在知乎上最喜欢的博主之一，他总能用最简单的语言精准的讲述，同时又带有自己丰富经验的智慧，让我受益匪浅。因为工作原因，需要系统使用推荐系统，所以购买了大佬的这本书，希望构建自己推荐系统方面完整的知识构架。   唐朝穿越指南 森林鹿 2021.7.27\u0026mdash;\u0026gt; 2021.8.1 当作科普读物读的，除了小时候读过《明朝那些事儿》之外是个纯纯的历史盲，上大学那会连朝代前后都搞不清楚。最近却莫名的对历史生出兴趣来。   长安的荔枝 马伯庸 2021.8.20\u0026mdash;\u0026gt; 2021.8.20 一个很平常的周五，下班比较早，8点就洗了澡躺在床上，故事很短，共计5个小时就翻完了最后一页。一将功成万骨枯，一事功成，也是万头皆白啊，其中一些官场的处事心思甚是巧妙，本职场白痴也是默默记下了。有趣的是，同时在看一本叫《唐朝穿越手册》的科普读物，很多唐朝生活细节都能对应上，很是奇妙   Machine Learning Yearning 吴恩达 2021.9.2\u0026mdash;\u0026gt; 2021.10.1    吴承恩捉妖记 马伯庸 2021.9.10\u0026mdash;\u0026gt; 2021.12.1    喜鹊谋杀案 安东尼·霍洛维茨 2021.10.1\u0026mdash;\u0026gt; 2021.10.6    诡秘之主 爱潜水的乌贼 2021.12.10\u0026mdash;\u0026gt; 2022.3.20 很少看网文的，动辄百万字总会让我失去耐心，但是这一本风太大了！本读者一定要看看起点哈利波特有多好看哈哈。总体来说不错，很喜欢诡秘被一点点揭开的感觉，但同时确实有很大的网文感，主角金手指太太大，前期看得出来有整体的大纲，但后期可能因为日更压力导致有些流水账，没有传统文学的精致感，主角半神后也有些失去紧张感。如果作者愿意再打磨打磨，缩减字数，完善逻辑大纲，出版的话一定是一部很棒的小说。   幽灵塔 江户川乱步 2022.3.30\u0026mdash;\u0026gt; 2022.4.1 凑单买的一本日系推理小说，没注意是20世纪的出品，读的时候有仿佛在看聊斋志异的轻微“幼稚感”，坏人居然是被打雷吓到心脏病死的（从来没有提到过这个人有心脏病），推理线索和进展也是突然遇到某个人口述出来的，铺垫很少，人物形象很平面，另外男主对女主感情真是令人疑惑   计算广告 刘鹏，王超 2022.4.6\u0026mdash;\u0026gt; 2022.5.6 搜广推不分家嘛   星星是冰冷的玩具 谢尔盖·卢基扬年科，肖楚舟[译] 2022.4.8\u0026mdash;\u0026gt; 2022.4.22 人类及其他几个弱小种族潜入外星文明，以求制约银河委员会中强大种族的方法，来摆脱被强大种族奴役的现状。迷雾渐渐揭开，几何学家、暗影族竟然都与人类同族。算是一篇软科幻，更着重社会哲学，更精确一点，是对社会结构、对自由的探讨。印象比较深刻的是几何学家文明中避免人工智能占领世界的方法是让它们觉得自己是唯一的智慧，几何学家的飞船还是个唯心主义者，真的脑洞大开。唯一遗憾的是其中一些谜团没有得到很好的解答，例如提到人类的超时空跳跃与门的原理相关，但具体如果相关，门的原理，或者说门是什么，门是作为一个智慧体在解析一切吗？这篇科幻再“硬”一点就好了   艺术之美 朱良志 2022.4.22\u0026mdash;\u0026gt; 我有罪，我实在没坚持看完，我是个失去高尚审美的人   前男友的遗书 新川帆立 2022.5.4\u0026mdash;\u0026gt; 2022.5.4 中规中矩，日式社会特点很浓重   北京折叠 郝景芳 2022.6.1\u0026mdash;\u0026gt; 2022.6.2 逻辑漏洞有点多，套了科幻壳的社会短篇，老刀只被告诉了通往第一空间的方法就在三个空间通行自如，既然这么简单，平时来回的人应该也挺多的。折叠的原因是想压缩底层冗余人民的生存时间，但是按这个折叠方法上层人民的时间也在被折叠啊？也要活一天睡一天，何必呢。科幻也有软科幻，也可以写人性，批判社会，但是也要世界观完整，逻辑自洽吧   喜鹊谋杀案 安东尼·霍洛维茨 2022.7.1\u0026mdash;\u0026gt; 2022.7.5 案中案，很复古（有福尔摩斯、阿加莎那味），很新颖   海葵 贝客邦 2022.9.11\u0026mdash;\u0026gt; 2022.9.12 看了几集《消失的孩子》后，嫌更新太慢打开了小说，不得不说作者真是太会写了，案件不算复杂，作者完全是通过流畅的文笔、悬疑的铺设、一再反转的探案线索让这本小说这么出彩。尤其是张叶和项义刚得到杨莫在302的指纹位置后在车上那段推理，从杨莫在衣柜上的指纹是为了找东西，到是为了藏东西，发现指纹只在衣柜上有抽屉里没有，说明是为了藏一个很大的物体，再到为了藏一个人，再到为了藏自己，半夜在被窝看书的我不夸张的说真的感受到了生理性的脊背发凉。结尾有些失望，不是虎头蛇尾而是狗尾续貂，可能觉得故事太简单想最后来个大反转最单纯的小绵羊其实是最终boss的设定，但总体来说有些牵强，第一点，许恩怀父母的旧事交代的不是很清楚，母亲为什么怕恩怀为什么要害死恩怀，全文一直在暗示父女感情不好，互相防备，这个伏笔个人觉得发现父亲的暗道性侵的原因有些单薄。第二点，前文一直在暗示许恩怀和杨远长得像，我还以为是杨远失散在外的女儿，结果也没有再回收这个点了，可能是两者都是海葵？许恩怀是善于伪装的毒海葵，而杨远作为不作为的父亲在不自知的情况下也扮演了家庭的毒的角色。再说第三点，许恩怀费心设计这么一大堆，每一个步骤都有极大的赌的成分，巧合性太强，推理牵强，如果许恩怀要害杨莫，可以用更有效的方法，如坏小孩里的失足坠楼，交通事故，做一个看上去高明但却不一定有效果的局，只是为了悬疑而悬疑。最后第四点，许恩怀为什么觉得杨莫消失，自己就可以取而代之呢？这点是我最想不通的，唯一能解释的就是毕竟还是孩子比较单纯吧。   呼吸 特德姜 2022.10.1\u0026mdash;\u0026gt; 2022.10.15 几十年的天才科幻作者生涯中只有十几篇中短篇（两本短篇集）的出产，包揽无数大奖，本短篇集中的绝中绝我要颁给《商人和炼金术士之门》，短短几万字，让我久久无法回神，一路写了超多书评   降临 特德姜 2022.10.15\u0026mdash;\u0026gt; 2022.10.31 不舍得看完   深空降临 子琼 2022.11.07\u0026mdash;\u0026gt; 连载中 很少看网文的，总是担心网文作者的更新压力和（收费模式导致的）凑字数嫌疑会使作品架构和整体效果大打折扣，不过最近有点馋克苏鲁，这本又是很多书友推荐的，开启了漫漫追连载之路。。。   实时数据处理和分析指南 希尔皮·萨克塞纳 2022.12.01\u0026mdash;\u0026gt; 新项目有用到实时技术，借助公司已有的相关平台，进行kafka数据交互及flink开发，以学校的理论学习为基础，进行理论及实践的补充学习   深入理解Flink——实时大数据处理实践  2023.02.01\u0026mdash;\u0026gt;    球状闪电 刘慈欣 2023.04.01\u0026mdash;\u0026gt; 2023.08.17 作者创造了球状闪电现象下的物理逻辑世界，在丰富的想象力的基础上，又能在宏观微观上都符合逻辑细节到位，前期挖的坑、迷惑的点都能很好得填上，在我疑惑林云为何能出现量子实体时，作者提出了量子态的自我观察概念，真是浑身舒爽，实在像是硬核的一场头脑风暴。一些小困惑写在这，期待解答与讨论：①林云能够进行自我观察从而减缓塌缩，但其他的人类例如郑敏也可以，同样也对宏世界量子态有深入研究，为何从未现身过只能通过留下字迹的方式沟通呢②对于留下痕迹的保留时长，量子态郑敏留在张彬手稿上的公式可以长久保留，量子态郑敏可以影响照片上郑敏的年龄（可能是主角的错觉），但墓碑上的公式就会在一段时间内塌缩；再有量子蓝玫瑰，主角和孩子都能看到蓝玫瑰和瞬间消失的过程，妻子却一次也看不到只能闻到③前期第一次尝试捕捉宏电子时，有一个时间0.5秒内捕捉杆要触碰到电子，个人认为这个0.5秒是为了凸显行动的难度，首先通过历史的观察记录，宏电子只是轨迹不定，但是不会突然消失，所以可以在外层多布置几个捕捉装置（事实上后期也是使用网来捕捉），另外宏电子有足球大小，精准度其实需求也不高。最后，最后的感情描写狗尾续貂，搞成偶像剧了，林云作为设定的女主，主角团里的男的都要喜欢她，甚至把她当做白月光在妻子面前怀念，想表达量子浪漫有很多种途径，无论什么最后都要拐到男女之情，尤其在作者并不擅长这种描写时，真是让人难受。   烧火工 刘慈欣 2023.08.18\u0026mdash;\u0026gt; 2023.08.18 童话短篇；对于星星的设定非常温柔，对于太阳烧火的设定非常有想象力👍童话般浪漫的文字   低智商犯罪 紫金陈 2023.08.18\u0026mdash;\u0026gt; 2023.09.10 多条线复杂纠缠最后汇成一条线，无数巧合负负得正令人拍案叫绝，作者构思能力和脑洞都很强大，是很适合拍电影的剧本。唯一缺点是很多网络老旧烂梗，强行黑色幽默，紫金陈还是更擅长严肃推理的。   电幻国度 西蒙·斯塔伦海格 2023.09.21\u0026mdash;\u0026gt; 2023.09.22 非常艺术的一本书，逻辑性故事性都不强。作者用非常短的篇幅构造了一个失去秩序的文明没落的世界。量变引起质变产生的崭新的意识癫狂而迷人，是科技的反噬，人类欲望和贪婪的惩罚。   关键时刻能救命的急救指南 贾大成 2023.09.28\u0026mdash;\u0026gt; 2023.10.20 像是一本百科全书，覆盖了生活中方方面面的急性病症的处理方法。总体来说，除了心肺复苏和烧烫伤等需要立即操作，对于其他的一些骨折包扎复位方法繁杂且大多可以快速找到专业医院的情况，我们做的保持患者通气，把患者放到合适的体位，最重要的还是抓紧时间送医或根据医生指导操作。另外但是学到了很多小知识，例如除了头孢，甲硝唑咳嗽糖浆都不能与酒在一周之内同喝，中风前兆等作为家里有老人的家庭作用很大。最后建议多一些图片，例如锁骨骨折的八字固定法，完全依靠文字太考验读者的想象力了哈哈。   我有一座恐怖屋 我会修空调 2023.09.28\u0026mdash;\u0026gt; 2023.12.20 85个小时，71天，终于看完了，撒花🎉整体来说是非常值得一看的哦，也没有特别恐怖，印象里比较恐怖的有高汝雪手机鬼、小顾午夜末班车，另外冥胎失去视觉的门后故事很完整且巧妙。作者在这些简单的恐怖描写刻画是比较好的，反而复杂的大场景会陷入什么都想写什么都写不好的困境，包括荔湾镇和通灵鬼校，又腻又长，有可以打磨的空间。   一日三秋 刘震云 2023.12.29\u0026mdash;\u0026gt; 2024.01.18 我不太熟悉这类文学题材，过去喜欢看悬疑科幻这些注重剧情跌宕起伏、悬疑伏笔越多越好的小说，所以对于欣赏这类文学小说的方式总是有些生疏。开头还总是兴致勃勃想对前期樱桃神啊鬼啊的逻辑挑刺儿，到中期就被“人文”到忘了这茬了。短短的一篇小说，横跨四五十年，前一页老董还是个人人膜拜的“直播”大师，一页过后就已经垂垂老矣，常常有一种不真实感，时间像流沙想握又握不住。 刘震云的文笔有很强的个人特色，幽默又质朴，仿佛生活劳动人民朴素的发生在我们身边。讲的是花二娘，是樱桃，是林长杰，又是我们自己，认真又不认真得活着。人看似有许多亲人，朋友，可真正懂得真心的又有几人？至于花二娘，每个人都有自己的心结，但只要对生活还有盼头，就还能憋出来个笑话应付过梦中的花二娘，不被压死。但如果对生活失望透顶，就不会再做任何努力。 另外刘震云对市井的生活经历是丰富且有自己思考理解的，往大了说就是懂“生活的智慧”，智慧中又有幽默，往往又都是冷幽默，莫名其妙得合我的胃口。   克莱因壶 冈岛二人 2023.12.29\u0026mdash;\u0026gt; 2024.01.09 我总是会抵触年代过于久远的科幻推理小说，因为大部分情况下当时显得非常先进的设定和想象力，可能是现今耳熟能详的了，就像盗梦空间(2010年)之于本小说(1989年)。但是我还是要给本小说一个大大的推荐，因为与盗梦空间相比，它仍有自己的独特和创造性，例如结局的处理。回到这本小说。太绝了，不仅是前面的真真假假假假真真，而且是最后主人公就算怀疑是虚假的，他也无法证明。即使能证明，他也没有其他途径，只能通过自杀来验证的无能为力的绝望感。引用原文的一句话“与“克莱因壶”扯上关系的 一瞬间，我就已经被吸进壶里了”，这不是说作者经历的一切全都是虚假，而是当有了虚假的记忆存在，即使分得再清也会沉溺其中，因为人类是由记忆塑造的，人类就是记忆，记忆就是人类，另一个科幻设定\u0026ndash;记忆移植通过更换新鲜的躯体保持永生也是基于此。回到本书，想象一下你的脑海中突然以第一视角被植入了一段其他人真实的爱恨经历，即使大家都告诉你那是虚假的，你真的能不被这段记忆影响继续现在平静的生活吗，现在终于懂了盗梦空间中小李子妻子自杀的原因。最后，因为这本书是被书评安利的，虽然没有完全的剧透，但是因为知道了大致走向，整个读书过程疑神疑鬼一惊一乍，怀疑每个字都有伏笔，太累了，以后看推理悬疑可要少看书评了。   月亮银行 靓灵 2024.01.05\u0026mdash;\u0026gt; 2024.01.29 作者更多着笔在情感、人性、社会上，科幻作为一个载体不硬核但是够细腻，每一篇都有一个主题，自闭症、混乱的交通、沉溺与记忆。个人最喜欢的第十篇音错，想象力、故事性全部在线，科幻的设置逻辑合理且异常巧妙，值得一看。   沿着季风的方向 刘子超 2024.01.12\u0026mdash;\u0026gt; 一直是个喜欢旅游的人，但因为工作拖着总是只能在一个地方呆几天，又是个懒人，所以总有种没玩透没读懂的感觉，有一天突然想看看真的深入旅游是什么样的，不是某地的详细介绍，而是以一个游客的第一视角让我真实得感受一个新的国家，幻想着那是我在慢慢成为一个“当地人”。这本书没读完，或许因为复杂的地名让我总是不知道说到哪，或许是还没完全代入自己，毕竟感受一个地方最好的方法肯定是要亲自去体验。也许明天我就重新提起兴趣接着读下去，也许是明年，也许是我启程去印度的前一天。   河边的错误 余华 2024.01.29\u0026mdash;\u0026gt;2024.2.18 余华是先锋文学的代表作家之一。先锋文学中小说的三要素：主旨、人物与情节被弱化。主要着眼于形式上和风格上的创新，强调自我意识。这些特点在这篇余华早期的短篇上表现得非常明显。余华曾写道：“我相信这样的记忆不仅仅属于我个人，这可能是一个时代的形象。”他只是如实地将记忆中的一幕幕再现于笔端，让我们得以窥探并找寻到他所生活的时代的缩影。这一个沉默又疯癫的时代，是一个我没有经历过的时代。所以这些文字对我来说还是有很大冲击性的。可以随手踢死的孩子、凶杀案中事不关己的目击者、日本鬼子侵略下即是受害者也是加害者的国人。每个人在这样一个短衣缩食又无规则的社会中保持了最大限度的性本恶中的自私和冷漠，我愿称之为变态疯子文学。《现实一种》仿佛一个疯子絮絮叨叨的阴暗潮湿诡异的作品，全文平铺直叙没有大起大落的剧情和动人心弦的反转，一个家庭的破碎从一个小插曲开始一发不可收拾，每个人的行为都怪异非常却又有迹可循。作为余华早期作品用词大胆，回到本章的标题，现实一种，一种现实，虽然不符合正能量的核心价值观，但确实有些人多多少少在麻木地生活工作，也是给我们的警醒不要掉入麻木冷漠的怪圈。《河边的错误》比起说是一个情节跌宕起伏的悬疑犯罪故事，我更想说它是一个时代的虚无。作者根本没想告诉我们凶手是谁，文中如惊弓之鸟的村民不知道凶手是谁，但是为了维持村庄表面的宁静，他们不需要真相，只想揪出一个“凶手。甚至作者也不知道谁是凶手，作者和我们一样只是观察着这一切并如实记录下来。书比电影更加隐晦，许亮、王玲等的背景和人物意义毫无介绍，在读书的时候我急迫地想知道有什么隐情，找寻他们行为的蛛丝马迹，可看到最后发现每个人都有秘密，在那个时代中人人都会变成格格不入的“疯子”，秘密是什么并不重要。最终局长让马哲装疯逃避坐牢，千方百计抓疯子的人，最终自己成了疯子。最后提一下马哲的名字，马哲本是唯物主义的本质，但通篇都是虚无，包括没有证据的推理和证人似真似假的口供。最后想感叹一下余华老师的描写功底，他写阴雨天，湿漉漉的街道如同煮开的水一样跳跃。他写日本人说话像水泡翻腾一样。他写军队训练有素的脚步声像众多螃蟹爬上岸来一样“沙沙”作响。观察力和想象力都是一绝。   高智商犯罪 紫金陈 2024.2.22\u0026mdash;\u0026gt;     The (Vampire) Diaries ::::: 2022.07.07 我可太爱克系风了，几年前书店翻到克拉克原著，可能是翻译过于涩涩，没有留下太多印象[捂脸]最近接触了些相关作品和概念，探索慢慢接触世界本真，或者说太喜欢这种打破人类认知、由调查员视角探索未知以逐渐接近世界最终的答案的科幻内核，但对影视化的B级触手怪比较无感，这些生物保持其“不可名状”的状态更加令人恐惧和好奇，否则容易偏离哲学内核变成怪兽电影（没有说怪兽电影不好的意思）\n","id":0,"section":"posts","summary":"记录读书历程，喜欢在书店畅销书货架及微信读书推荐栏目淘书，看的书大都比较大众。有想过把专业工具书放在paper list中，但最后因为界限模糊","tags":null,"title":"⭐♥Book List♥⭐","uri":"https://biofrostyy.github.io/2024/02/%E4%B9%A6%E5%8D%95/","year":"2024"},{"content":"AB测试听起来原理很简单，控制变量法看看用户对于一个策略的实施是否有明显效果。\n但是在当今互联网大规模地构建一个正确可靠的A/B测试平台仍然是一个巨大的挑战：不仅要在实验设计环节应对溢出效应和小样本的双重挑战，平衡好实验偏差与方差以确定合适的实验单元、分组方法和分析方法，给出合理的实验设计，而且要在分析环节应对方差计算、P值计算、多重比较、混淆因素、假阴性（实际策略有效果，但是检测显示无效果）等多种统计陷阱。\n因此，要获得高质量的结果需要对实验和统计有专家级的理解，这无疑增加了实验门槛，难以达成任何人进行实验都可得出可信结论的目标。本文会从统计、工程角度详细介绍当代互联网AB测试工程的场景特点及最前沿解决方案。\n本文主要站在互联网数据科学家角度，尽量做到沉淀经验和统计细节兼备。致力于涵盖所有AB测试相关模块，三万字读懂工业界AB测试的应用。\n目录 一.AB测试及其统计原理  1.1 假设检验 1.2 AB结果有意义需要满足的三个基本要素   二.业界AB测试的一般步骤  2.1 分流  2.1.1 基于层、域嵌套的重叠分流框架 2.1.2 基于冲突检测的分流框架   2.2 实验设计  2.2.1 明确改动点 2.2.2 指标类型与加工逻辑  2.2.2.1 指标业务类型 2.2.2.2 指标统计类型   2.2.3 实验分组方式 2.2.4 分流单元 2.2.5 样本量预估  2.2.5.1 样本量预估公式 2.2.5.2 生产场景中的确认指标  2.2.5.2.1 AB测试的本质:H0到底正不正确？ 2.2.5.2.2 Baseline rate 2.2.5.2.3 Minimum Detectable Effect     2.2.6 增加p值显著速度  2.2.6.1 降方差（Variance Reduction）  2.2.6.1.1 CUPED降方差 2.2.6.1.2 Stratified Sampling 2.2.6.1.3 Post Stratification       2.3 统计引擎  2.3.1 数据诊断 2.3.2 选择效应估计方法  2.3.2.1 差值法 2.3.2.2 DID双重差分  2.3.2.2.1 DID双重差分介绍 2.3.2.2.1 DID双重差分需要满足的假设 2.3.2.2.1.1 线性关系假设 2.3.2.2.1.2 个体处理稳定性假设 2.3.2.2.1.3 平行趋势假设 2.3.2.2.1.3.1 对模型处理： 2.3.2.2.1.3.1.1 经典DID模型 2.3.2.2.1.3.1.2 异时DID 2.3.2.2.1.3.1.3 广义DID 2.3.2.2.1.3.1.4.异质性DID 2.3.2.2.1.3.1.5 三重差分法 2.3.2.2.1.3.2 对数据处理 2.3.2.2.1.3.2.1 倾向得分匹配 2.3.2.2.1.3.2.2 合成控制法     2.3.4 方差计算  2.3.4.1 方差错误导致统计陷阱 2.3.4.2 正确计算方差的方法   2.3.5 p_value显著性检验  2.3.5.1 p_value检验陷阱 2.3.5.2 p_value检验方式       三.AB测试的一般人员分工 四.AA测试 五.生产上多种多样的AB测试  5.1 随机轮转实验 5.2 更高效的AB测试\u0026ndash;interleaving 5.3 用户裂变场景怎么避免污染 5.4 低响应场景     一.AB测试及其统计原理  1.1 假设检验 1.2 AB结果有意义需要满足的三个基本要素   二.业界AB测试的一般步骤  2.1 分流  2.1.1 基于层、域嵌套的重叠分流框架 2.1.2 基于冲突检测的分流框架   2.2 实验设计  2.2.1 明确改动点 2.2.2 指标类型与加工逻辑  2.2.2.1 指标业务类型 2.2.2.2 指标统计类型   2.2.3 实验分组方式 2.2.4 分流单元 2.2.5 样本量预估  2.2.5.1 样本量预估公式 2.2.5.2 生产场景中的确认指标  2.2.5.2.1 AB测试的本质:H0到底正不正确？ 2.2.5.2.2 Baseline rate 2.2.5.2.3 Minimum Detectable Effect     2.2.6 增加p值显著速度  2.2.6.1 降方差（Variance Reduction）  2.2.6.1.1 CUPED降方差 2.2.6.1.2 Stratified Sampling 2.2.6.1.3 Post Stratification       2.3 统计引擎  2.3.1 数据诊断 2.3.2 选择效应估计方法  2.3.2.1 差值法 2.3.2.2 DID双重差分  2.3.2.2.1 DID双重差分介绍 2.3.2.2.1 DID双重差分需要满足的假设 2.3.2.2.1.1 线性关系假设 2.3.2.2.1.2 个体处理稳定性假设 2.3.2.2.1.3 平行趋势假设 2.3.2.2.1.3.1 对模型处理： 2.3.2.2.1.3.1.1 经典DID模型 2.3.2.2.1.3.1.2 异时DID 2.3.2.2.1.3.1.3 广义DID 2.3.2.2.1.3.1.4.异质性DID 2.3.2.2.1.3.1.5 三重差分法 2.3.2.2.1.3.2 对数据处理 2.3.2.2.1.3.2.1 倾向得分匹配 2.3.2.2.1.3.2.2 合成控制法     2.3.4 方差计算  2.3.4.1 方差错误导致统计陷阱 2.3.4.2 正确计算方差的方法   2.3.5 p_value显著性检验  2.3.5.1 p_value检验陷阱 2.3.5.2 p_value检验方式       三.AB测试的一般人员分工 四.AA测试 五.生产上多种多样的AB测试  5.1 随机轮转实验 5.2 更高效的AB测试\u0026ndash;interleaving 5.3 用户裂变场景怎么避免污染 5.4 低响应场景    一.AB测试及其统计原理 在这一章我会根据工程实践中的AB实践流程及理论基础进行详细介绍。\nAB测试，又叫随机对照试验，来源于假设检验，我们现在有两个随机均匀的样本组A、B，对其中一个组A做出某种改动，实验结束后分析两组用户行为数据，通过显著性检验，判断这个改动对于我们所关注的核心指标是否有显著的影响。\n如果我们在做完实验之后，通过显著性检验发现P值足够小，我们则推翻原假设，证明这项改动会对我们所关注的核心指标产生显著影响，否则接受原假设，认为该改动未产生显著影响。如果用一句话来概括的话，AB测试其实就是随机均匀样本组的对照实验。这个就是AB测试的原理。\n1.1 假设检验 先想一个例子，我们做了一次AB测试，获得实验组A组转化率8%，和对照组B组转化率7%，此时8%\u0026gt;7%，但是我们能直接说A组有效果吗？不能，因为有可能是因为正常的波动导致的，无统计学意义（不显著）。\n那该怎么证明其有统计学意义呢？答案是假设检验，假设检验基于一个非常朴素的想法：如果一件事发生的概率小于$\\alpha$(一般取0.05)，那么它就不会发生。如果发生了，那肯定是前提假设有问题。所以假设检验实际上是一个先做原假设，再推翻的过程。\n故A/B测试的假设检验包含两个假设：\n原假设（Null hypothesis, 也叫$H0$）：我们希望通过实验结果推翻的假设。在上述例子里，原假设可以表述为“A和B无明显区别，即A的算法无提升”。\n备择假设（Alternative hypothesis, 也叫$H1$）：我们希望通过实验结果验证的假设。在上述例子里，可以表述为“A与B有明显区别，即A的算法有明显提升（单侧）”。\n我们要计算的概率则是在“A和B无明显区别，即A的算法无提升”的假设下，A组呈现8%转化率的概率。统计学中我们想计算一个事件的概率首先需要知道其分布。幸运的是我们随机抽样的行为符合中心极限定理：即对于独立并同样分布的随机变量，即使原始变量本身不是正态分布，标准化样本均值的抽样分布也趋向于标准正态分布，用人话说就是无论原始变量如何分布，对其进行n次抽样，每次抽样抽取m个样本，只要n和m足够大，这n次抽样出的n个样本均值就趋向于正态分布，且均值与原始变量均值趋于相同。\n回到上述例子，B组的转化率为7%，基于原假设（A组与B组无明显差异），我们认为A组应该符合一个均值为7%的正态分布。为了方便计算，将其规则化为为均值为0的、方差为1的标准正态分布，计算规则化后的数值标准正态分布进行比较，如果落在下图空白部分，即大于1.65或小于-1.65，那么认为此事件发生概率小于5%。\n如果概率概率小于5%的事件让我一次就观测到了，这不符合常理，那么原假设并不正确，我们得到了有统计学意义的结果：A组的策略确实让转化率得到了提升，可以持续应用于实践。\n继续深入，上面提到的“将其规则化为为均值为0的、方差为1的标准正态分布”的规则化方法具体公式是什么呢。\n在统计学中，有明确的公式如下。\n 绝对值类指标。我们平常直接计算就能得到的单个指标，不需要多个指标计算得到。一般都是统计该指标在一段时间内的均值或者汇总值，比如DAU，平均停留时长等。这类指标一般较少作为AB测试的观测指标。  对于绝对值指标，我们使用t检验判断AB两组的表现是否有均值差异：\n$$t=\\cfrac{(\\bar x - \\mu _0)}{\\sqrt{s/n}}$$\n其中$\\bar x$为抽样的样本均值，$\\mu _0$为要推翻的均值（AB测试中为B组的均值）,S为抽样的标准差，n为抽样的样本量。\n 比率类指标。与绝对值类指标相对应，我们不能直接计算得到，而是通过多个指标计算得到。比如某页面的点击率，我们需要先计算页面的点击数和展现数，两者相除才能得到该指标。类似的，还有一些转化率、复购率等等。AB测试观测的大部分指标都是比率类指标。  对于比率类指标，我们使用基于伯努利概率的Z检验判断AB两组的表现是否有比率差异：\n$$Z=\\cfrac{\\bar p - p_0}{\\sqrt{p_0(1-p_0)/n}}$$\n其中$\\bar p$为抽样样本的比例，$p_0$为要推翻的比例，n为抽样的样本量。\n为了直观理解上述公式，回到刚才的例子跟我想象三个情况：\n 如果A组转化率为20%呢？是不是我们认为20%比B组的7%大很多所以更有可能是策略导致而非正常波动。 如果我告诉你，正常情况下B组的转化率方差很小，一般都只在3.8%-4.2%之间波动呢？是不是我们就认为此时策略实施后提升到5%，更有可能是策略生效，而非正常波动。 如果A组和B组的样本量有分别有100个用户，10000个用户和100万个用户，哪个看起来更能证明策略生效呢？当然是100万个用户，因为样本量的提升会自然抵抗异常值导致的波动。  到这里，我们应该就理解A组对比B组是否显著有效，不仅与A组($p_1$)本身相对B组($p_0$)的提升有关，还与AB组的方差($\\sigma$)、样本量(n)有关。且A组相对B组提升越大、方差越小、样本量越大，统计结果越显著。后续我们的实验策略和结果分析都会围绕这几点来进行。\n1.2 AB结果有意义需要满足的三个基本要素 基于假设检验的原理，我们发现在运行AB实验时，要满足三个基本要素：\n①施加不同策略的实验组、对照组可比较，即实验组、对照组的特征均值在实验前相同或在实验前具有固定差异，便于实验后计算哪些差异是由于策略不同导致的；\n 否则，实验后难以确定实验组对照组的差异是由策略导致的还是由于分组导致的，难以准确衡量策略的真实效应；  ②策略之间没有干扰，实验群体相互独立，即当我们比较策略A和策略B时，接受策略A的用户行为不会受接受策略B的用户行为影响；\n 否则，可能会高估策略效应。举个例子，在履约配送范围实验中，橘色是实验组，商户A范围的扩大，会使用户的需求从商户B转移到商户A，如果评估的是商户集合的单量，会造成实验组单量相对对照组多，实验环节，得出扩大配送范围，会提高整体单量，但是当策略在全国应用后，发现单量并没有明显增加，因为实验期间观察的增加仅仅是单量转移，实验组单量转移到了对照组。  ③实验群体的数量要足够，以满足功效要求，避免实验结果假阴性，即实际策略有效果但是由于样本量不足没有检测出来。\n 否则，难以确定策略无效果是真正的无效果还是由于样本量不足，没有检测到策略效果。  二.业界AB测试的一般步骤 不管何种类型的AB实验，都符合分流-\u0026gt;实验-\u0026gt;数据分析-\u0026gt;决策的基本流程，以及需要满足AB实验的3个基本要素。分流是实验平台的顶层设计，它规范和约束了不同实验者如何在平台上独立运行各自实验而不相互影响，运行实验，看似简单，但是成功运行不同类型实验的前提是实验场景要满足其理论假设。\nAB实验主要是通过观察抽样的样本来推断总体的行为，属于预测型结论，数据分析涉及大量的统计学理论，稍有不慎，容易掉入统计陷阱。上述流程，任一环节出错，都可能导致错误的结论，因此，AB实验统计一个数字容易，得到可靠可信的统计结论并不容易。\n2.1 分流 通过实验衡量真实的用户反应，以确定新产品功能的效果，如果无法同时运行多个并行实验，将会大大减慢迭代速度。扩大同时运行实验的数量对于实现更快的迭代是必不可少的。\n分流框架像法律法规规范着规范着大家的日常行为，使大家在社会大家庭中有序生活一样，它规范和约束着不同的实验在不相互影响的前提下如何共享和使用流量，它是实验平台的顶层设计。基于约束的分流框架让实验者指定约束，平台冲突检测根据实验者指定的约束，进一步判断是否允许实验。\n为了增加可以同时运行的实验数量，提高并行性，并允许同时运行多个互斥实验，业界出现了两种分流框架，一种是像谷歌、微软、脸书这种单边业务形态的公司，采用层、域嵌套的分流框架；另一种是像Uber、DoorDash这种多边业务形态的公司，采用基于约束的分流框架。具体如下图所示：\n2.1.1 基于层、域嵌套的重叠分流框架 该分流框架的特点是事先将流量随机打散做桶号标识并提前规划流量的用途，如上图所示，提前将全国流量划分为10等份并用1到10桶号来标识流量，1-6号桶的流量用于短期策略验证，7-10号桶用于长期性策略验证。为了支持同时运行多个互斥实验，提升迭代效率，分别在1-6号桶和7-10号桶中，进一步区分了正交桶和互斥桶，落在正交桶中的流量可以同时进入多个实验，在每进入一个实验前，重新打散一次流量，避免上一实验的残留效应做下一实验造成影响，实现了多个互斥实验的并行运行，落在互斥桶的流量，每次只能进入一个实验，用于运行不满足随机打散条件的实验。基于桶号划分的用于特定用途的一组流量集合，我们称为域；同一流量进入的不同类型的实验，我们称为层。\n该分流框架的优点是不仅可以实现流量复用，扩大实验并行度，而且较容易避免具有潜在交互作用的实验可能给用户带来的糟糕体验。引入层的概念，将系统参数划分为多个层，约束让组合在一起可能产生较差用户体验的实验必须位于同一层，同一用户可进入不同层多个实验以实验流量复用，并防止其进入同一层多个实验，避免糟糕的用户体验。\n不足之处在于：首先，这种分流框架的一大前提是提前将流量打散，这种在大流量的单边场景下尚可，在小流量的多边场景下，难以行得通。多边场景下，考虑到溢出效应，无法直接采用单边实体进行分流，而是采用聚类的方式将有相互作用的多边聚合成一个大的实体，基于大的实体进行分流，考虑到有限的实体数量，这种提前打散的方式难以获得均匀的流量；其次，域提前规定了流量用途，这种提前隔离的方式，降低了流量利用率，无法满足小流量场下实验功效要求，如即使在互斥域中没有实验，也无法采用该流量进行其它的正交实验；第三，这种提前预规划流量用途的分流框架灵活性不足，如果后期发现域的设置不合理，要改变域的配置将付出较大的代价。\n2.1.2 基于冲突检测的分流框架 该分流框架的特点是由实验者制定约束，平台根据实验者制定的约束，确保无法避免潜在交互影响的实验没有同时曝光给用户。如微软、Uber等公司，实验平台都集成了检测交互作用的自动化系统，以避免实验间潜在交互影响。以Uber为例，将策略看作是一组独立参数的集合，并提前声明对应策略涉及的专用参数和与其它策略共享的参数，配置实验时检测是否有任何影响相同参数的两个实验重叠，只要它们没有重叠，便允许实验创建或更新操作。\n该分流框架的优点是灵活且能最大限度的复用流量，相较于重叠流量框架，不受提前划分的域的约束只能在特定域中进行实验，即使对应的域中此时并无其它实验。只要满足进行并行实验的条件，便可以任意圈定流量进行实验；不足之处在于：实验平台需要构建自动检测交互作用的能力。\n以美团的基于冲突检测的分流框架为例，介绍约束框架的设计思路。\n在展开之前，先引入三个概念：算法Key、场景和实验模版。\n算法Key代表一组可独立测试的功能，在技术层面可以表示为一组独立参数集合，场景代表对应算法Key（对于联合实验而言，是多个算法Key；非联合实验，是一个算法Key）下具有相同实验模版的实验集合，实验模版为一组相同实验类型、实验单位、分组方法、评估方法的配置。\n考虑到：\n①同一算法Key，不同实验是针对同一功能不同版本的测试，实验间要互斥；\n②不同算法Key之间，只要其对应的功能之间没有潜在的交互作用，其对应的实验间天然正交可以放心的复用流量，如存在潜在交互作用，只要确保流量能被随机打散，便可消除策略间的潜在相互作用对实验结论的影响。\n因此，针对并行实验，初步的约束如下：\n①同一算法Key下的任意两个实验不能复用流量，冲突；\n②不同算法Key下存在潜在交互作用的两个实验，只要有一个实验类型是随机对照实验，皆可复用流量。约束②不仅避免了全因子流量框架不同策略实验间潜在相互影响的风险，而且避免了重叠流量框架因不同域流量隔离导致的流量复用率低的问题，特别是在准实验、观察性研究比随机对照实验多的情形下，由于准实验、观察性研究分处不同的域，无法实现随机对照实验和准实验、观察性研究之间的流量复用。\n考虑到同一算法Key下不同实验因目标流量或迭代验证的功能不同，同一个算法Key下的不同实验与另一算法Key下的不同实验间是否冲突取决于其对应的测试功能或实验方法，我们引入场景来描述不同算法Key的功能描述和其对应的实验方法，并根据业务经验构造不同场景间的业务影响矩阵。基于不同场景的业务影响矩阵、场景实验方法和并行实验约束，生成场景实验冲突矩阵，基于此矩阵完成不同算法Key实验间的冲突检测。\n不同场景下的联合实验，与其对应算法Key下所有场景实验冲突，与其它算法Key场景实验，根据约束2进一步判定；为避免冲突实验间的流量重叠，提供了基于表达式定义流量范围的能力，通过检测表达式流量覆盖范围避免冲突实验间的流量重叠。基于约束的分流框架，不预先规划流量用途，也没有层、域复杂概念，实验时按需选择流量，只要通过冲突检测，就可以上线实验，不仅降低了用户使用门槛，而且提高了平台灵活性，以适应履约业务场景。\n2.2 实验设计 实验设计是降低溢出效应、提高实验功效、关注实验公平等多种目标进行方差和偏差平衡的过程。\n2.2.1 明确改动点 这是AB测试最为重要的一点，只有我们知道我们要验证什么，我们的AB测试才能真正发挥作用。否则做了很多工作后，发现重要指标我们甚至没有埋点监控，这就浪费了我们的工作时间。\n故实验开始之前，首先要和产品或者项目经理明确实验的变量是什么？我们要保证AB测试的“单一因素原则”，即每一个实验的评估的因素都是单一因素，一次实验不能糅合多个影响因素，否则会导致实验效果难以区分是哪个因素的影响。\n2.2.2 指标类型与加工逻辑 2.2.2.1 指标业务类型   核心指标 核心数据指标通常情况下只有一个，或者是极少数指标的合集，很多时候是一家公司或组织的核心 KPI，可以驱动业务核心价值，比如注册转化率（衡量注册流程优化试验效果）、活动按钮点击率（评估某项推广活动试验的 CTR 效果）、人均使用时长（评估某项推荐算法对用户粘性的改进效果）等。 在确定核心指标时，需要满足两个关键原则：第一，简单的，易理解的，可以在公司/团队范围内被广泛接受；第二，相对稳定的，无需频繁为了一个新功能更新核心指标。 核心指标除了用来衡量试验的效果，还可以用来计算试验所需的样本量（将在后文中详细介绍），由此可见，核心指标直接关系着试验的成败，需要重点关注。\n  驱动指标 驱动指标一般比核心指标更短期，变化更快也更灵敏，帮助我们更加快速、全面地观测业务变化。我们可以通过两个案例做进一步了解： 在某项商品推广活动试验中，核心指标是下单转化率，运营同学可以使用客单价、人均下单次数、退货率等作为驱动指标。观察发现，虽然下单率有所提升，但退货率或客单价指标下降，此时便需要做针对性调整。 某个视频推荐列表试验中，核心指标是人均播放视频数量，驱动指标是完播率、人均观看时长，通过核心指标和多个驱动指标的配合来进行推荐算法效果的评估。 通过以上我们可以看出，驱动指标能够帮助我们从更多、更全面的角度来观察试验给业务带来的影响，尤其是当发现问题时，能够帮助我们及时分析原因、调整策略，通过不断优化达到最终的试验目的。 在确定驱动指标时，需要满足三个原则：第一，驱动指标与核心指标的目标一致，能够直接反映业务变化；第二，当指标发生变化时，能够有既定的途径和方法来优化指标，是可行动、与业务相关的；第三，驱动指标是核心指标的先导指数，需要具备足够的灵敏性，快速衡量大部分试验的效果。\n  护栏指标 护栏指标，可以理解为保护业务的指标，在实际应用过程中，护栏指标的异常可以有效反映出试验设计、基础设施、数据处理环节是否正常，能够帮助我们在评价试验效果时做出正确的权衡取舍，避免因为短期指标优化影响长期指标，从而得出值得信任的试验结果。 举个例子，我们在试验中设置一定的比例让用户命中试验分组（通常建议各组流量平均分配），实际运行中如果发现样本量和构建时的预期不一致，那么可以猜测是否是分流服务出了问题，导致可信度降低。  2.2.2.2 指标统计类型 对于绝对值类指标和比率类指标，我们的P值计算公式不同，后续在统计引擎部分会进行详细介绍。\n  绝对值类指标。我们平常直接计算就能得到的单个指标，不需要多个指标计算得到。一般都是统计该指标在一段时间内的均值或者汇总值，比如DAU，平均停留时长等。这类指标一般较少作为AB测试的观测指标。\n  比率类指标。与绝对值类指标相对应，我们不能直接计算得到，而是通过多个指标计算得到。比如某页面的点击率，我们需要先计算页面的点击数和展现数，两者相除才能得到该指标。类似的，还有一些转化率、复购率等等。AB测试观测的大部分指标都是比率类指标。\n  2.2.3 实验分组方式 根据能否在实验前控制策略的分配，我们将实验分为AB实验和观察性研究（Observational Studies），在AB实验分支下，根据能否控制策略的随机分配，又将AB实验分为随机对照实验（Randomized Experiments）和准实验（Quasi Experiments）。\n  随机对照实验 可以控制对实验对象施加策略的场景，如①测试不同的产品UI对用户的影响，进而决定使用哪种UI；②快速验证首页商品列表图素材对转化率的影响。这些典型的C端实验场景，不仅有海量用户且用户在实验组、对照组间的行为不会相互影响，可以通过随机分组的方式找到同质且独立的实验组和对照组，这类实验称之为随机对照实验，是业界衡量策略效应的黄金标准。\n  准实验 我们不能对实验对象进行随机分配，只能有选择的进行实验组和对照组的分配，这种虽然能够控制策略分配但不能控制策略随机分配的实验，我们称之为准实验，常用的准实验方法如双重差分。\n  随机对照实验，因为其能够保证实验组、对照组两组的特征均值相同，不会因为分组差异干扰对真实效应的衡量，是业界衡量策略效应的黄金标准。在不满足随机对照实验约束的业务场景下衡量策略效应，我们采用准实验的方法，通过改进分组方法消除实验组、对照组可观测特征的差异或使其保持恒定差异，分析环节采用适配准实验场景的分析方法。\n如果由于场景约束，只能基于实验后得到的数据来进行实验的话，就只能采用适用于观察性研究的方法。准实验和观察性研究虽然不是衡量策略效应的金标准，但是如果使用得当，也可以得出相对科学可信的分析结论。\n2.2.4 分流单元 分流单元，顾名思义是接受实验的最小粒度单元，在典型的C端实验场景中，例如：测试不同的产品UI对用户的影响，进而决定使用哪种UI。此时测试的最小分流单元为单个用户。但在一些特殊场景中无法根据单个用户划分，这些场景可能需要基于地理区域划分。 此时实验方式下应该采用多大的分流单元，是综合考虑溢出效应、实验功效、公平性等因素多方平衡的结果。 例如，在美团的履约场景实验中，实验组、对照组运单可以来源于同一区域，由于同一区域的运单可以共享骑手，运单间不独立，导致实验组、对照组存在溢出效应。轮转实验是解决该问题的一个可选项，前提是我们需要在如下两个相互冲突的目标之间做平衡。\n 我们希望可以划分更多的实验单元来增加样本量，这就需要我们将实验单元划的足够小以得出更多的实验单元来保障我们有更多的样本量来满足实验灵敏度要求。 我们希望实验单元划的足够大确保将相互影响的个体包含在一个独立单元中，以消除溢出效应对实验结果的影响。  在有限的样本下，如果只是进行简单的随机分组，不仅会导致实验组对照组的一些指标在实验前存在偏差，而且会由于样本量不足导致无法检测出策略的微小提升，我们到底是通过控制影响指标差异的协变量和改进分组方式来达成偏差和方差的平衡，还是实验前允许偏差存在，通过实验后纠偏的方法进行补充，这些都需要在实验设计时基于算力，以及基于分组方式和分析方法组合方案得出的数据表现综合判断，来制定合理的实验方案。\n2.2.5 样本量预估 2.2.5.1 样本量预估公式 回顾1.1章假设检验，无论是还是比率型指标，p值都与且只与，A组值，B组值，方差及样本量有关。\n将样本量表达式提出：\n$$n=\\cfrac{\\sigma^2}{\\Delta^2}(Z_{\\alpha/2}+Z_\\beta)^2$$\n其中$n$是每组所需样本量，因为A/B测试一般至少2组，所以实验所需样本量为2n； α和β分别称为第一类错误概率和第二类错误概率，一般分别取0.05和0.2； Z为正态分布的分位数函数； Δ为两组数值的差异，如点击率1%到1.5%，那么Δ就是0.5%； σ为标准差，是数值波动性的衡量，σ越大表示数值波动越厉害。\n当观测的指标为绝对值类型/比率型指标时，σ的计算公式有所差异。\n观测指标可以分为两种类型：\n1、绝对值类指标。\n$$\\sigma^2=\\cfrac{2*\\sum^n_1(x_i-\\bar x)^2}{n-1}$$\n2、比率类指标。\n$$\\sigma^2=p_1·(1-p_1)+p_2·(1-p_2)$$\n上面式子中$p_1$我们称为基础值，是实验关注的关键指标现在的数值（对照组）；$p_2$我们称为目标值，是希望通过实验将其改善至的水平；第一类错误和第二类错误上边已经提到，暂不多做说明，通常设定α为0.05，β为0.2。\n从上述两个公式可以知道，在其他条件不变的情况下，如果数据本身的波动性较小，或实验两组数值差异越大，所需要的样本量就越小。从直观上看，就是本身越平稳时产生的较大变化越明显，那么需要的样本量证明越小。\n2.2.5.2 生产场景中的确认指标 由于在AB测试中，比率型指标更常用，我们以比率型指标为例，比例类数值的假设检验在统计学中叫做两样本比例假设检验。在实际生产场景中，要确定这个最小样本量，我们只需要确认这四个指标：\n$1-β$：Statistical power\nα：Significance level\n$p_1$：Baseline rate\n$p_2-p_1$：Minimum detectable effect\n2.2.5.2.1 AB测试的本质:H0到底正不正确？ 首先，回顾A/B测试中的两个假设：\n原假设（Null hypothesis, 也叫H0）：我们希望通过实验结果推翻的假设。 备择假设（Alternative hypothesis, 也叫H1）：我们希望通过实验结果验证的假设。\n统计学中有两类错误：\n 一类错误（Type I error)：冤枉好人，即A有提升但是我们错误地认为此算法无效。我们把第一类错误出现的概率用α表示。这个α，就是Significance Level。  在做A/B测试的时候，我们希望第一类错误越低越好。实际操作中，我们把α人为定一个上限，一般是5%。也就是说，在做实验的时候，我们都会保证第一类错误出现的概率永远不超过5%。\n 二类错误（Type II error)：漏网之鱼，用β表示。根据条件概率的定义，可以计算出β = 1 - Statistical power。  一般情况下对两类错误上限的选取（α是5%，β是20%），我们可以了解到A/B实验的重要理念：宁肯砍掉4个好的产品，也不应该让1个不好的产品上线。\n2.2.5.2.2 Baseline rate 这个看的是在实验开始之前，对照组本身的表现情况，对应T检验中的均值。在我们的实验里，baseline就是红色按钮的历史点击率。从直观上我们可以这么理解baseline： 当baseline很大（接近1）或者很小（接近0）的时候，实验更容易检测出差别（power变大），如果保持power不变，那么所需要的样本数量变小。举个例子，假设红色按钮的点击率是0%。那么，哪怕绿色按钮只有一个用户点击，相对于对照组来说也是挺大的提升。所以即便是微小的变化，实验也会更容易地检测出来。 同理，当baseline居中（在0.5附近徘徊）的时候，实验的power会变小。\n这个参数完全是历史数据决定的。在我们的实验中，我们假定，实验开始之前的历史点击率是15%。所以Baseline Rate=15%。\n此值用于确认上诉公式中的$\\sigma$方差估计量\n$$\\sigma=\\bar e=e_1(1-e_1)+e_2(1-e_2)$$\n其中$e_1=$ Baseline rate\n$e_2=$ Baseline rate+Minimum Dtectable Effect\n2.2.5.2.3 Minimum Detectable Effect 顾名思义，这个参数衡量了我们对实验的判断精确度的最低要求。 参数越大（比如10%），说明我们期望实验能够检测出10%的差别即可。检测这么大的差别当然比较容易（power变大），所以保持power不变的情况下，所需要的样本量会变小。参数越小（比如1%），说明我们希望实验可以有能力检测出1%的细微差别。检测细微的差别当然更加困难（power变小），所以如果要保持power不变的话，需要的样本量会增加。\n在工作中，这个参数的选定往往需要和业务方一起拍板。在我们的实验中，我们选定Minimum Detectable Effect=5%。这意味着，如果绿色按钮真的提高了点击率5个百分点以上，我们希望实验能够有足够把握检测出这个差别。如果低于5个百分点，我们会觉得这个差别对产品的改进意义不大（可能是因为点击率不是核心指标），能不能检测出来也就无所谓了。\n2.2.6 增加p值显著速度 根据上述样本量估计及第一章介绍的假设检验原理计算，我们知道随着检测天数增多，样本量增大。P值会慢慢收敛，直到接近0.05以下实验成立。\n但是在互联网行业的快速迭代中，快速的收敛，即通过更短的天数得到准确的结果，是我们所需要的。根据公式我们有三个思路使得AB实验快速得到显著结果：\n 上线对指标影响较大的策略，然而多数情况下这种策略可遇而不可求 增加实验的样本量，应该是普遍用的最多的，既然需要样本的总量固定，那么可通过提高每日实验流量配比让且更快速地收敛 缩减指标的方差，根据前面样本量计算和显著性检验介绍的公式可以知道，指标方差越小，所需样本量越小，也越容易统计显著  2.2.6.1 降方差（Variance Reduction） AB测试根据大数定理和中心极限定理，大都符合下图分布。而不显著的核心原因之一就是方差太大。如果有办法能够降低方差，那实验结果自然更容易显著一些。\n2.2.6.1.1 CUPED降方差 CUPED（Controlled-experiment Using Pre-Experiment Data），是一种利用实验前的数据来缩减指标方差，进而提高实验灵敏度的方法。由微软2013年发表。Netflix2016年进行多种降方差方法的case study实验，此种方法效果最好。 下图是Netflix的实验结果，黄色的CUPED调整过的在第7天就完全收敛到0.05以下，此时未调整过的p值仍有0.13左右无意义。\n下面我们来介绍CUPED的原理和实现方法。 CUPED的核心思路是构造了一个新的指标，假定实验原来观测的指标为Y，新的指标为$Y_{cv}$（cv表示control variable）。\n$$\\hat{Y_{cv}} = \\bar Y - \\theta \\bar X + \\theta EX$$\n其中 $\\theta$ 是任意实数。由于$- \\theta( \\bar X) + \\theta E(X)=0$那么$\\hat{Y_{cv}}$ 是$E(Y)$的无偏估计。构造出来的Ycv有很好的特性，一是Ycv的均值是我们实验关注的指标Y的无偏估计，二是Ycv的方差小于原来指标的方差。\n$ \\hat {Y_{cv}} $的方差是：\n$$\\begin{aligned} var(\\hat{Y_{cv}})\u0026amp;=var({\\bar{Y} - \\theta \\bar{X}})\\\u0026amp;=var({{Y} - \\theta {X}})/n\\\u0026amp;=\\cfrac{1}{n}(var(Y)+\\theta ^2 var(X)-2\\theta cov(Y,X)) \\end{aligned}$$\n根据上述公式，我们发现当$\\theta = cov(Y,X)/var(X)$时，我们构建出来的$\\hat{Y_{cv}}$方差最小，而最小的方差则是$var(\\hat{Y_{cv}} )=var(\\bar Y)(1- \\rho ^2)$。\n此公式中$\\rho = cor(Y,X)$是Y和X的协方差。故协方差越大，降方差的效果越好。\n此时问题就集中在我们怎么的到这个和Y协方差大的协变量X，我们需要保证所选的这个变量不会受实验策略的影响，且与Y非常相关。多数情况下我们会选择实验前的此指标数据。如果是新用户或活跃频次等原因某些用户无实验前数据，那么微软论文提到可以再引入一个二元协变量，来表示该用户是否在实验前所选周期内出现了。不过Booking.com用的是另一个处理方法，是采用实验前指标的均值来填充那些在实验前所选周期未出现用户的指标的值。\n当然我们除了实验前数据，可以选择实验期间的数据，只是一定要保证所选的这个变量不会受实验策略的影响。比如用户首次进入实验的所属当天星期几就可以作为一个协变量。另外，任何在实验策略效果生效前的实验期间的数据都满足不受实验策略影响的限制，因此都可以作为协变量，尤其是实验策略生效比例极低的情况下，这些数据用来做协变量可能会很有用。\n非用户级别指标的处理方法。因为前面假设实验的基本单元和指标计算的基本单元是用户，如果实验指标的计算单元更细，比如CTR，那么需要delta method和方差缩减一起使用。这时候协变量也就不止限于用户级别数据了，更细粒度如页面级别也是可以的。\n此时我们再回忆一下，降方差的公式及图形展示。\n$$CUPED-adjusted metric = Y - \\theta (X - \\bar X)$$\n$$\\theta = cov(Y,X)/var(X)$$\n2.2.6.1.2 Stratified Sampling 2.2.6.1.3 Post Stratification 2.3 统计引擎 2.3.1 数据诊断 分析环节的数据诊断，旨在提醒实验者注意可能违反实验假设的情况。很多人认为实验一定按照设计运行，实际上这一假设失败的概率远高于人们的预期。失败实验的分析结论通常是有严重偏颇的，甚至一些结论是完全错误的。在输出显著性分析报告之前，通过护栏指标检验，确保业务不会因策略的迭代受到伤害，通过分组同质性检验、“SRM”检验查看实验执行是否符合预期，确保实验本身的可信度，抽样分布检验，为后续选择合适的显著性检验方法提供依据。\n2.3.2 选择效应估计方法 自动选择与数据和实验设计匹配的分析方法，避免统计陷阱。根据分组方式，提供了差值法和双重差分两种效应估计方法。\n2.3.2.1 差值法 对于完全随机分组可以直接进行两组值的比较。\n2.3.2.2 DID双重差分 上文中提到，除了完全随机分组，准实验需要通过DID双重差分规避由于无法完全随机分组造成的组间差异。\n2.3.2.2.1 DID双重差分介绍 步骤：\n 分组：对于一个自然实验，其将全部的样本数据分为两组：一组是受到干预影响，即实验组；另一组是没有受到同一干预影响，即对照组； 目标选定：选定一个需要观测的目标指标，如购买转化率、留存率，一般是希望提升的KPI； 第一次差分：分别对在干预前后进行两次差分（相减）得到两组差值，代表实验组与对照组在干预前后分别的相对关系； 第二次差分：对两组差值进行第二次差分，从而消除实验组与对照组原生的差异，最终得到干预带来的净效应。  $$y_{it}=\\alpha + \\delta I(s(it)=s) + \\gamma_{s(i)} + \\beta_t +\\epsilon_{it}$$\n其中$s(i)$代表个体所属的组别（实验组/对照组），$\\gamma _{s(i)}$代表干预前的量级，$\\beta _i$捕捉处理组与控制组的固有差别，$\\lambda _t$代表干预后的自然增量($\\lambda _0=0$)，$\\delta$代表干预带来的增量，$I(\u0026hellip;)$是指示函数，当其中的等式$s(i)=s$成立时取值为1、否则为0。\n通过最小二乘法OLS估计，我们需要计算的干预带来的增量$\\delta$即实验组干预前后的均值的差减去对照组干预前后均值的差：\n$$\\delta=(\\bar y_{T,1} - \\bar y_{T,0})-(\\bar y_{C,1} - \\bar y_{C,0})$$\n2.3.2.2.1 DID双重差分需要满足的假设 通过前面的描述可以看到，双重差分法的计算过程很简单，即实验组干预前后的均值的差减去对照组干预前后均值的差。但若希望得到的结果是准确的，对应的样本数据需要满足下面三个假设：前两个假设使用时通常会满足、无需专门验证，需要重点验证第三个假设。\n2.3.2.2.1.1 线性关系假设 该假设来自于线性回归，认为因变量（Treatment）与结果变量存在线性关系。\n2.3.2.2.1.2 个体处理稳定性假设 （The Stable Unit Treatment Value Assumption，SUTVA）\n个体的outcome是取决于个体干预变量treatment的一个函数，该假设由两部分组成\n 一致性（Consistency）：个体接受处理后所导致的潜在结果是唯一的。  例：我养狗了会变开心，无论是什么狗、不存在因为狗是黑的就不开心\n 互不干预（No interference）：个体接受处理后导致的潜在结果不受其他个体处理的影响。  例：我在淘宝上领到了红包之后会更愿意买东西，不因为我同事也领了红包就意愿降低了\n2.3.2.2.1.3 平行趋势假设 （Parallel Trend Assumption）\n定义：实验组和对照组在没有干预的情况下，结果的趋势是一样的。即在不干预的情况下，前后两个时间点实验组与对照组的差值一致。\n检验方式：通常情况下我们可以通过画图或者按照定义计算的方式验证样本是否满足假设\n这个假设在随机实验下，通常是满足的，因为两批用户是很近似且同质的用户。但在观察实验的情景下，有可能会不满足，此时不能简单粗暴、不加处理的直接使用DID，需要对模型或数据进行处理。\n2.3.2.2.1.3.1 对模型处理： 2.3.2.2.1.3.1.1 经典DID模型 在传统DID模型上控制了个体固定效应（individual fixed effects）和时间固定效应（time fixed effects），并去除单独变量。模型如下：\n$$y_{it}=\\alpha + \\delta I(s(it)=s) + u_{s(i)} + \\lambda t +\\epsilon{it}$$\n其中$u _{s(i)}$为个体固定效应，加入个体固定效应后，就不必再放入处理组虚拟变量\\gamma _{s(i)},否则会引起多重共线性问题。因为前者包含比后者更多的信息（前者控制到个体层面，而后者仅控制到组别层面）。 其中$\\lambda_t$为时间固定效应，同理,加入时间固定效应就不用再加处理期虚拟变量（post t）。否则，将导致严格多重共线性，因为前者包含比后者更多的信息（前者控制了每一期的时间效应，而后者仅控制处理期前后的时间效应）\n后仍使用最小二乘法OLS，根据聚类稳健标准误（cluster-robust standard errors），进行拟合得到系数$\\delta$。\n例： 根据上图所示，在当前的业务场景下，并不满足传统DID模型的平行趋势假设，如果贸然使用，会造成估计偏差，时间效应的双重差分模型和个体+时间效应的双重差分模型，虽然都满足平行趋势假设，但从实际置信区间看，后者因为考虑了策略对不同个体的差异，波动较小，估计结果更加接近实际值，所以应采用后者。\n2.3.2.2.1.3.1.2 异时DID 在传统与经典DID的模型设定中，一个隐含假设是，处理组的所有个体开始受到政策冲击的时间均完全相同。但有时也会遇到每位个体的处理期不完全一致的情形（heterogeneous timing）；比如，某项试点政策在不同城市分批推出。此时，可使用“异时DID”（heterogeneous timing DID）。\n异时DID的关键在于，既然每位个体的处理期不完全一致，则处理期虚拟变量也因个体而异，故应写为post(i,t)，既依赖于个体 i，也依赖于时间 t。\n2.3.2.2.1.3.1.3 广义DID 以上各种DID方法均假设存在处理组与控制组的区别，但有时某项政策在全国统一铺开，此时只有处理组，并没有控制组，是否还能使用DID呢？答案是“能”，可以尝试“广义DID”（generalized DID）。\n使用广义DID的重要前提是，虽然所有个体均同时受到政策冲击，但政策对于每位个体的影响力度并不相同，不妨以 intensity（i） 来表示。\n2.3.2.2.1.3.1.4.异质性DID 传统的处理效应模型一般假设“同质性处理效应”（homogeneous treatment effects），即所有个体的处理效应都相同。显然，此假定太苛刻，在实践中难以成立。更为合理的假定则为“异质性处理效应”（heterogeneous treatment effects），即允许每位个体的处理效应不尽相同。具体而言：\n1）在DID的框架下，引入异质性处理效应，即在于对交互项（treatpost）的调整，即引入在组别上的交互项（treatpost*group）。\n2）模型建立上，在经典DID的模型中，再引入三重交互项 ，构建异质性DID模型。\n2.3.2.2.1.3.1.5 三重差分法 （Difference-in-differences-in-differences, DDD） 定义：再做一次双重差分消除实验与对照组差异带来的增量，剩下的即干预带来的增量。\n三重差分的概念比较抽象，这里通过一个例子来说明：\n背景：假设淘宝针对杭州的学生（实验组）发放红包，其他人不发。如果想衡量红包带来的转化增量，直观的会选用杭州的非学生作为对照组。但由于学生和非学生的购买力和趋势本身具有差异，此时平行趋势假设无法满足。 解决方案：此时我们可以引入一个其他的城市，譬如宁波。计算宁波学生与非学生的DID，可以认为这里的DID是来自于人群差异的增量；最后我们用杭州的DID减去宁波的DID则得到发红包带来的净效应。\n2.3.2.2.1.3.2 对数据处理 当前常用的处理方式由如下三种：倾向得分匹配（Propensity Score Matching，PSM）、三重差分法（Difference-in-differences-in-differences, DDD）、合成控制法（Synthetic Control Method）。可以理解为人工构建相对同质的实验组和对照组的方法。\n2.3.2.2.1.3.2.1 倾向得分匹配 相对于后两种方式，该方法在工业界更常用。 目的：从干预的人群和未干预的人群里找到两批人符合平行趋势假设 业务理解：在这两个人群里找个两批同质的人（该场景下的同质：在treatment维度上有近似表现的人）\n例子：在探究领取红包对用户购买行为影响的场景下，对用户领取红包的倾向做预测（打分），认为分数相近的用户是matching、即同质的。圈选出分数相同的用户之后再验证平行趋势假设。 完成PSM后数据会呈现一些规律（如图所示）：\n干预人群与非干预人群的score分布 —— 匹配后分布一致\n抽样后人群在一些画像（如年龄、性别、职业）上的分布会更接近，这间接验证了两组人群更为相似（同质）。但我们需要注意的是，通常情况下不能直接根据用户画像去圈同质人群，因为属性分布相似只是propensity score相似的一个必要条件，而不是完全条件。且属性多种多样并不容易进行判断及决策。\n2.3.2.2.1.3.2.2 合成控制法 多用于城市政策的结果估算。 定义：通过对其他对照组加权，构造出一个虚拟的对照组，使其满足假设。\n例：探究消费券对杭州市民的影响，取20%上海市民、30%南京市民、50%合肥市民构建一个虚拟的、与杭州类似的对照组。 为了使虚拟对照组与干预组更接近，需要求解最优的权重组合 ，使得虚拟对照组与实验组最相似。\n值得一提的当随机分流的量级不一致时，也可能会导致不同质，因为量级越大存在多个user_id对应一个自然人的情况越多，会影响user_id粒度的指标统计；\n检验同质性的方法：对于一般的AB分流，我们可以通过分流后两组人群在性别、年龄等属性上分布的相似度来检验同质性。\n2.3.4 方差计算 2.3.4.1 方差错误导致统计陷阱 如果不能正确的估计方差，那么P值和置信区间都将是错的，这些错误会导致我们通过假设检验得到错误的结论。高估的方差会导致假阴性，而低估的方差会导致假阳性。下面是几个估计方差时的常见错误。 2.3.4.2 正确计算方差的方法 方差计算根据是否随机抽样，将其分为独立样本方差计算和非独立样本方差计算，独立样本方差计算，根据指标是增量提升还是相对提升和分流单位与分析单位是否一致综合因素，提供了直接计算和Delta方法计算，避免方差计算陷阱，非独立样本，通过模拟数据的实际分布，给出方差的准确计算。\n2.3.5 p_value显著性检验 2.3.5.1 p_value检验陷阱 容易忽视的检验方式导致的P值计算陷阱：统计学对于多大样本量即可认为中心极限定理成立并没有完全的定论，并非所有大样本场景下的样本分布都满足正态性假设，避免有偏样本采用默认正态分布下的检验方法。Weich t假设检验是参数检验常用的一种检验方法，其本质上假定实验组、对照组样本均值等的渐近正态性成立，该理论实际上是建立在大样本情形下的中心极限定理基础上。统计学对于多大样本量即可认为中心极限定理成立并没有完全的定论，这实际上也取决于原始分布本身偏离正态分布的程度。\n从经验来看，若样本仅稍微偏离正态总体，大于30的样本量或许就足够了。然而对于有偏样本，Ron Kohavi等(2014) 指出当样本偏度大于等于1时，一个经验准则便是只有计算样本均值的观测样本量大于$355 s^2$时才可认为中心极限定律成立。实际抽取了一个样本量为13832的活动实验，其实验组、对照组差值的抽样分布呈现右偏，不符合正态分布，如下图所示：\n如果所有场景下默认采用正态分布情形下的检验方式计算P值，容易导致错误的P值计算。\n2.3.5.2 p_value检验方式 样本量小于30的超小样本下，采用非参的Fisher检验以满足功效要求，在样本量大约1万的超大样本下，接受检验统计量的渐近正态性成立并采用Weich t假设检验；在样本量大于30小于1万下，进一步样本实际分布情况，如果统计量渐近正态性成立，则采用Weich t检验，如果不成立，采用Bootstrap估分布进行统计推断。\n三.AB测试的一般人员分工 谈到实验部分，我们就不得不先介绍一下业界有关AB测试的工作分工。 在工作中，AB测试一般由数据科学家主导，配合算法工程师和后端工程同学，数科同学在策略迭代之初，就参与到算法年度目标的讨论中，辅助算法一起制定量化策略好坏的综合评估指标，并基于场景特点选择合适的实验方法，完成对应场景下的实验设计，后端工程同学，负责将新方法集成到实验平台，作为公共能力为用户提供服务。\n我们将整个流程分为三个阶段：构建想法，通过AB实验验证想法、沉淀知识库形成实验记忆。\n构建想法是实验的输入阶段，构建想法的质量直接决定了实验的效果，如果这个阶段构建的想法不够好，那么AB实验阶段只能起到验证错误的作用，降低犯错误的概率，无法带来增长。\n验证想法就是实践AB实验的过程，可以分为实验假设、实验设计、实验运行、实验分析和实验决策五个关节，实验假设环节，即形成实验目标，构建综合评估指标，实验设计，基于场景约束，选择合适的实验方法。\n最后，通过Launch Review发起实验决策；将成功和失败的案例沉淀下来，形成实验记忆，不仅可以帮助我们发现策略的通用性，而且有助于帮助我们从失败中寻找机会。\n四.AA测试 不同与在A/B测试中是使两个不同页面来进行实验。在A/A测试中，两组用户被分配到两个完全相同的页面组，那为什么要用两个完全相同的页面来做实验？这是因为A/A测试的实验目的并不是为了发现两个版本是否会带来转化的提升，而是为了验证两组实验用户之间是没有显著差别的。\n当公司采用新A/B测试工具时，会选择先完成A/A测试。A/A测试可以为他们提供如下3大帮助：\n 检查A/B测试工具的准确性 在A/A测试中，当实验组和对照组之间绝对没有差异时，那么就可以判断两组用户在关键维度上是一样的、不会对接下来的A/B测试产生混淆与干扰因素。但是，当A/A测试在两个相同变体之间产生了获胜者的情况下，那后续的A/B测试的结果就会是有问题的。  在互联网工程中，数据科学家们常常通过模拟几百次AA实验，查看关注指标的P值是否在0到1之间均匀分布，的方法确认实验方法的可靠性。下表是美团在随机轮转实验中，引入入Fisher和Neyman检验时的模拟验证。\n  为将来的A/B测试设置基准转换率 需要知道将要用作与结果进行对比的基准转换率。A/A测试可以帮助设置网站的基准转化率。让我们来看看这个示例：假设正在运行一个A/A测试，其中实验组A在10,000个访问者中产生了303个转换，而相同的对照组B在10,000个访问者中给出了307个转换。当两个版本之间没有差异时，A的转换率为3.03％，B的转换率为3.07％。因此，将来A/B测试的基准转换率范围可以设置为3.03-3.07％。如果A/B测试的实验组结果是在这个范围内的，那就说明在A/B测试中的实验组和控制组没有显著差异。\n  确定最小样本量 如上章最小样本量确认的公式，AA测试可以帮助我们实验并初始基准转化率及判断精确度的最低要求。Avast的A/A测试进行了12天，他们从中获得了很多数据。总体来看，该测试涉及超过1000万用户和6500笔交易。在“免费”细分中，他们看到转换率差异为3％，平均订单价值（AOV）差异为4％。在“付费”细分中，他们看到了2％的转化率差异和1％的AOV差异。（注意，以上结果是A/A测试结果，A/A测试的目的是为了证明结果没有显著差异。） “所有这些差异在统计数据上都是不显著的，” Michal说。他补充说：“鉴于A/A测试的这些结果，我们设定了内部A/B测试的阈值。例如，如果A/B测试的转换率或AOV的差异小于5％，我们就需要注意这个提升不是实验操纵的结果，而是偶然的。”\n  五.生产上多种多样的AB测试 通过上方的基础介绍我们已经了解了传统AB测试。毋庸置疑AB测试是理论上最客观最准确的线上评估方法。但由于AB测试对于场景假设的要求过于严苛，如随机性、约束性等。除了上述提到的准实验外，在本章我会结合多互联网公司的实践场景进行AB测试的实际应用变体的拓展。\n5.1 随机轮转实验 5.2 更高效的AB测试\u0026ndash;interleaving 先随着我想象一个每时每刻都在互联网发生的场景，你是手机淘宝主页面的算法工程师，主页面由主图推荐、活动推荐和产品推荐等非常多的模块组成，这些模块都在持续的优化迭代，最令人担心的是，每个模块都会互相影响最终导致用户的决策。同时你负责的模块还有几十个算法待测验。你要怎么做AB测试，以在最快的时间内，不被其他场景实验影响保证准确性的情况下，判断出你手上的最优算法呢？\n我们先分析这个场景下的问题：\n1：在互联网场景下，非常活跃用户的数量是少数，但其贡献的点击、时长却占较大的比例，因此实验组和对照组中活跃用户被分在A组的多还是被分在B组的多，将对结果产生较大影响，从而掩盖模型的真实效果。对于这个问题，传统AB测试会按照活跃度分层抽样以保证AB两组之间活跃率相对平衡，但是可能引入其他维度的bias，这个要多方面看各要素的tradeoff了。而interleaving解决这个问题的思想则是不对测试人群进行分组，而是让所有测试者都可以自由选择A和B。在实验结束时，统计每个人可选择A和选择B的比例。这样就消除了AB组测试者自身属性分布不均的问题，也提升了用户选择的效率。\n2：线上AB Test必然要占用宝贵的线上流量资源，当需要测试的备选场景、算法数量非常多时，例如近期同一页面有五个模块需要测试，需要做到互不干涉就需要把线上流量切分为5份，同时对于某个模块的算法研发侧，大量候选算法需要逐一进行AB Test，又会对流量进行切分几十到几百个。根据上一章AB测试样本量小节，样本需要达到一定量才有统计学意义。故大量的算法测试需求和有限的用户流量需这二者之间的矛盾必然愈演愈烈。这就迫切需要设计一个样本需求少且快速的线上评估方法。Netflix促生了以下的AB test的预先阶段来帮助从上百各算法中初筛出个位数的优质算法进入接下来准确的AB测试阶段。\n为此，Netflix设计了一个两阶段的线上测试过程，如下图。 第一阶段利用被称为Interleaving的测试方法进行候选算法的快速筛选，从大量初始想法中筛选出少量“优秀的”Ranking算法。 第二阶段是对缩小的算法集合进行传统的AB Test，以测量它们对用户行为的长期影响。\n在Interleaving测试中，不分AB组，而是只有一组订阅用户，这些订阅用户会接受到通过混合算法A和B的排名生成的交替排名。这就使得用户同时可以在一行里同时看到算法A和B的推荐结果（用户无法区分一个item是由算法A推荐的还是算法B推荐的）。进而可以通过点击或计算观看时长等指标来衡量到底是算法A好还是算法B好。 当然，在用Interleaving方法进行测试的时候，必须要考虑位置偏差的存在，避免来自算法A的视频总排在第一位。因此需要以相等的概率让算法A和算法B交替领先。这类似于在野球场打球时，两个队长先通过扔硬币的方式决定谁先选人，然后在交替选队员的过程。 底是算法A好还是算法B好。 了解了interleaving具体是什么之后，我们回到最初我们提出的两个问题：高效性和准确性。 高效性： 判定算法A是否比算法B好的“错误”概率。可以看出的是interleaving的方法利用10^3个样本就能够判定算法A是否比B好，而AB test则需要10^5个样本才能够将错误率降到5%以下。这就意味着利用一组AB Test的资源，我们可以做100组Interleaving实验。这无疑大大加强了线上测试的能力。 准确性： Interleaving中的实验指标与AB Test指标之间的相关性。每个数据点代表一个Ranking算法。 我们发现Interleaving指标与AB Test评估指标之间存在非常强的相关性，这就验证了在Interleaving实验中胜出的算法也极有可能在之后的AB Test中胜出。 总结 快速的本质是因为用户可以同时看到A和B时选择A，和只能看到A选择A的，信息量是不同的，所以可以通过1/100的少量用户就可以得到结果。 为什么不直接使用inter leaving的结果呢，因为从宏观看最终策略是A还是B，用户都只能看到A或B，而不是AB的混合，所以混合测验出的结果只能是参考，最终需要真实的单独策略模拟即AB测试。从统计看interleaving仍会有位置bias问题，比如一种ranking能够推出最好的top3，但是另一种ranking能够推出最好的top10。这两种混排在一起之后，受限于用户浏览深度，第一种看起来会好很多，但单独上线的指标说不定还是第二个好。所以interleaving只用于初筛，最终还是要用过传统ab test做最终验证。\n5.3 用户裂变场景怎么避免污染 泛裂变社交是指通过社交工具在用户自有的圈子里进行一次或多次的传播，在很短时间内形成用户介绍用户的模式，从而在短时间内实现大量的用户增长及销售额增加。 裂变营销是把个人当成传播的媒介，把原本要给广告商的费用花在单个用户的身上，从而推动用户来帮忙传播。\n理想情况下，AB实验中，实验组是能够参与策略玩法的用户群，对照组是不能参与策略玩法的用户群。但是现在普遍常见的泛社交裂变业务模式会使实验桶和对照桶互相影响，对AB增量的计算会产生一定的污染性，导致增量计算有所偏差，那么如何处理这种污染来还原真实的AB增量，需要一个科学的评估方案。\n5.4 低响应场景 ","id":1,"section":"posts","summary":"AB测试听起来原理很简单，控制变量法看看用户对于一个策略的实施是否有明显效果。 但是在当今互联网大规模地构建一个正确可靠的A/B测试平台仍然是","tags":[],"title":"从数据科学家视角深入理解AB测试","uri":"https://biofrostyy.github.io/2023/12/%E4%BB%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E8%A7%86%E8%A7%92%E6%B7%B1%E5%85%A5ab%E6%B5%8B%E8%AF%95/","year":"2023"},{"content":"背景：交通出行业务场景 业务目标：拆分为用户侧和商品侧，利用用户侧用户的基本属性、社交关系和旅行状态，与商品侧做匹配，匹配的主要目的是三个方面： 需求：用算法预测用户去哪里，或者预测下阶段会去哪里 效率：首页推荐比用户搜索得到的体验更好，减少了用户看到感兴趣0D的时间。 心智：不同的用户，有不同的心智和想法，在出行中，如何捕捉到不同用户的不同心智，也就是用户的潜在心理，来引导与激发用户需求，也是一个难点。\n激发需求：用户为什么想要出去？ 当前热门城市，如冬季的哈尔滨、长白山等滑雪旅游城市。刘亦菲主演的《》大火后的云南大理。这些都是激发用户需求的互联网热门流行。另外节假日，如春节、国庆等固定的假日也会激发出行需求。最后就是用户自发的行为，刚好有闲时或刚好发现一个好地方。这一阶段被定义为种草，此时用户还不确定要不要出行去哪出行，平台的作用是激发并引导用户的出行需求。 行前阶段：用户已经确定要去哪里玩，那么就要确定去那个酒店、交通方式是什么，飞机还是火车，还有相应的演出及景区预约门票等，这是行前阶段。 行中阶段：用户已经到达目的地，捕获信息包括用户当点地点及心智，推荐目的在于周边有什么好玩的，回程或者下一个目标交通、酒店、门票的购买。 行程之后：用户出行后的反馈，然后再循环。\n在用户出行的周期中，主要面临四个挑战： 一.决策长周期性。在传统的电商平台，例如淘宝或美团外卖，用户每个月甚至每天都会进行购物，但是旅游的决策周期非常长，一年可能只出行几次。 二.行为稀疏性。对比电商APP密集的用户行为，旅行决策的长周期性一定程度上导致了旅行场景，用户的行为序列的稀疏性。 三.行为序列性。这是旅游场景的优势。用户出行总是先通过交通工具到一个地方，定酒店住宿，再乘坐交通工具到另一个地方，用户先做什么后做什么有明显的逻辑性。我们的难点就是充分利用逻辑性。 四.时空关联性。如果用户2022年春节期间回了老家，那么其2023年春节期间也很有可能去到同一个目的地，如何捕捉时空关联性，也是一个要解决的问题。\n算法 推荐算法整体上分为召回和排序两个阶段： 召回：主要有热门目的地、热门航向、Swing用户的长期需求、重定向以及用户搜索。 排序：目前线上模型主要支持单任务模型和多任务模型。 单任务学习：使用特征融合或特征交叉的AutoInt或者WDL模型，其中，用户历史典籍行为序列，采用DIN模型。 多任务学习：ESMM、MMOE、PLE等模型。\n为什么要这样做？ 效果如何呢？ 问题的原因是什么？\n有其他因素导致的伪相关，相关不等于因果，辛普森悖论 前后对比引入的干扰因素，尤其是在大互联网公司导致的 现有的序列数据处理方法大都基于RNN或CNN。\n$$y_t = f(y_{t-1}, x_t) \\tag{RNN}$$\nRNN是一个串行的递归序列结构，本质是一个马尔科夫决策过程，无法并行，速度较慢，且无法很好得捕捉全局的结构信息（对长句子、段落来说是致命的）。\n$$y_t = f(x_{t-1}, x_t,x_{t+1}) \\tag{CNN}$$\nCNN起源于Facebook，窗口式遍历整个向量序列，CNN方便不同于RNN，它可以并行，故而捕捉到一些全局信息。\n$$y_t = f(x_{t}, A,B) \\tag{Transformer}$$\nTransformer，即这篇文章的主角，可以并行处理整个序列(训练耗时短)，并能同等训练短期及长期依赖(对长句子也有能有效捕捉信息)，并且对大数据集或有限数据集均有良好表现。由于这些优点，Transformer被使用在序列数据训练中并取得优异成绩，例如语言模型，行为序列建模中。\n经过一段时间的文献阅读及研究求教，我在这里摒弃网络上里讲了一百八十遍的论文直译、高深名词和公式推导，以通俗易懂的角度来理解Transfomer的本质，它与之前的序列建模模型有什么本质区别？到底是架构中的什么在发挥作用？我们应该怎么使用它？\nTransformer的本质 到底是Transformer中的什么在发挥作用?\nTransformer的核心：多头自注意力机制和位置向量多头自注意力机制Multi-Head Self-Attention 虽然Attention机制并不新鲜，但是Google在这里把Attention视作一个层看待，是一个更加正式的架构设计。 $$Attention(Q,K,V) = softmax(\\frac {QK^T} {\\sqrt d_k})V$$ 论文中定义了清晰的Attention公式，其中Q，K，V是三个$n*d_k、d_k*m、m*d_v$的矩阵，都是通过原序列矩阵S，与参数query,key,value相乘后得到的，Q、K、V进行上式相乘运算后得到一个$n*d_v$的矩阵，我们可以认为：这个Attention层，将序列Q($n*d_k$)编码成了一个新的$n*d_v$的序列。\n想要理解这个公式，我们需要先回想一下自注意力$Softmax(SS^T)S$的含义。\n首先$S·S^T$，矩阵S($n*d$)是一个item的序列，每个item都是其中的一个向量。一个矩阵乘以自己转置的运算，可以看做这些item向量分别与其他向量计算内积（第一行乘第一列、第一行乘第二列\u0026hellip;），而向量的内积又表征了两个向量的夹角，即一个向量在另一个向量上的投影，又即两个向量的相关度。投影值越大，两个向量的相关度越大，如果两个向量夹角90°，那么这两个向量线性无关，如果夹角0°，则线性相关性最大。自然而然，这个相关性即是attention，相关性越大，需要的关注度就越大。\n矩阵$\\large SS^T$是一个方阵，我们以行向量的角度理解，里面保存了每个向量和自己与其他向量进行内积运算的结果。softmax就是将这些运算结果(注意力)归一化。让某个item对所有items的注意力加和值为1。\n用$\\large SS^T$这个权重方阵，乘原item矩阵，就得到了自注意力编码结果。\n即上图中，softmax后的“早”字注意力结果为[0.4,0.4,0.2]，可以理解为“早”字由0.4个“早”字，0.4个“上”字，0.2个“好”字组成。其中对“早”和“上”字的关注度比对“好”字的注意力高。经过加权加和后得到新的、编码后的“早”字item的向量。\n在$Softmax(SS^T)S$的基础上，将三个S分别换成(Q,K,V)，不必对(Q,K,V)感到陌生，它们都是S乘上不同的参数(query,key,value),本质上都是$\\large S$的线性变换，如下图。\n那么为什么不直接使用$\\large X$而要对其进行线性变换呢？\n当然是为了提升模型的拟合能力，矩阵$\\large W$都是可以训练的，起到一个缓冲的效果。多头注意力，也是指参数矩阵$\\large W$有多种组合，后将这些attention编码后的结果进行拼接。\n此时对于$Attention(Q,K,V) = softmax(\\frac {QK^T} {\\sqrt d_k})V$，我们已经理解了$softmax({QK^T})V$。\n假设Q，K都服从均值为0，方差为1的标准高斯分布，那么$QK^T$中元素的均值为0，方差为d。当d变得很大时，$QK^T$中的元素的方差也会变得很大，如果QK^T$中的元素方差很大，那么$softmax(QK^T)$的分布会趋于陡峭（分布方差大，分布集中在绝对值大的区域）。总结一下就是$softmax(QK^T)$的分布会和$d$有关。因此中每个元素除以$d$后，方差又变为了1。这使得的分布的陡峭程度和$d$成功解耦，从而使得Transformer在训练过程中的梯度值保持稳定。总的来说，d的维度就是$QK^T$的维度，除以d的目的就是降低方差。\n而在NLP中，我们需要不同的模式识别。比如第一个head用来识别词语间的指代关系(某个句子里有一个单词it，这个it具体指什么呢），第二个head用于识别词语间的时态关系（看见yesterday就要用过去式）等等，来捕捉单词之间多种维度上的相关系数 attention score，将每个head(维度)上的相关系数分数打出，可以具象化地感受每个head的关注点，以句子\u0026quot;The animal didn\u0026rsquo;t cross the streest because it was too tired\u0026quot;为例。设头的数量为num_heads，那么本质上，就是训练num_heads个$W_Q,W_K,W_V$ 个矩阵，用于生成num_heads个 Q,K,V 结果。每个结果的计算方式和单头的attention的计算方式一致，最后将多个头的结果concat起来。值得注意的是，为了使多头的结果维度不受影响且运算量不增加，$W_Q,W_K,W_V$的维度要相应变小为d_model//num_heads。\n位置编码Position Embedding 最开始介绍模型的时候，我们提到过，Transformer只需要自注意力就可以捕捉序列信息，可上一部分讨论的自注意力机制只根据每个item自身的Embedding编码来计算注意力，即只是个精妙的“词袋模型”而已！它并不能捕捉item的前后顺序信息。举个例子，就算把句子中的词都打乱顺序，得到的结果还是一样的。\n那Transformer是通过什么来学习顺序信息的呢？那就是Position Embedding，位置向量，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了，可以看到下图中，位置编码被应用于增强模型输入，让输入的词向量具有它的位置信息，是一个相对独立的模块。\n说到位置编码，其实Facebook的CNN序列模型中就有过应用，但在CNN与RNN模型中，位置编码比较粗糙，因为RNN和CNN本就可以捕捉到位置信息，所以位置编码的作用并不突出。但在Transformer中，位置编码是位置信息的唯一来源，是整个模型的核心成分，论文也对其做了更详细的研究和描述。\n一种好的位置编码方案需要满足以下几条要求：\n它能为每个时间步输出一个独一无二的编码； 不同长度的句子之间，任何两个时间步之间的距离应该保持一致； 模型应该能毫不费力地泛化到更长的句子。它的值应该是有界的； 它必须是确定性的;\n这样它才能用来表征item的绝对关系和相对关系。以往的Position Embedding中，基本都是根据任务训练出来的向量。而Google直接给出了一个构造Position Embedding的公式：\n这种编码不是单一的数值，而是包含句子中特定位置信息的[公式]维向量（非常像词向量）。PE是一个矩阵[句子长度，模型隐层维度(BERT base中取768)]，其中矩阵的每一行都是对应词的位置向量，位置向量长度为模型隐层维度，之后会与输入词向量进行相加（直接相加的原因请看下一章）。下面就是PE这个矩阵的计算方法。\n$$\\begin{cases} PE(pos,2i) = sin(pos/10000^{2i/d_{model}})\\ PE(pos,2i+1) = cos(pos/10000^{2i/d_{model}}) \\end{cases}$$\n其中pos为token在序列中的位置号码，它的取值是0到序列最大长度之间的整数。BERT base最大长度是512，pos取值能一直取到511。当然如果序列真实长度小于最大长度时，后面的位置号没有意义，最终会被mask掉。\n$d_{model}$是位置向量的维度，与整个模型的隐藏状态维度值相同，即嵌入向量的维度，这个维度在bert base模型里设置为768。\n$i$ 是从0到$d_{model}/2-1$之间的整数值，即0，1，2，\u0026hellip;383。\n$2i$ 是指向量维度中偶数维，即第0维，第2维，第4维，直到第766维。\n$2i+1$ 是维度中奇数维，即第1维，第3维，第5维，直到第767维。\nPE(pos,2i)是PE矩阵中第pos行，第2i列的数值，是个标量。这里是在第偶数列上的值，偶数列用正玄函数计算。\nPE(pos,2i+1) 是PE矩阵中第pos行，第2i+1列的数值，是个标量。这里是在第奇数列上的值，奇数列用余玄函数计算。\n由于三角函数是周期函数，随着位置号的增加，相同维度的值有周期性变化的特点。同样对于两个长度相同的句子，它们的位置编码完全一样。\n为什么要使用sin和cos值呢，因为相对绝对位置，相对位置更重要，而三角函数的性质： $$sin(α+β) = sinαcosβ + cosαsinβ$$ $$cos(α+β) = cosαcosβ - sinαsinβ$$\n因此可以推导出，两个位置向量的点积是一个与他们两个位置差值（即相对位置）有关，而与绝对位置无关。这个性质使得在计算注意力权重的时候(两个向量做点积)，使得相对位置对注意力发生影响，而绝对位置变化不会对注意力有任何影响，这更符合常理。但是这里似乎有个缺陷，就是这个相对位置没有正负之分，比如\u0026quot;华\u0026quot;在\u0026quot;中\u0026quot;的后面，对于\u0026quot;中\u0026quot;字，\u0026ldquo;华\u0026quot;相对位置值应该是1，而\u0026quot;爱\u0026quot;在\u0026quot;中\u0026quot;的前面，相对位置仍然是1，这就没法区分到底是前面的还是后面的。\nGoogle在论文中说到他们比较过直接训练出来的位置向量和上述公式计算出来的位置向量，效果是接近的，所以可以直接使用，无需再耗费算力训练位置向量，毕竟Attention自身的复杂度也是比较高的。\n注：在bert的代码中采用的是可训练向量方式。\nTransformer架构 Encoder-Decoder\n其中Encoder部分应用比较多，例如BERT中使用了Encoder部分进行预训练，行为序列建模也只使用了Encoder部分。\nEncoder Encoder是将输入重编码的一个过程，输入$X_{Embedding}$[batch size, sequence length, embedding dimention]，输出相同shape的$X_{hidden}$[batch size, sequence length, embedding dimention]\n从Input开始，通过查表进行Embedding，此时输入$X_{Embedding}$[batch size, sequence length, embedding dimention] (论文中embedding dimention d_model = 512)\n$X_{Embedding}$流转到Positional Encoding，按照上文的正余弦计算公式，计算sequence中的位置向量矩阵$X_{pos}$，并进行相加(为何直接进行相加，而不是concat，后文会探讨)$X_{Embedding}=X_{Embedding}+X_{pos}$，为了可以相加，pos的shape要于embedding的完全相同，即[batch size, sequence length, embedding dimention]\n此时加入位置信息的$X_{Embedding}$进入了重头戏\u0026ndash;自注意力，将$X_{Embedding}$乘不同的权重矩阵$W_Q,W_K,W_V$进行线性映射产生Q，K，V。Key就是键用来和你要查询的Query做比较，比较得到一个分数（相关性或者相似度）再乘以Value这个值得到最终的结果。多头就是多个上述attention模块（参数不共享），以此增加泛化能力，最后将所有的结果concat，由于权重矩阵$W_Q,W_K,W_V$根据头数进行压缩d_model//num_heads，此时$X_{hidden}$维度仍为[batch size, sequence length, embedding dimention]\nAdd \u0026amp; Norm这一步进行了残差连接和Layer Normalization。残差连接就是把输入$X_{Embedding}$和多头自注意力的输出连接起来，即$X_{Embedding}+Attention(Q,K,V)$，此时输出维度为[batch size, sequence length, embedding dimention]。Layer Normalization作用是把神经网络中隐藏层归一为标准正态分布，加速收敛，具体操作是将每一行的每个元素减去这行的均值，再除以这行的标准差，从而得到归一化后的数值。\n上述归一化后的结果[batch size, sequence length, embedding dimention]输入前馈网络，简单的两层线性映射再经过激活函数一下，即$X_{hidden} = Relu(X_{hidden}W_1W_2)$，后再进行一遍上述的Add \u0026amp; Norm操作。\nDecoder Decoder类似于RNN，是一个item间串行的过程。\n将Encoder 输出的编码信息矩阵C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。注意Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 \u0026ldquo;I have a cat \u0026quot;。\n上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 \u0026ldquo;\u0026quot;，预测第一个单词 \u0026ldquo;I\u0026rdquo;；然后输入翻译开始符 \u0026ldquo;\u0026rdquo; 和单词 \u0026ldquo;I\u0026rdquo;，预测单词 \u0026ldquo;have\u0026rdquo;，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。\n第一步：是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 \u0026ldquo;I have a cat\u0026rdquo; (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。\n第二步\u0026ndash;Masked Multi-Head Attention：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。不同的是在$QK^T$后，需要按位乘Mask矩阵，以遮挡每个单词之后的信息，然后才能乘$V$。\n第三步\u0026ndash;Multi-Head Attention：第二个多头注意力，主要特点为K，V矩阵来自Encoder的编码矩阵C，而只有Q来自decoder上一步的输出Z，这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。\n第四步\u0026ndash;预测： Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，输入Z如下： Softmax 根据Z的每一行预测下一个单词：\n要注意的是，这是训练的时候，可以通过mask直接并行输入整个句子，推理时还是要串行训练的。\nTransformer与其他序列建模模型的区别 Transformer好在哪？是哪些特质让它拥有这些优点？它有没有相对于其他模型的缺点？\n优点\n  可以直接计算每个词之间的相关性，不需要通过隐藏层传递\n  可以并行计算，可以充分利用GPU资源\n  这里的并行指的不是很多seq形成的batch同时运行，而是模型本身对单个seq输入训练/推理时的并行能力。Encoder部分不用说，因为position embedding的引入，无需再像RNN一样逐个item计算。对于Decoder端，做推理的时候类似RNN, 是很难并行的. 但是训练的时候可以一口气把整个seq输入进去,通过mask做后续遮掩，做到类似encoder部分的并行。\nTransformer要怎么用 BST模型是阿里搜索推荐团队2019年发布在arXiv上的文章《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》。核心为使用Transformer捕捉用户行为序列的序列信息。目前BST已经部署在淘宝推荐的精排阶段，每天为数亿消费者提供推荐服务。\n对于用户行为序列(UBS:User Behaviour Sequence)的信息捕捉，已有的UBS的建模方式可以归纳为：\n1.sum/mean pooling，工业实践中的效果还不错 2.weight pooling，关键点是weight的计算方式。例如经典模型DIN，DIN使用注意力机制来捕捉候选item与用户点击item序列之间的相似性作为weight 3.RNN类，考虑时序信息，例如阿里妈妈利用GRU捕捉USB序列信息，将DIN升级为DIEN。这是一种非常大的突破，因为在推荐中首次考虑了购买序列的前后时序，即“未来”的信息，例如买了手机的用户，下一刻可能会购买耳机、保护膜。\n随着Transformer在很多NLP任务中的表现超过RNN，相比RNN也有可并行的独特优势，利用Transformer代替RNN来捕捉时序信息是个很自然的想法，BST就应运而生了。其中的核心创新点就是使用Transformer来建模输入特征中的时序特征。\n结构如下： BST符合CTR中典型的 MLP+Embedding 结构，核心在图中右半部分，即使用 Transformer Layer 建模 User Behavior Sequence。\nTransformer的原理已经讲述过了，这里使用的就是encoder部分将行为矩阵映射为另一个矩阵，需要补充的是，论文中使用的行为长度为20，即截断取用户最近的N个行为，若用户少于N个行为则直接padding补零向量，DIEN中最长使用50长度。embedding size中在4～64之间。\nTransformer其他思考 为什么输入向量可以直接相加 https://www.zhihu.com/question/374835153\n多头注意力中，并无法每个头都平均准确得关注不同的点，只有几个头是重要的，可以进行剪枝 https://www.zhihu.com/question/341222779\n  ","id":2,"section":"posts","summary":"背景：交通出行业务场景 业务目标：拆分为用户侧和商品侧，利用用户侧用户的基本属性、社交关系和旅行状态，与商品侧做匹配，匹配的主要目的是三个方面","tags":[],"title":"飞猪用户出行需求预测","uri":"https://biofrostyy.github.io/2023/12/%E6%97%85%E8%A1%8Capp%E7%94%A8%E6%88%B7%E5%87%BA%E8%A1%8C%E9%9C%80%E6%B1%82%E9%A2%84%E6%B5%8B/","year":"2023"},{"content":"背景 不知道有没有人和我一样在学习乐器时好奇。\n为什么do是do呢？\n为什么是高八度，不是高六度呢，do和高八度的do有什么关系？\n为什么和C间、E和F间是半音，而其他C和D、D和E等之间都是全音呢？\n音高是连续的，人类以什么样的标准制造音阶以获得和谐好听的音乐？\n这其中有没有类似于绘画中的“黄金分割律”的数学规律呢？\n一.音的本质 音是振动产生的，音的属性具有对应的物理含义：音高（频率）、音长（时值）、音强（振幅）和音色（频谱）。其中，音高是音的主要属性。振动规则（周期性强）、容易区分音高（弥散度小）的音称为乐音，是音乐中的主角。\n频率过高或过低的声音人耳不能感知或感觉不舒服，音乐中常使用的频率范围大约是 16~4000 赫兹，而人声及器乐中最富于表现力的频率范围大约是 60~1000 赫兹。\n在弦乐器上拨动一根空弦，它发出某个频率的声音，如果要求你唱出这个音你怎能知道你的声带振动频率与空弦振动频率完全相等呢？这就需要“共鸣原理”：当两种振动的频率相等时合成的效果得到最大的加强而没有丝毫的减弱。因此你应当通过体验与感悟去调整你的声带振动频率使声带振动与空弦振动发生共鸣，此时声带振动频率等于空弦振动频率。\n二.绝对音高 从历史上看，绝对音高一直都在变化，不同国度、不同地区、不同地区、不同时代都有不同的标准，根据对欧洲保留下来的管风琴所做的测定，中央C上方A（即我们熟知的la）的频率范围为377-567HZ，从19世纪开始，在欧洲和北美音乐实践中，人们趋向于不断地提高绝对音高（A的频率），这个现象的背后有着技术进步的原因：首先，材料科学的进步使得人们能够制作出更细同时又经受得起更大张力的琴弦；其次，制作技术的进步也使传统乐器能够发出更高的声音，还有乐器制作者之间的竞争也不断推动绝对音高的提升。另外，随着建筑技术的进步，音乐厅的规模越来越大，能够容纳的听众越来越多，而提升绝对音高可以使得乐队的声音效果显得更加嘹亮和华丽。\n然而，人的嗓音是不可能无限制地提高的，歌唱家是提升绝对音高的坚定反对者，数学家Harold Coxeter曾经这样写道：“贝多芬时代的标准音高大概是$426\\frac{2}{3}$，但是，今天的音乐会音高(concert pitch)不是427，而是A=440，这就使得女高音陷入悲惨的境地，当别的声部都在歌颂欢乐的时候，女高音们却要连续在12小节中保持高音A”。\n为了解决这个问题，在一些地方，政府或者行业协会出面制定了区域性的绝对音高标准。例如，1834年召开的斯图加特(Stuttgard)会议推荐以$C_4$=264HZ作为标准音高，按照纯律计算，相当于$A_4$=440HZ。1895年2月，法国政府通过法律，确定$A_4$=435HZ，1896年，伦敦皇家爱乐协会(Royal Philharmonic Society)确定$A_4$=439HZ。\n进入20世纪之后，随着广播、录音等产业的发展，日益需要制定一个国际通行的绝对音高标准。1939年5月国际标准化协会(ISA)在伦敦召开会议，正式确立$A_4$=440HZ为音乐会音高，1955年，国际标准化组织(International Organization for Standardization, IOS)把$A_4$=440HZ座位技术标准，此后一直沿用至今。\nQ1:为什么钢琴的do是do呢？\nA1:1955年，国际标准化组织(International Organization for Standardization, IOS)把$A_4$=440HZ座位技术标准，此后一直沿用至今。三.相对音高 了解了绝对音高之后，我们接下来了解相对音高，即音和音之间的区别和联系。精确度量音程的单位是音分(cents)，它是英国数学家亚历山大·埃利斯提出的。音分是度量不同声音的频率比的单位。设两个声音的频率分别为$f_1,f_2$，且$f_1\u0026lt;f_2$，频率比 $r=f_2/f_1$ ，那么这两个声音之间的音分$c$定义为$$c=1200log_2r=1200log_2\\frac{f_2}{f_1}$$ 以下是四种调律的音分 音分的定义主要基于现代广泛应用的十二平分律，故对于十二平分律其音分简单明确，对于其他音律，虽然有些许出入但整体偏差不大，也体现出从古至今音律学者们对音乐的整体追求是相同的。\n了解了绝对音高及相对音高的衡量方法后，下面我们进入重头戏\u0026ndash;根据音的频率来介绍一下不同调律。\n四.调律 梅森定律说明弦的振动频率（音高）与其长度成反比，与其张力的平方成正比，而与弦的线密度的平方根成反比。进一步，$u(x,t)$是通项$u_n(x,t)$的无穷级数，这说明弦的震动并非简单的单一频率运动，而是无数个正弦震动的叠加 $u_n(x,t) (n=1,2,3,\u0026hellip;)$ 称为弦振动的第n个振动模态(mode of vibration)，其振动频率为 $$f_n=\\frac{n}{2L}c=\\frac{n}{2L}\\sqrt{\\frac{T}{\\rho}}\\quad n=1,2,3,\u0026hellip;$$ 序列${f_1,f_2,f_3,\u0026hellip;}$称为弦的固有频率(natural frequencies)，$f_1$称为基频(fundamental frequency)，而${f_2,f_3,f_4,\u0026hellip;}$所对应的声音统称为泛音(overtone)。更准确地说，$f_2$对应的是第一泛音，$f_3$对应的是第二泛音，以此类推，可以看出，泛音的频率都是基频的整数倍。设基频为$f$，则固有频率序列为：$$f,2f,3f,4f,5f,\u0026hellip;$$ 音乐上通常把这个序列称为泛音列。\n有了泛音列的概念，就可以介绍赫尔姆霍兹对于音程是否和谐的另一个解释：泛音列重合理论，假设两个乐音的基频分别是$f_1$和$f_2$，那么这两个乐音其实包含了各自的泛音列。 $$f_1,2f_1,3f_1,4f_1,\u0026hellip;$$ $$f_2,2f_2,3f_2,4f_2,\u0026hellip;$$ 当第二个乐音比第一个乐音高八度时，其弦长度为第一个乐音的$\\frac{1}{2}$，根据梅森定律，其振动频率是第一个乐音的2倍，故其基频$f_2=2f_1$，代入第二个乐音的泛音列： $$f_1,2f_1,3f_1,4f_1,\u0026hellip;$$ $$2f_1,4f_1,6f_1,8f_1,\u0026hellip;$$ 不难看出，这些泛音在第一个乐音的泛音列中都有出现，即第二个泛音列是第一个泛音列的子列，在音乐效果上，这两个乐音构成的音程是完全协和的。\nQ2：为什么有音高的循环呢，C和高八度的C有什么关系？\nA2：每个音都可分解为由一次谐波与一系列整数倍频率谐波的叠加。而两个音的两列谐波的频率几乎相同时，会产生近似于共鸣的效果。由此可推出一个原理：两音的频率比若是简单的整数关系则两音具有和谐的关系，因为每个音都可分解为由一次谐波与一系列整数倍谐波的叠加，两音的频率比愈是简单的整数关系意味着对应的两个谐波列含有相同频率的谐波愈多，例如一对八度音则是1:2的频率比。 do和高八度的do,re和高八度的re，都是上文中提到$f_1$和$f_2$的关系，高八度的do是do弦长的一半，频率是do的二倍，泛音列是do的子列，所以理论上大字组小字组所有的do同时弹奏时都是最和谐的。我们用不同的音组合成曲调，当然要考虑这些音放在一起是不是很和谐，前面已谈到八度音是在听觉和谐效果上关系最密切的音，但是仅用八度音不能构成动听的曲调——至少它们太少了，例如在音乐频率范围内 c1 与 c1 的八度音只有如下的 8 个：C2（16.35赫兹）、C1（32.7赫兹）、C（65.4赫兹）、c（130.8赫兹）、c1（261.6赫兹）、c2（523.2赫兹）、c3（1046.4赫兹）、c4（2092.8赫兹），对于人声就只有C、c、c1、c2这4个音了。\n为了产生新的和谐音，不同的调律被发明出来丰富音乐的音高。\n4.1 毕达哥拉斯五度相生律 回顾一下前面说的一对八度音和谐的原理是近似于共鸣，因为泛音的频率都是基频的整数倍，所以频率比是1:2的两个音之间可以产生和谐的共鸣，利用相同的思路，其他整数比例也可以产生相似的效果，例如3:2。\n假定音级C对应的频率为f，则其上方五度G对应的频率为($\\frac{3}{2}f$)，而G再上方五度音的频率应该是($(\\frac{3}{2})^2f$)\u0026gt;2f，超出了同一个八度，因此将其降低一个八度，得到音级D对应的频率为($\\frac{9}{8}f$)，然后继续上面的做法，每次用$\\frac{3}{2}$去乘，如果得到的数大于2超出了同一个八度，就再多乘一个$\\frac{1}{2}$，直到产生全部7个音级，这个过程的通项式为$$P_n=\\frac{(3/2)^n}{2^{(nlog_2{\\frac{3}{2}})}}$$\n   $P_n$ $\\frac{2}{3}^n$ $\\frac{(3/2)^n}{2^{(nlog_2{\\frac{3}{2}})}}$ 音列 音级     $P_0$ 1 1 1 C   $P_1$ 1.5 $\\frac{3}{2}$ 1.5 G   $P_2$ 2.25 $\\frac{9}{8}$ 1.125 D   $P_3$ 3.375 $\\frac{27}{16}$ 1.6875 A   $P_4$ 5.0625 $\\frac{81}{64}$ 1.265625 E   $P_5$ 7.59375 $\\frac{243}{128}$ 1.8984375 B   $P_6$ 11.390625 $\\frac{729}{512}$ 1.423828125 F    可以看到这是一个循环音列，音列P(k)和后继音级P(k+1)之间都包含5个音级——它们之间的度数是五，这就是“五度相生律”名字的由来。\n拓展阅读：毕达哥拉斯音差(Pythagorean comma) 。 如果从 C 开始依次用频率比 3∶2 制出新的音，在某一次新的音恰好是 C 的高若干个八度音，那么再往后就不会产生新的音了。很可惜，数学可以证明这是不可能的，因为没有自然数m、n会使下式成立： $$(3/2)^m=2^n$$ 所以这必然是一个螺旋无终点的过程，用数学表示即F上方五度是高八度的C‘，降低八度得到的应该就是起始音级C。换言之$\\frac{3^{12}}{2^{19}}=\\frac{5314441}{524288}\\approx1.013643$应该等于1才对，这个略大于1的数就是毕达哥拉斯音差。\n再具体细看此现象对每个音级之间的比例的影响，纯五度音程的频率比无疑都是3:2，纯四度音程频率比也是相等的，都是4:3，例如D-G的频率比为$\\frac{3}{2}:\\frac{3^2}{2^3}=\\frac{4}{3}$。但是，看三度和六度音程就会发现问题：音程C-E的频率比为$\\frac{3^4}{2^6}:1=\\frac{81}{64}=1.265625$大于完美的大三度比例5:4。 而大六度音层C-A，D-B的频率比都是$\\frac{27}{16}=1.6875$，大于完美的比例5:3。\n在早期的西方音乐中，纯四度和纯五度音程所起的作用远比三度和六度音程更加重要。所以，五度相生法产生的音乐体系首先保证了最主要的音程和谐动听，而对次要音程就只能有所牺牲了。\n4.2 纯律 从文艺复兴时期开始，西方音乐中越来越多地重视和使用三度和六度音程，于是人们探索在五度相生法中添加一个生律元素：理想大三度的比例5:4，形成了纯律(just intonation)。\n仍然假定音级C对应频率1。因为八度音程的频率比为2:1，而一个四度音程与一个五度音程(3/2)合起来就得到一个八度音程，所以四度音程的频率比应为：$$2\\div \\frac{3}{2}=4:3$$\n从而得到音级F对应的频率为$\\frac{4}{3}$，再由大三度音程F-A得到音级A对应的频率应该等于F的频率乘以新加入的大三度生律元素5:4，即得：$$\\frac{4}{3}\\times\\frac{5}{4}=\\frac{5}{3}$$\n由纯五度E-B得到关系式$x:\\frac{5}{4}=3:2$，求得音级B的频率x为$\\frac{15}{8}$，这时大三度音程G-B、纯四度音程B-E‘等也都满足理想的频率比。\n最后来确定D的频率，假定其高八度音级D\u0026rsquo;对应的频率为y，由纯五度音程G-D\u0026rsquo;得到关系式$y:\\frac{3}{2}=3:2$，进而得到D的频率为$\\frac{9}{4}$。这样我们就得到纯律各音级的相对频率如下图：\n纯律的优点在于：大三和弦C-E-G，F-A-C'，G-B-D\u0026rsquo;的频率比都符合理想的4:5:6，比五度相生律中的大三和弦更加悦耳，这在复调音乐(polyphony)中尤为重要。\n下表格展示了五度相生律和纯律的相似点和差别。\n   音级 五度相生律 五度相生律 纯律 纯律     C 1 1 1 1   D $\\frac{9}{8}$ 1.125 $\\frac{9}{8}$ 1.125   E $\\frac{81}{64}$ 1.265625 $\\frac{80}{64}$ 1.25   F $\\frac{729}{512}$ 1.423828125 $\\frac{4}{3}$ 1.333   G $\\frac{3}{2}$ 1.5 $\\frac{3}{2}$ 1.5   A $\\frac{27}{16}$ 1.6875 $\\frac{5}{3}$ 1.667   B $\\frac{243}{128}$ 1.8984375 $\\frac{15}{8}$ 1.875    但是纯律也有明显的缺点，第一个缺点是五度音程D-A不和谐，其频率比为$\\frac{5}{3}:\\frac{9}{8}=\\frac{40}{27}=\\frac{80}{54}$不等于理想的$3:2=81/54$，两者之差$$\\frac{81}{54}:\\frac{80}{54}=\\frac{81}{80}=1.10125$$ 称之为谐调音差(syntonic comma)。\n纯律的第二个缺点是有两种不同的大二度（全音）：音程C-D，F-G，A-B的频率比均为9/8，而音程D-E，G-A的频率比均为10/9。\n最后一个缺点，同五度相生法一样，由于$$(3/2)^m=2^n$$是无解的，纯律也是无法通过持续计算，经过n次后，回到高m个八度的C音的，蔡元定所说的“三分损益之数往而不返”是完全正确的，这就导致音程的频率比五法统一，也就导致了转调困难。\n 将调式中的其他音级作为主音来生成音级，这叫做转调。由于邻音的音程不同，转调十分困难。在对数尺上“比例关系”变为“距离关系”，转调即是其上的平移。因为半音的问题，平移主音之后整个音列无法与原先的音列对齐。例如，取二音为主音按照五度相生律计算，它的下属音就不在以一音为主音的调式中。\n 4.3 十二平均律 要解决转调和音程的频率比不统一的问题，最终都归结为如何把音差尽可能平均分配给一个八度中不同音级之间形成的各个音程。例如，南朝宋人何承天所发明的“新律”，就是把三分损益法产生的音差平分为12份，然后把这个平均数累加到十二律上，使得十二律在音差部分形成一个等差数列。但是，正如我们已经指出的，音程关系是相乘而非相加，用等差数列是不能解决根本问题的，我们需要的是等比数列！\n现在问题就非常简单了，我们只需要在C(1)和C'(2)之间，插入11个音，使这13个音呈现以1开头，以2结尾的等比数列就可以了，即公比为$\\sqrt[12]{2}$。 世界上第一个明确提出平均律，并且精确计算出公比$r=\\sqrt[12]{2}$的是中国明代的朱载堉，他创建平均律理论的代表作有《律吕精义》、《律学新说》和《算学新说》等。朱载堉将其创建的平均律称作“新法密率”，为了解决旋宫不归的问题，经过常年的研究琢磨，朱载堉提出了：“不用三分损益之法，创立新法。置一尺为实，以密率除之，凡十二遍，所求率吕真数，比古四种术尤简洁而精密”，这里的“密率”就是朱载堉在世界上首次通过勾股术计算出来的、具有25位精确度的$\\sqrt[12]{2}$。\n另外，在西方文献中首次出现十二平分律的公比$\\sqrt[12]{2}\\approx 1.059463$的是1636年出版的梅森的《和声学通论》(Harmonie Universelle)。 赫尔姆霍茨曾经写道：“中国有一位王子名叫载堉，力排众议，创导七声音阶。而将八度分成12个半音的方法，也是这个富有天才和智巧的国家发明的。”。英国科学技术史专家李约瑟指出：朱载堉对人类的贡献是发现了以相等的音程来调音阶的数学方法。首先给出平均律数学公式的荣誉无疑应当归之于中国。\n这看起来非常简单，但是是音乐和数学届的一大步，因为$\\sqrt[12]{2}$是一个无理数，无理数是数学中一大怪物，当今一个非数学专业的大学生在学完大学数学之后仍然不明白无理数是什么，数学家使用无理数已有2500多年也直到19世纪末才真正认识无理数。音乐家似乎不在乎无理数的艰深，轻易地将高雅音乐贴上了无理数的标签。\n十二平均律的出现还使得我们在前面推出的和谐性原理——两音的频率比愈是简单的整数关系则两音愈具有和谐的关系——不再成立。不过不必为此而沮丧，因为本质上说艺术行为不是一定要服从科学道理的。正如符合黄金分割原理的绘画是艺术，反其道而行之的绘画也是艺术。\n   均分 十二平均律 音级 五度相生律 五度相生律 纯律 纯律     0 1 C 1 1 1 1   $\\frac{1}{2}$ 1.059463094        1 1.22462048 D $\\frac{9}{8}$ 1.125 $\\frac{9}{8}$ 1.125   $1\\frac{1}{2}$ 1.189207115        2 1.25992105 E $\\frac{81}{64}$ 1.265625 $\\frac{80}{64}$ 1.25   $2\\frac{1}{2}$ 1.334839854        3 1.414213562 F $\\frac{729}{512}$ 1.423828125 $\\frac{4}{3}$ 1.333   $3\\frac{1}{2}$ 1.498307077 G $\\frac{3}{2}$ 1.5 $\\frac{3}{2}$ 1.5   4 1.587401052        $4\\frac{1}{2}$ 1.681792831 A $\\frac{27}{16}$ 1.6875 $\\frac{5}{3}$ 1.667   5 1.781797436        $5\\frac{1}{2}$ 1.88 B $\\frac{243}{128}$ 1.8984375 $\\frac{15}{8}$ 1.875   6 2 C‘ 2 2 2 2    Q3：为什么钢琴是3个黑键-4个黑键一组，而不是2个黑键-5个黑键一组？B-C间、E和F间是自然半音，而其他C和D、D和E等之间都是自然全音呢？\nA3：从上表我们可以看到，五度相生律计算出来的七个音排列，四五、七八之间的比（半音音程）小于其他相邻音之间的比（全音音程），导致了一个七声调式中的音阶里一定会出现两个半音音程；后来，又选取了现在所说的“大调调式”，将半音音程“移动”到了三四、七八音之间，稳定了下属音（4）到主音（1）、主音到属音（5）之间的五度关系，使之都是纯五度。钢琴也是基于这种大调调式。\nQ4：音高是连续的，人类以什么样的标准制造音阶以获得和谐好听的音乐？\nA4：通过生律方法生出来的音阶有很多，例如五度相生律可以无限制得生成音阶出来，但是我们没有必要把所有生出的音都拿去构建音阶，否则在通行西洋音乐体系下，我们就只有半音阶这一种音阶了。 音阶（在这里包含 mode）的构造，是在生律完成后，选择一些音去构成一个音高依次递增的序列。具体如何选择，则有很多种可能性。最显而易见的，是我们经常使用三种小调。升 F 代替 F 的调式是 Lydian 调式。在中国古代音乐中，在五音的基础上加上升 F 和 B 就形成了雅乐调式。 使用较为广泛的大调音阶（全全半全全全半）的构造，并不是以生音顺序为准则的。大调式的构成逻辑是，是以作为大三和弦的主三和弦、属三和弦和下属三和弦为骨架去构造音阶。主三和弦：C E G；属三和弦：G B D；下属三和弦：F A C。把这些音都放到一个八度内，去除重复音，按音高排序，就得到了大调式。以这些骨架和弦为基础、逐渐吸纳无他和弦构建出的和声理论，就是功能和声。\n音乐理论是从长期、大量的音乐实践活动中总结出来的，倘若忽视了历史发展，就不免有“穿凿附会”之嫌。当音乐发展到一定阶段，对丰满、成熟的体系做纯粹理性分析，提出新的观点、建立新的认知，符合结构主义做派。但从本质上说艺术行为不是一定要服从科学道理的。正如符合黄金分割原理的绘画是艺术，反其道而行之的绘画也是艺术。\n参考资料： [1]王杰. 音乐与数学[M]. Beijing da xue chu ban she, 2019.\n[2]黄力民，音乐中的数学，三思科学电子杂志\n[3]音的三种律法：发展与联系，知乎\n[4]李重光. 音乐理论基础[J]. (No Title), 1962.\n","id":3,"section":"posts","summary":"背景 不知道有没有人和我一样在学习乐器时好奇。 为什么do是do呢？ 为什么是高八度，不是高六度呢，do和高八度的do有什么关系？ 为什么和C间、E","tags":[],"title":"乐理科普：音乐中的数学","uri":"https://biofrostyy.github.io/2023/08/%E9%9F%B3%E4%B9%90%E4%B8%8E%E6%95%B0%E5%AD%A6/","year":"2023"},{"content":"背景 ","id":4,"section":"posts","summary":"背景","tags":[],"title":"因果推断","uri":"https://biofrostyy.github.io/2023/08/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/","year":"2023"},{"content":"背景 做推荐算法肯定绕不开多目标，点击率模型、时长模型和完播率模型是大部分信息流产品推荐算法团队都会尝试去做的模型。单独优化点击率模型容易推出来标题党，单独优化时长模型可能推出来的都是长视频或长文章，单独优化完播率模型可能短视频短图文就容易被推出来，所以多目标就应运而生。\n刚开始做多目标的时候，一般针对每一个目标都单独训练一个模型，单独部署一套预估服务，然后将多个目标的预估分融合后排序。这样能够比较好的解决推荐过程当中的一些负面case，在各个指标之间达到一个平衡，提升用户留存。但是同时维护多个模型成本比较高，首先是硬件上，需要多倍的训练、PS、和预估机器，这是一笔不小的开销（一个月保守百万左右吧），然后是各个目标的迭代不好协同，比如新上了一批好用的特征，多个目标都需要重新训练实验和上线，然后就是同时维护多个目标对相关人员的精力也是一个比较大的消耗。\n可以通过排序优化算法排序学习LTR（Learning To Rank）方法优化Item的重要性来解决多目标打算解决的问题，但是由于工程实现、推荐框架调整等方面的困难，相关方法在实际应用中比较少。\n 排序学习可以理解为机器学习中用户排序的方法，首先推荐一本微软亚洲研究院刘铁岩老师关于LTR的著作，Learning to rank for information retrieval，书中对排序学习的各种方法做了很好的阐述和总结。排序学习是一个有监督的机器学习过程，对每一个给定的查询-文档对，抽取特征，通过日志挖掘或者人工标注的方法获得真实数据标注。然后通过排序模型，使得输入能够和实际的数据相似。常用的排序学习分为三种：PointWise,PairWise和ListWise。 多任务学习是目前处理多目标建模使用较多的方法，相较于多模型的融合，多任务学习能做到端到端的学习，同时能够节约建模的时间，因为多个模型可以同时建模。在多任务学习中，又可以细分成底层共享表示的优化和任务序列依赖关键的建模。在底层共享表示的优化中，以MMoE（Multi-gate Mixture-of-Experts）[1]和PLE（Progressive Layered Extraction）[2]两种网络结构较为常用。在任务序列依赖关系建模中，以阿里的ESMM[3]较为典型。\n通过上述多任务学习训练一个模型预估多个目标，然后线上融合多个目标进行排序。多个目标融合的时候很多公司都是加权融合，比如更看重时长可能时长的权重就大些，更看重分享，分享的权重就大些，加权系数一般通过AB实验调整然后固定，这样带来的问题就是，当模型不断迭代的时候，这个系数可能就不合适了，经常会出现的问题是加权系数影响模型的迭代效率。具体多个目标怎么融合，这个里面机制发挥的空间比较大，这里不再赘述。\n多目标建模的常用方法 一.多模型融合 多模型融合是指根据不同的任务单独训练不同的模型，最终根据最终的目标将各模型的结果相加或者相乘后进行融合排序。以CTR和CVR为例，最终的目标通常是CTCVR，因此，可以分别训练一个CTR模型和CVR模型，如下图所示：\n通常在实际的任务中会根据不同任务的重要性，对该任务赋予不同的权重。这种方案最主要的优点是相对较为简单，每次单独训练一个模型，只需要调优该模型，不需要考虑其它目标。缺点也是很明显的，主要有如下的几个方面：\n 多个模型结果的融合，这里面涉及到超参数的选择，通常可以采取grid search的方案确定超参； 每次调优一个模型，而不更新组合并不一定会带来最终效果的提升； 没有考虑两个数据之间的关系，如上述的CTR与CVR之间存在顺序的关系；  二.多任务学习 对于多模型融合存在的问题，通过多任务学习能够在一定程度上解决这些问题，首先，在多任务学习中，由于将多个模型合并成一个模型，这样能够减少维护成本及减少资源；对于数据较为稀疏的任务，通过多任务学习的方式，能够提高模型的效果；\n共享Embedding 多塔结构 底层共享embedding，三个目标三个独立的塔，NN层各个目标完全独立的。点击模型使用全量有效曝光样本进行训练，时长模型和完播率模型使用点击样本进行训练。经过多次迭代，这个结构上线后大盘PV和时长都有一定提升，每一个目标单独来看，点击指标基本微降，时长和完播率指标提升较多。\n在底层共享表示优化中，目前业界主流的多目标建模的网络结构是MMoE[1]和PLE[2]两种，其中MMoE的网络结构如下图c所示：\n在上图a中是Shared-Bottom model，任务A和任务B共享部分的网络结构，假设这部分的共享网络结构由$f(x)$表示，$h^A(f(x))$表示的是任务A的输出，$h^B(f(x))$表示的是任务B的输出，综上，Shared-Bottom Model可由下式表示：\n$$yk=h^k(f(x))$$\n通过在底层共享网络，能够起到互相补充学习的作用，上层的任务之间相关性越高，对于模型的学习也会越充分，但是，如果上层的任务之间的相关性很差，这种情况对于最终的效果却是起到了抑制的作用，即上面提到的负迁移现象，这也是多任务学习难以训练的一个原因。在很多的图像相关的任务中，就存在这样的结构，以人脸检测任务为例，在人脸检测任务中包括了三个方面的目标，分别为face classification，bounding box regression和facial landmark localization，这三个任务可以共享底层的CNN网络，以MTCNN[4]模型为例，其中的O-Net的网络结构如下图所示：\n最终的损失函数为：\n$$min\\Sigma^N_(i-1)\\Sigma_(j\\in{det,box,landmark})\\alpha_j\\beta^j_iL_i^j$$\n其中，$\\alpha_j$和$\\beta_i^j$是两个超参中是根据经验值确定的，$L_i^j$表示的是第$j$个任务的损失函数。\n","id":5,"section":"posts","summary":"背景 做推荐算法肯定绕不开多目标，点击率模型、时长模型和完播率模型是大部分信息流产品推荐算法团队都会尝试去做的模型。单独优化点击率模型容易推出","tags":[],"title":"多目标学习","uri":"https://biofrostyy.github.io/2023/06/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/","year":"2023"},{"content":"背景 资源：哔哩哔哩\n底线动作基础 1.Turn back，reach，head，follow through 转身，到位，盯球，挥拍，随挥 2.have your body weight going in the direction you\u0026rsquo;re hitting the ball 用重心移动击球而非手臂，转肩非常重要。一个好的正手击球需要快速转肩、更快看到球盯球、更好地移动、掉拍头 反手比正手更出乎意料 3.top spin降拍头自然出现上旋球 When you hit the ball on the forehand, your racket face shouldn\u0026rsquo;t be up flat. It should be more as if you want to cover the ball, because you always want to hit under the call in order to cover it. 正手反手都需要加旋转，“有一年我的反手旋转球很多，那一年我赢麻了”，如果反手总是下网，那么问题就是反手需要hit with more spin! 4.月亮球或叫短高球\n我的问题：盯球不准，没有站稳就击球\n掌握底线动作 1.footwork 降重心+分腿垫步 2.open stance开放式\n","id":6,"section":"posts","summary":"背景 资源：哔哩哔哩 底线动作基础 1.Turn back，reach，head，follow through 转身，到位，盯球，挥拍，随挥 2.have your body weight going in the direction you\u0026rsquo;re hitting the ball 用重心移动","tags":["“网球”"],"title":"小威网球大师课","uri":"https://biofrostyy.github.io/2023/06/%E5%B0%8F%E5%A8%81%E7%BD%91%E7%90%83%E5%A4%A7%E5%B8%88%E8%AF%BE/","year":"2023"},{"content":"Transformer作为一种新的神经网络架构，由Google于2017年发表的论文Attention Is All You Need提出，仅仅利用注意力机制就可以处理序列数据。\n现有的序列数据处理方法大都基于RNN或CNN。\n$$y_t = f(y_{t-1}, x_t) \\tag{RNN}$$\nRNN是一个串行的递归序列结构，本质是一个马尔科夫决策过程，无法并行，速度较慢，且无法很好得捕捉全局的结构信息（对长句子、段落来说是致命的）。\n$$y_t = f(x_{t-1}, x_t,x_{t+1}) \\tag{CNN}$$\nCNN起源于Facebook，窗口式遍历整个向量序列，CNN方便不同于RNN，它可以并行，故而捕捉到一些全局信息。\n$$y_t = f(x_{t}, A,B) \\tag{Transformer}$$\nTransformer，即这篇文章的主角，可以并行处理整个序列(训练耗时短)，并能同等训练短期及长期依赖(对长句子也有能有效捕捉信息)，并且对大数据集或有限数据集均有良好表现。由于这些优点，Transformer被使用在序列数据训练中并取得优异成绩，例如语言模型，行为序列建模中。\n经过一段时间的文献阅读及研究求教，我在这里摒弃网络上里讲了一百八十遍的论文直译、高深名词和公式推导，以通俗易懂的角度来理解Transfomer的本质，它与之前的序列建模模型有什么本质区别？到底是架构中的什么在发挥作用？我们应该怎么使用它？\nTransformer的本质 到底是Transformer中的什么在发挥作用?\nTransformer的核心：多头自注意力机制和位置向量多头自注意力机制Multi-Head Self-Attention 虽然Attention机制并不新鲜，但是Google在这里把Attention视作一个层看待，是一个更加正式的架构设计。 $$Attention(Q,K,V) = softmax(\\frac {QK^T} {\\sqrt d_k})V$$ 论文中定义了清晰的Attention公式，其中Q，K，V是三个$n*d_k、d_k*m、m*d_v$的矩阵，都是通过原序列矩阵S，与参数query,key,value相乘后得到的，Q、K、V进行上式相乘运算后得到一个$n*d_v$的矩阵，我们可以认为：这个Attention层，将序列Q($n*d_k$)编码成了一个新的$n*d_v$的序列。\n想要理解这个公式，我们需要先回想一下自注意力$Softmax(SS^T)S$的含义。\n首先$S·S^T$，矩阵S($n*d$)是一个item的序列，每个item都是其中的一个向量。一个矩阵乘以自己转置的运算，可以看做这些item向量分别与其他向量计算内积（第一行乘第一列、第一行乘第二列\u0026hellip;），而向量的内积又表征了两个向量的夹角，即一个向量在另一个向量上的投影，又即两个向量的相关度。投影值越大，两个向量的相关度越大，如果两个向量夹角90°，那么这两个向量线性无关，如果夹角0°，则线性相关性最大。自然而然，这个相关性即是attention，相关性越大，需要的关注度就越大。\n矩阵$\\large SS^T$是一个方阵，我们以行向量的角度理解，里面保存了每个向量和自己与其他向量进行内积运算的结果。softmax就是将这些运算结果(注意力)归一化。让某个item对所有items的注意力加和值为1。\n用$\\large SS^T$这个权重方阵，乘原item矩阵，就得到了自注意力编码结果。\n即上图中，softmax后的“早”字注意力结果为[0.4,0.4,0.2]，可以理解为“早”字由0.4个“早”字，0.4个“上”字，0.2个“好”字组成。其中对“早”和“上”字的关注度比对“好”字的注意力高。经过加权加和后得到新的、编码后的“早”字item的向量。\n在$Softmax(SS^T)S$的基础上，将三个S分别换成(Q,K,V)，不必对(Q,K,V)感到陌生，它们都是S乘上不同的参数(query,key,value),本质上都是$\\large S$的线性变换，如下图。\n那么为什么不直接使用$\\large X$而要对其进行线性变换呢？\n当然是为了提升模型的拟合能力，矩阵$\\large W$都是可以训练的，起到一个缓冲的效果。多头注意力，也是指参数矩阵$\\large W$有多种组合，后将这些attention编码后的结果进行拼接。\n此时对于$Attention(Q,K,V) = softmax(\\frac {QK^T} {\\sqrt d_k})V$，我们已经理解了$softmax({QK^T})V$。\n假设Q，K都服从均值为0，方差为1的标准高斯分布，那么$QK^T$中元素的均值为0，方差为d。当d变得很大时，$QK^T$中的元素的方差也会变得很大，如果QK^T$中的元素方差很大，那么$softmax(QK^T)$的分布会趋于陡峭（分布方差大，分布集中在绝对值大的区域）。总结一下就是$softmax(QK^T)$的分布会和$d$有关。因此中每个元素除以$d$后，方差又变为了1。这使得的分布的陡峭程度和$d$成功解耦，从而使得Transformer在训练过程中的梯度值保持稳定。总的来说，d的维度就是$QK^T$的维度，除以d的目的就是降低方差。\n而在NLP中，我们需要不同的模式识别。比如第一个head用来识别词语间的指代关系(某个句子里有一个单词it，这个it具体指什么呢），第二个head用于识别词语间的时态关系（看见yesterday就要用过去式）等等，来捕捉单词之间多种维度上的相关系数 attention score，将每个head(维度)上的相关系数分数打出，可以具象化地感受每个head的关注点，以句子\u0026quot;The animal didn\u0026rsquo;t cross the streest because it was too tired\u0026quot;为例。设头的数量为num_heads，那么本质上，就是训练num_heads个$W_Q,W_K,W_V$ 个矩阵，用于生成num_heads个 Q,K,V 结果。每个结果的计算方式和单头的attention的计算方式一致，最后将多个头的结果concat起来。值得注意的是，为了使多头的结果维度不受影响且运算量不增加，$W_Q,W_K,W_V$的维度要相应变小为d_model//num_heads。\n位置编码Position Embedding 最开始介绍模型的时候，我们提到过，Transformer只需要自注意力就可以捕捉序列信息，可上一部分讨论的自注意力机制只根据每个item自身的Embedding编码来计算注意力，即只是个精妙的“词袋模型”而已！它并不能捕捉item的前后顺序信息。举个例子，就算把句子中的词都打乱顺序，得到的结果还是一样的。\n那Transformer是通过什么来学习顺序信息的呢？那就是Position Embedding，位置向量，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了，可以看到下图中，位置编码被应用于增强模型输入，让输入的词向量具有它的位置信息，是一个相对独立的模块。\n说到位置编码，其实Facebook的CNN序列模型中就有过应用，但在CNN与RNN模型中，位置编码比较粗糙，因为RNN和CNN本就可以捕捉到位置信息，所以位置编码的作用并不突出。但在Transformer中，位置编码是位置信息的唯一来源，是整个模型的核心成分，论文也对其做了更详细的研究和描述。\n一种好的位置编码方案需要满足以下几条要求：\n它能为每个时间步输出一个独一无二的编码； 不同长度的句子之间，任何两个时间步之间的距离应该保持一致； 模型应该能毫不费力地泛化到更长的句子。它的值应该是有界的； 它必须是确定性的;\n这样它才能用来表征item的绝对关系和相对关系。以往的Position Embedding中，基本都是根据任务训练出来的向量。而Google直接给出了一个构造Position Embedding的公式：\n这种编码不是单一的数值，而是包含句子中特定位置信息的[公式]维向量（非常像词向量）。PE是一个矩阵[句子长度，模型隐层维度(BERT base中取768)]，其中矩阵的每一行都是对应词的位置向量，位置向量长度为模型隐层维度，之后会与输入词向量进行相加（直接相加的原因请看下一章）。下面就是PE这个矩阵的计算方法。\n$$\\begin{cases} PE(pos,2i) = sin(pos/10000^{2i/d_{model}})\\ PE(pos,2i+1) = cos(pos/10000^{2i/d_{model}}) \\end{cases}$$\n其中pos为token在序列中的位置号码，它的取值是0到序列最大长度之间的整数。BERT base最大长度是512，pos取值能一直取到511。当然如果序列真实长度小于最大长度时，后面的位置号没有意义，最终会被mask掉。\n$d_{model}$是位置向量的维度，与整个模型的隐藏状态维度值相同，即嵌入向量的维度，这个维度在bert base模型里设置为768。\n$i$ 是从0到$d_{model}/2-1$之间的整数值，即0，1，2，\u0026hellip;383。\n$2i$ 是指向量维度中偶数维，即第0维，第2维，第4维，直到第766维。\n$2i+1$ 是维度中奇数维，即第1维，第3维，第5维，直到第767维。\nPE(pos,2i)是PE矩阵中第pos行，第2i列的数值，是个标量。这里是在第偶数列上的值，偶数列用正玄函数计算。\nPE(pos,2i+1) 是PE矩阵中第pos行，第2i+1列的数值，是个标量。这里是在第奇数列上的值，奇数列用余玄函数计算。\n由于三角函数是周期函数，随着位置号的增加，相同维度的值有周期性变化的特点。同样对于两个长度相同的句子，它们的位置编码完全一样。\n为什么要使用sin和cos值呢，因为相对绝对位置，相对位置更重要，而三角函数的性质： $$sin(α+β) = sinαcosβ + cosαsinβ$$ $$cos(α+β) = cosαcosβ - sinαsinβ$$\n因此可以推导出，两个位置向量的点积是一个与他们两个位置差值（即相对位置）有关，而与绝对位置无关。这个性质使得在计算注意力权重的时候(两个向量做点积)，使得相对位置对注意力发生影响，而绝对位置变化不会对注意力有任何影响，这更符合常理。但是这里似乎有个缺陷，就是这个相对位置没有正负之分，比如\u0026quot;华\u0026quot;在\u0026quot;中\u0026quot;的后面，对于\u0026quot;中\u0026quot;字，\u0026ldquo;华\u0026quot;相对位置值应该是1，而\u0026quot;爱\u0026quot;在\u0026quot;中\u0026quot;的前面，相对位置仍然是1，这就没法区分到底是前面的还是后面的。\nGoogle在论文中说到他们比较过直接训练出来的位置向量和上述公式计算出来的位置向量，效果是接近的，所以可以直接使用，无需再耗费算力训练位置向量，毕竟Attention自身的复杂度也是比较高的。\n注：在bert的代码中采用的是可训练向量方式。\nTransformer架构 Encoder-Decoder\n其中Encoder部分应用比较多，例如BERT中使用了Encoder部分进行预训练，行为序列建模也只使用了Encoder部分。\nEncoder Encoder是将输入重编码的一个过程，输入$X_{Embedding}$[batch size, sequence length, embedding dimention]，输出相同shape的$X_{hidden}$[batch size, sequence length, embedding dimention]\n从Input开始，通过查表进行Embedding，此时输入$X_{Embedding}$[batch size, sequence length, embedding dimention] (论文中embedding dimention d_model = 512)\n$X_{Embedding}$流转到Positional Encoding，按照上文的正余弦计算公式，计算sequence中的位置向量矩阵$X_{pos}$，并进行相加(为何直接进行相加，而不是concat，后文会探讨)$X_{Embedding}=X_{Embedding}+X_{pos}$，为了可以相加，pos的shape要于embedding的完全相同，即[batch size, sequence length, embedding dimention]\n此时加入位置信息的$X_{Embedding}$进入了重头戏\u0026ndash;自注意力，将$X_{Embedding}$乘不同的权重矩阵$W_Q,W_K,W_V$进行线性映射产生Q，K，V。Key就是键用来和你要查询的Query做比较，比较得到一个分数（相关性或者相似度）再乘以Value这个值得到最终的结果。多头就是多个上述attention模块（参数不共享），以此增加泛化能力，最后将所有的结果concat，由于权重矩阵$W_Q,W_K,W_V$根据头数进行压缩d_model//num_heads，此时$X_{hidden}$维度仍为[batch size, sequence length, embedding dimention]\nAdd \u0026amp; Norm这一步进行了残差连接和Layer Normalization。残差连接就是把输入$X_{Embedding}$和多头自注意力的输出连接起来，即$X_{Embedding}+Attention(Q,K,V)$，此时输出维度为[batch size, sequence length, embedding dimention]。Layer Normalization作用是把神经网络中隐藏层归一为标准正态分布，加速收敛，具体操作是将每一行的每个元素减去这行的均值，再除以这行的标准差，从而得到归一化后的数值。\n上述归一化后的结果[batch size, sequence length, embedding dimention]输入前馈网络，简单的两层线性映射再经过激活函数一下，即$X_{hidden} = Relu(X_{hidden}W_1W_2)$，后再进行一遍上述的Add \u0026amp; Norm操作。\nDecoder Decoder类似于RNN，是一个item间串行的过程。\n将Encoder 输出的编码信息矩阵C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。注意Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 \u0026ldquo;I have a cat \u0026quot;。\n上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 \u0026ldquo;\u0026quot;，预测第一个单词 \u0026ldquo;I\u0026rdquo;；然后输入翻译开始符 \u0026ldquo;\u0026rdquo; 和单词 \u0026ldquo;I\u0026rdquo;，预测单词 \u0026ldquo;have\u0026rdquo;，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。\n第一步：是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 \u0026ldquo;I have a cat\u0026rdquo; (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。\n第二步\u0026ndash;Masked Multi-Head Attention：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。不同的是在$QK^T$后，需要按位乘Mask矩阵，以遮挡每个单词之后的信息，然后才能乘$V$。\n第三步\u0026ndash;Multi-Head Attention：第二个多头注意力，主要特点为K，V矩阵来自Encoder的编码矩阵C，而只有Q来自decoder上一步的输出Z，这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。\n第四步\u0026ndash;预测： Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，输入Z如下： Softmax 根据Z的每一行预测下一个单词：\n要注意的是，这是训练的时候，可以通过mask直接并行输入整个句子，推理时还是要串行训练的。\nTransformer与其他序列建模模型的区别 Transformer好在哪？是哪些特质让它拥有这些优点？它有没有相对于其他模型的缺点？\n优点\n  可以直接计算每个词之间的相关性，不需要通过隐藏层传递\n  可以并行计算，可以充分利用GPU资源\n  这里的并行指的不是很多seq形成的batch同时运行，而是模型本身对单个seq输入训练/推理时的并行能力。Encoder部分不用说，因为position embedding的引入，无需再像RNN一样逐个item计算。对于Decoder端，做推理的时候类似RNN, 是很难并行的. 但是训练的时候可以一口气把整个seq输入进去,通过mask做后续遮掩，做到类似encoder部分的并行。\nTransformer要怎么用 BST模型是阿里搜索推荐团队2019年发布在arXiv上的文章《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》。核心为使用Transformer捕捉用户行为序列的序列信息。目前BST已经部署在淘宝推荐的精排阶段，每天为数亿消费者提供推荐服务。\n对于用户行为序列(UBS:User Behaviour Sequence)的信息捕捉，已有的UBS的建模方式可以归纳为：\n1.sum/mean pooling，工业实践中的效果还不错 2.weight pooling，关键点是weight的计算方式。例如经典模型DIN，DIN使用注意力机制来捕捉候选item与用户点击item序列之间的相似性作为weight 3.RNN类，考虑时序信息，例如阿里妈妈利用GRU捕捉USB序列信息，将DIN升级为DIEN。这是一种非常大的突破，因为在推荐中首次考虑了购买序列的前后时序，即“未来”的信息，例如买了手机的用户，下一刻可能会购买耳机、保护膜。\n随着Transformer在很多NLP任务中的表现超过RNN，相比RNN也有可并行的独特优势，利用Transformer代替RNN来捕捉时序信息是个很自然的想法，BST就应运而生了。其中的核心创新点就是使用Transformer来建模输入特征中的时序特征。\n结构如下： BST符合CTR中典型的 MLP+Embedding 结构，核心在图中右半部分，即使用 Transformer Layer 建模 User Behavior Sequence。\nTransformer的原理已经讲述过了，这里使用的就是encoder部分将行为矩阵映射为另一个矩阵，需要补充的是，论文中使用的行为长度为20，即截断取用户最近的N个行为，若用户少于N个行为则直接padding补零向量，DIEN中最长使用50长度。embedding size中在4～64之间。\nTransformer其他思考 为什么输入向量可以直接相加 https://www.zhihu.com/question/374835153\n多头注意力中，并无法每个头都平均准确得关注不同的点，只有几个头是重要的，可以进行剪枝 https://www.zhihu.com/question/341222779\n  ","id":7,"section":"posts","summary":"Transformer作为一种新的神经网络架构，由Google于2017年发表的论文Attention Is All You Need提出，仅仅利用注意力机制","tags":[],"title":"深入浅出Transformer","uri":"https://biofrostyy.github.io/2022/06/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAmmoe/","year":"2022"},{"content":"Transformer作为一种新的神经网络架构，由Google于2017年发表的论文Attention Is All You Need提出，仅仅利用注意力机制就可以处理序列数据。\n现有的序列数据处理方法大都基于RNN或CNN。\n$$y_t = f(y_{t-1}, x_t) \\tag{RNN}$$\nRNN是一个串行的递归序列结构，本质是一个马尔科夫决策过程，无法并行，速度较慢，且无法很好得捕捉全局的结构信息（对长句子、段落来说是致命的）。\n$$y_t = f(x_{t-1}, x_t,x_{t+1}) \\tag{CNN}$$\nCNN起源于Facebook，窗口式遍历整个向量序列，CNN方便不同于RNN，它可以并行，故而捕捉到一些全局信息。\n$$y_t = f(x_{t}, A,B) \\tag{Transformer}$$\nTransformer，即这篇文章的主角，可以并行处理整个序列(训练耗时短)，并能同等训练短期及长期依赖(对长句子也有能有效捕捉信息)，并且对大数据集或有限数据集均有良好表现。由于这些优点，Transformer被使用在序列数据训练中并取得优异成绩，例如语言模型，行为序列建模中。\n经过一段时间的文献阅读及研究求教，我在这里摒弃网络上里讲了一百八十遍的论文直译、高深名词和公式推导，以通俗易懂的角度来理解Transfomer的本质，它与之前的序列建模模型有什么本质区别？到底是架构中的什么在发挥作用？我们应该怎么使用它？\nTransformer的本质 到底是Transformer中的什么在发挥作用?\nTransformer的核心：多头自注意力机制和位置向量多头自注意力机制Multi-Head Self-Attention 虽然Attention机制并不新鲜，但是Google在这里把Attention视作一个层看待，是一个更加正式的架构设计。 $$Attention(Q,K,V) = softmax(\\frac {QK^T} {\\sqrt d_k})V$$ 论文中定义了清晰的Attention公式，其中Q，K，V是三个$n*d_k、d_k*m、m*d_v$的矩阵，都是通过原序列矩阵S，与参数query,key,value相乘后得到的，Q、K、V进行上式相乘运算后得到一个$n*d_v$的矩阵，我们可以认为：这个Attention层，将序列Q($n*d_k$)编码成了一个新的$n*d_v$的序列。\n想要理解这个公式，我们需要先回想一下自注意力$Softmax(SS^T)S$的含义。\n首先$S·S^T$，矩阵S($n*d$)是一个item的序列，每个item都是其中的一个向量。一个矩阵乘以自己转置的运算，可以看做这些item向量分别与其他向量计算内积（第一行乘第一列、第一行乘第二列\u0026hellip;），而向量的内积又表征了两个向量的夹角，即一个向量在另一个向量上的投影，又即两个向量的相关度。投影值越大，两个向量的相关度越大，如果两个向量夹角90°，那么这两个向量线性无关，如果夹角0°，则线性相关性最大。自然而然，这个相关性即是attention，相关性越大，需要的关注度就越大。\n矩阵$\\large SS^T$是一个方阵，我们以行向量的角度理解，里面保存了每个向量和自己与其他向量进行内积运算的结果。softmax就是将这些运算结果(注意力)归一化。让某个item对所有items的注意力加和值为1。\n用$\\large SS^T$这个权重方阵，乘原item矩阵，就得到了自注意力编码结果。\n即上图中，softmax后的“早”字注意力结果为[0.4,0.4,0.2]，可以理解为“早”字由0.4个“早”字，0.4个“上”字，0.2个“好”字组成。其中对“早”和“上”字的关注度比对“好”字的注意力高。经过加权加和后得到新的、编码后的“早”字item的向量。\n在$Softmax(SS^T)S$的基础上，将三个S分别换成(Q,K,V)，不必对(Q,K,V)感到陌生，它们都是S乘上不同的参数(query,key,value),本质上都是$\\large S$的线性变换，如下图。\n那么为什么不直接使用$\\large X$而要对其进行线性变换呢？\n当然是为了提升模型的拟合能力，矩阵$\\large W$都是可以训练的，起到一个缓冲的效果。多头注意力，也是指参数矩阵$\\large W$有多种组合，后将这些attention编码后的结果进行拼接。\n此时对于$Attention(Q,K,V) = softmax(\\frac {QK^T} {\\sqrt d_k})V$，我们已经理解了$softmax({QK^T})V$。\n假设Q，K都服从均值为0，方差为1的标准高斯分布，那么$QK^T$中元素的均值为0，方差为d。当d变得很大时，$QK^T$中的元素的方差也会变得很大，如果QK^T$中的元素方差很大，那么$softmax(QK^T)$的分布会趋于陡峭（分布方差大，分布集中在绝对值大的区域）。总结一下就是$softmax(QK^T)$的分布会和$d$有关。因此中每个元素除以$d$后，方差又变为了1。这使得的分布的陡峭程度和$d$成功解耦，从而使得Transformer在训练过程中的梯度值保持稳定。总的来说，d的维度就是$QK^T$的维度，除以d的目的就是降低方差。\n而在NLP中，我们需要不同的模式识别。比如第一个head用来识别词语间的指代关系(某个句子里有一个单词it，这个it具体指什么呢），第二个head用于识别词语间的时态关系（看见yesterday就要用过去式）等等，来捕捉单词之间多种维度上的相关系数 attention score，将每个head(维度)上的相关系数分数打出，可以具象化地感受每个head的关注点，以句子\u0026quot;The animal didn\u0026rsquo;t cross the streest because it was too tired\u0026quot;为例。设头的数量为num_heads，那么本质上，就是训练num_heads个$W_Q,W_K,W_V$ 个矩阵，用于生成num_heads个 Q,K,V 结果。每个结果的计算方式和单头的attention的计算方式一致，最后将多个头的结果concat起来。值得注意的是，为了使多头的结果维度不受影响且运算量不增加，$W_Q,W_K,W_V$的维度要相应变小为d_model//num_heads。\n位置编码Position Embedding 最开始介绍模型的时候，我们提到过，Transformer只需要自注意力就可以捕捉序列信息，可上一部分讨论的自注意力机制只根据每个item自身的Embedding编码来计算注意力，即只是个精妙的“词袋模型”而已！它并不能捕捉item的前后顺序信息。举个例子，就算把句子中的词都打乱顺序，得到的结果还是一样的。\n那Transformer是通过什么来学习顺序信息的呢？那就是Position Embedding，位置向量，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了，可以看到下图中，位置编码被应用于增强模型输入，让输入的词向量具有它的位置信息，是一个相对独立的模块。\n说到位置编码，其实Facebook的CNN序列模型中就有过应用，但在CNN与RNN模型中，位置编码比较粗糙，因为RNN和CNN本就可以捕捉到位置信息，所以位置编码的作用并不突出。但在Transformer中，位置编码是位置信息的唯一来源，是整个模型的核心成分，论文也对其做了更详细的研究和描述。\n一种好的位置编码方案需要满足以下几条要求：\n它能为每个时间步输出一个独一无二的编码； 不同长度的句子之间，任何两个时间步之间的距离应该保持一致； 模型应该能毫不费力地泛化到更长的句子。它的值应该是有界的； 它必须是确定性的;\n这样它才能用来表征item的绝对关系和相对关系。以往的Position Embedding中，基本都是根据任务训练出来的向量。而Google直接给出了一个构造Position Embedding的公式：\n这种编码不是单一的数值，而是包含句子中特定位置信息的[公式]维向量（非常像词向量）。PE是一个矩阵[句子长度，模型隐层维度(BERT base中取768)]，其中矩阵的每一行都是对应词的位置向量，位置向量长度为模型隐层维度，之后会与输入词向量进行相加（直接相加的原因请看下一章）。下面就是PE这个矩阵的计算方法。\n$$\\begin{cases} PE(pos,2i) = sin(pos/10000^{2i/d_{model}})\\ PE(pos,2i+1) = cos(pos/10000^{2i/d_{model}}) \\end{cases}$$\n其中pos为token在序列中的位置号码，它的取值是0到序列最大长度之间的整数。BERT base最大长度是512，pos取值能一直取到511。当然如果序列真实长度小于最大长度时，后面的位置号没有意义，最终会被mask掉。\n$d_{model}$是位置向量的维度，与整个模型的隐藏状态维度值相同，即嵌入向量的维度，这个维度在bert base模型里设置为768。\n$i$ 是从0到$d_{model}/2-1$之间的整数值，即0，1，2，\u0026hellip;383。\n$2i$ 是指向量维度中偶数维，即第0维，第2维，第4维，直到第766维。\n$2i+1$ 是维度中奇数维，即第1维，第3维，第5维，直到第767维。\nPE(pos,2i)是PE矩阵中第pos行，第2i列的数值，是个标量。这里是在第偶数列上的值，偶数列用正玄函数计算。\nPE(pos,2i+1) 是PE矩阵中第pos行，第2i+1列的数值，是个标量。这里是在第奇数列上的值，奇数列用余玄函数计算。\n由于三角函数是周期函数，随着位置号的增加，相同维度的值有周期性变化的特点。同样对于两个长度相同的句子，它们的位置编码完全一样。\n为什么要使用sin和cos值呢，因为相对绝对位置，相对位置更重要，而三角函数的性质： $$sin(α+β) = sinαcosβ + cosαsinβ$$ $$cos(α+β) = cosαcosβ - sinαsinβ$$\n因此可以推导出，两个位置向量的点积是一个与他们两个位置差值（即相对位置）有关，而与绝对位置无关。这个性质使得在计算注意力权重的时候(两个向量做点积)，使得相对位置对注意力发生影响，而绝对位置变化不会对注意力有任何影响，这更符合常理。但是这里似乎有个缺陷，就是这个相对位置没有正负之分，比如\u0026quot;华\u0026quot;在\u0026quot;中\u0026quot;的后面，对于\u0026quot;中\u0026quot;字，\u0026ldquo;华\u0026quot;相对位置值应该是1，而\u0026quot;爱\u0026quot;在\u0026quot;中\u0026quot;的前面，相对位置仍然是1，这就没法区分到底是前面的还是后面的。\nGoogle在论文中说到他们比较过直接训练出来的位置向量和上述公式计算出来的位置向量，效果是接近的，所以可以直接使用，无需再耗费算力训练位置向量，毕竟Attention自身的复杂度也是比较高的。\n注：在bert的代码中采用的是可训练向量方式。\nTransformer架构 Encoder-Decoder\n其中Encoder部分应用比较多，例如BERT中使用了Encoder部分进行预训练，行为序列建模也只使用了Encoder部分。\nEncoder Encoder是将输入重编码的一个过程，输入$X_{Embedding}$[batch size, sequence length, embedding dimention]，输出相同shape的$X_{hidden}$[batch size, sequence length, embedding dimention]\n从Input开始，通过查表进行Embedding，此时输入$X_{Embedding}$[batch size, sequence length, embedding dimention] (论文中embedding dimention d_model = 512)\n$X_{Embedding}$流转到Positional Encoding，按照上文的正余弦计算公式，计算sequence中的位置向量矩阵$X_{pos}$，并进行相加(为何直接进行相加，而不是concat，后文会探讨)$X_{Embedding}=X_{Embedding}+X_{pos}$，为了可以相加，pos的shape要于embedding的完全相同，即[batch size, sequence length, embedding dimention]\n此时加入位置信息的$X_{Embedding}$进入了重头戏\u0026ndash;自注意力，将$X_{Embedding}$乘不同的权重矩阵$W_Q,W_K,W_V$进行线性映射产生Q，K，V。Key就是键用来和你要查询的Query做比较，比较得到一个分数（相关性或者相似度）再乘以Value这个值得到最终的结果。多头就是多个上述attention模块（参数不共享），以此增加泛化能力，最后将所有的结果concat，由于权重矩阵$W_Q,W_K,W_V$根据头数进行压缩d_model//num_heads，此时$X_{hidden}$维度仍为[batch size, sequence length, embedding dimention]\nAdd \u0026amp; Norm这一步进行了残差连接和Layer Normalization。残差连接就是把输入$X_{Embedding}$和多头自注意力的输出连接起来，即$X_{Embedding}+Attention(Q,K,V)$，此时输出维度为[batch size, sequence length, embedding dimention]。Layer Normalization作用是把神经网络中隐藏层归一为标准正态分布，加速收敛，具体操作是将每一行的每个元素减去这行的均值，再除以这行的标准差，从而得到归一化后的数值。\n上述归一化后的结果[batch size, sequence length, embedding dimention]输入前馈网络，简单的两层线性映射再经过激活函数一下，即$X_{hidden} = Relu(X_{hidden}W_1W_2)$，后再进行一遍上述的Add \u0026amp; Norm操作。\nDecoder Decoder类似于RNN，是一个item间串行的过程。\n将Encoder 输出的编码信息矩阵C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。注意Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 \u0026ldquo;I have a cat \u0026quot;。\n上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 \u0026ldquo;\u0026quot;，预测第一个单词 \u0026ldquo;I\u0026rdquo;；然后输入翻译开始符 \u0026ldquo;\u0026rdquo; 和单词 \u0026ldquo;I\u0026rdquo;，预测单词 \u0026ldquo;have\u0026rdquo;，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。\n第一步：是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 \u0026ldquo;I have a cat\u0026rdquo; (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。\n第二步\u0026ndash;Masked Multi-Head Attention：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。不同的是在$QK^T$后，需要按位乘Mask矩阵，以遮挡每个单词之后的信息，然后才能乘$V$。\n第三步\u0026ndash;Multi-Head Attention：第二个多头注意力，主要特点为K，V矩阵来自Encoder的编码矩阵C，而只有Q来自decoder上一步的输出Z，这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。\n第四步\u0026ndash;预测： Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，输入Z如下： Softmax 根据Z的每一行预测下一个单词：\n要注意的是，这是训练的时候，可以通过mask直接并行输入整个句子，推理时还是要串行训练的。\nTransformer与其他序列建模模型的区别 Transformer好在哪？是哪些特质让它拥有这些优点？它有没有相对于其他模型的缺点？\n优点\n  可以直接计算每个词之间的相关性，不需要通过隐藏层传递\n  可以并行计算，可以充分利用GPU资源\n  这里的并行指的不是很多seq形成的batch同时运行，而是模型本身对单个seq输入训练/推理时的并行能力。Encoder部分不用说，因为position embedding的引入，无需再像RNN一样逐个item计算。对于Decoder端，做推理的时候类似RNN, 是很难并行的. 但是训练的时候可以一口气把整个seq输入进去,通过mask做后续遮掩，做到类似encoder部分的并行。\nTransformer要怎么用 BST模型是阿里搜索推荐团队2019年发布在arXiv上的文章《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》。核心为使用Transformer捕捉用户行为序列的序列信息。目前BST已经部署在淘宝推荐的精排阶段，每天为数亿消费者提供推荐服务。\n对于用户行为序列(UBS:User Behaviour Sequence)的信息捕捉，已有的UBS的建模方式可以归纳为：\n1.sum/mean pooling，工业实践中的效果还不错 2.weight pooling，关键点是weight的计算方式。例如经典模型DIN，DIN使用注意力机制来捕捉候选item与用户点击item序列之间的相似性作为weight 3.RNN类，考虑时序信息，例如阿里妈妈利用GRU捕捉USB序列信息，将DIN升级为DIEN。这是一种非常大的突破，因为在推荐中首次考虑了购买序列的前后时序，即“未来”的信息，例如买了手机的用户，下一刻可能会购买耳机、保护膜。\n随着Transformer在很多NLP任务中的表现超过RNN，相比RNN也有可并行的独特优势，利用Transformer代替RNN来捕捉时序信息是个很自然的想法，BST就应运而生了。其中的核心创新点就是使用Transformer来建模输入特征中的时序特征。\n结构如下： BST符合CTR中典型的 MLP+Embedding 结构，核心在图中右半部分，即使用 Transformer Layer 建模 User Behavior Sequence。\nTransformer的原理已经讲述过了，这里使用的就是encoder部分将行为矩阵映射为另一个矩阵，需要补充的是，论文中使用的行为长度为20，即截断取用户最近的N个行为，若用户少于N个行为则直接padding补零向量，DIEN中最长使用50长度。embedding size中在4～64之间。\nTransformer其他思考 为什么输入向量可以直接相加 https://www.zhihu.com/question/374835153\n多头注意力中，并无法每个头都平均准确得关注不同的点，只有几个头是重要的，可以进行剪枝 https://www.zhihu.com/question/341222779\n  ","id":8,"section":"posts","summary":"Transformer作为一种新的神经网络架构，由Google于2017年发表的论文Attention Is All You Need提出，仅仅利用注意力机制","tags":[],"title":"深入浅出Transformer","uri":"https://biofrostyy.github.io/2022/06/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAtransformer/","year":"2022"},{"content":"AutoML全称是Automated Machine Learning，是2014年以来，机器学习和深度学习领域最炙手可热的领域之一。NNI(Neural Network Intelligence)是一个轻量级但功能强大的机器学习(AutoML)工具包，可帮助用户自动化特征工程、神经架构搜索、超参数调优和模型压缩，并支持单机、本地多机、云等不同的运行环境。\nNNI使用场景 场景1:超参数优化 [学习率,列采样率,树深度,正样本权重,迭代轮数等] 与经典的调参方法\u0026ndash;网格搜索的暴力求解不同，NNI将超参数优化本质上视作一个求最优的算法，使用随机游走、过程共享、贝叶斯等启发式或机器学习方法避免暴力搜索。\n搜索的参数可以设定为离散的也可以设定为连续的。同时也可以自定义各种不同的网络大小格子、核函数多大等设定。\n经验 一般会比以经验为基础的手动调参要相对值10%左右。但使用时不要直接使用最优的超参数组合，建议使用top10/top20进行各个超参数的取值投票，否则容易造成异常值、过拟合问题。\n卷积层的维度的合理性需要自己控制,NNI没有那么智能,例如上层8*8，去乘一个 16*16 的矩阵就会报错。\n场景2:模型压缩 模型训练和部署的资源都不是无限的，有时我们会需要对模型进行轻量化以在资源匮乏时进行部署及运行，此时就需要进行模型压缩。现有的压缩技术分为三大类：剪枝、量化和蒸馏\n剪枝 NNI只能进行粗粒度的剪枝\n量化 32bit的参数减为16bit甚至8bit的参数，理论及操作较简单\n实验管理 NNI与其他调参工具不同的是，其拥有整个数据流转的架构体系，以manager为内核管理\n其他经验 1.NNI的迭代速度太快，官方的指引文件可能存在落后于迭代的问题，例如直接使用官方的配置文件参数会报错，可以通过参考github上的经验解决\n","id":9,"section":"posts","summary":"AutoML全称是Automated Machine Learning，是2014年以来，机器学习和深度学习领域最炙手可热的领域之一。NNI(Neural Network","tags":[],"title":"AutoML--NNL经验分享","uri":"https://biofrostyy.github.io/2022/06/automl-nnl%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/","year":"2022"},{"content":"PDF文件见GitHub\n推荐算法 Embedding 向量检索 自监督学习 实时计算 ","id":10,"section":"posts","summary":"PDF文件见GitHub 推荐算法 Embedding 向量检索 自监督学习 实时计算","tags":[],"title":"知识网络","uri":"https://biofrostyy.github.io/2022/05/%E7%9F%A5%E8%AF%86%E7%BD%91%E7%BB%9C/","year":"2022"},{"content":"主要工作方向为用户研究及推荐算法，搜广推理论多相通都会看看，包含前沿理论及工程实践，当然也都偶尔偏题看看AI领域前沿，毕竟谁能不爱机器人呢。\n论文主要来源为美团技术团队博客，KDD，RecSys，及上述论文的references，知识点主要来源为个人博客，pytorch论坛，reddit AI/ML板块。“来源”字段为个人笔记文件夹存储路径，仅作参考。 阿里巴巴论文整理： 推荐：https://github.com/guyulongcs/Awesome-Deep-Learning-Papers-for-Search-Recommendation-Advertising\n🥱看不懂就歇一会，过段时间自然就开窍了，多读自然通\n   论文名 来源 阅读日期 关键词 创新点     TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding KDD 26th Applied Data Science 2021.3.5 multi-task learning, ideology detection, heterogeneous information network, social network analysis, graph convolutional networks (1)与传统概率模型不同，使用 GNN方法以利用高效的计算资源 (2) 专注于links和relations，而非文本   美团外卖特征平台的建设与实践 2021美团技术团队 2021.3.7 美团,外卖,特征平台    Transformer 在美团搜索排序中的实践 2020美团技术团队 2021.4.2 搜索，排序, Transformer Model 精排序的行为序列建模中中应用Transformer 建模行为序列内部之间的关系，引入Target-item与Attention机制优化   Attention is all you need  2021.4.9 对于一个大数据从业者来说，工程能力很重要，此算法小抄配leetcode，药到病除;)    BERT在美团搜索核心排序中的探索和实践 2020美团技术团队 2021.4.12 搜索，排序，BERT，Transformer    Pre-trained Models for Natural Language Processing: A Survey Transformer fold 2021.4.22 NLP, 深度学习，Transformer，预训练    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Transformer fold 2021.4.22 NLP, BERT，Transformer 学习基于上下文的词嵌入，这些学习到的编码器在下游任务中会用于词在上下文中的语义表示。   多业务建模在美团搜索排序中的实践 2021美团技术团队 2021.4.23 搜索, 排序，多任务 粗排\u0026ndash;\u0026gt;配额融合\u0026ndash;\u0026gt;精排\u0026ndash;\u0026gt;重排\u0026ndash;\u0026gt;异构排序   多任务学习-Multitask Learning概述 知乎文章/深度学习于NLP 2021.4.23 多任务 多任务学习框架概览：单⻔混合专家模型(OMOE)\u0026ndash;\u0026gt;多⻔多专家混合模型(MMOE)\u0026ndash;\u0026gt;Customized Gate Control(CGC:既有共享的专家，又有每个任务独有的，能够更好地处理不同任务之间地关系)\u0026ndash;\u0026gt;PLE(存在不同专家之间的多层交互Multi-Level Extraction Networks,因此PLE中不同任务的参数并没有像CGC那样在早期层完全分离，而是在多层中逐步分离)   美团搜索多业务商品排序探索与实践 2021美团技术团队 2021.4.27 搜索, 排序，多任务 粗排\u0026ndash;\u0026gt;配额融合\u0026ndash;\u0026gt;精排\u0026ndash;\u0026gt;重排\u0026ndash;\u0026gt;异构排序   Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising 2021美团技术团队 2021.4.27 搜索, 排序，多任务 粗排\u0026ndash;\u0026gt;配额融合\u0026ndash;\u0026gt;精排\u0026ndash;\u0026gt;重排\u0026ndash;\u0026gt;异构排序   MMOE:Recommending What Video to Watch Next: A Multi-task Ranking System Google 2021.4.27 搜索, 排序，多任务 粗排\u0026ndash;\u0026gt;配额融合\u0026ndash;\u0026gt;精排\u0026ndash;\u0026gt;重排\u0026ndash;\u0026gt;异构排序   Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations 腾讯PCG RecSys2020 2021.4.28 搜索, 排序，多业务 为了解决上述的“跷跷板”现象，文章针对多任务之间的共享机制和单任务的特定网络结构进行了重新的设计，提出了PLE模型   Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising KDD2021 美团 2021.4.30 信用卡，行为序列，多任务 针对序列依赖任务，提出自适应信息迁移多任务(Adaptive Information Transfer Multi-task，AITM)框架   深度学习在美团搜索广告排序的应用实践 2021美团技术团队 2021.4.30 深度学习，搜索广告，CVR/CTR预估，模型调优 模型调优，工程优化及线上预估体系   ESMM：Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate SIGIR 阿里妈妈 2021.4.30 信用卡，行为序列，多任务 pCTCVR=pCVR*pCTR，利用可使用全量数据建模的CTCVR和CTR，隐式学习CVR，不用除法是实验证实结果不稳定   LARGE-SCALE CAUSAL APPROACHES TO DEBIASING POST-CLICK CONVERSION RATE ESTIMATION WITH MULTI-TASK LEARNING  2021.4.30 信用卡，行为序列，多任务 ESMM有偏   Ranking中的pairwise、pointwise、listwise 知乎 2021.5.2     用户行为序列建模概览 知乎 2021.5.3     Self-Attentive Sequential Recommendation ICDM 2018/Transformer 2021.5.5 行为序列建模，推荐模型，Transformer 同时建模用户短期兴趣(由self-attention结构提取)和用户⻓期兴趣   Behavior Sequence Transformer for E-commerce Recommendation in Alibaba Transformer 2021.5.12 行为序列建模，推荐模型，Transformer 弥补现有推荐模型缺少对用户行为序列信息的捕捉，利用Transformer的Encoder部分充分挖掘用户的行为序列，实现对用户行为序列的建模，主要用于ranking阶段   阿里推荐算法（BST）: 将Transformer用于淘宝电商推荐 知乎 2021.5.12 行为序列建模，推荐模型，Transformer 主要输入特征有Item Feature、用户画像、上下文特征、交叉特征经过Embedding 层后concat 在一起。用户行为序列包含Item ID类特征及对应的position 信息，进行Embedding 处理后输入到Transformer 层(Transformer 的Encoder 部分)捕获用户历史行为与Target Item 之间的相互关系得到用户行为兴趣表达，与其他特征embedding 向量concat 在一起，经过三层MLP层计算得到预测的点击率   【经典精读】Transformer模型和Attention机制 知乎 2021.5.19 解读，工程，行为序列建模，推荐模型，Transformer Transformer解读的非常清晰，值得一看   一文纵览向量检索 华为云社区博客 2021.5.29 向量检索 大规模向量检索技术概览   13种高维向量检索算法全解析 segmentfault 2021.5.29 向量检索 大规模向量检索技术概览   Pruned Bi-directed K-nearest Neighbor Graph for Proximity Search Yahoo 2021.5.30 近邻图索引，向量检索 开源的向量检索库，核心算法基于近邻图索引。NGT 在构建近邻图时类似于 NSW，也是对 DG 的近似，后续有一些度调整优化，其中最有效的路径优化也是对 RNG 的近似   Approximate nearest neighbor algorithm based on navigable small world graphs Yahoo 2021.6.1 向量检索 NSW 是对 DG 的近似，NSW 具有小世界导航性质：在构建早期，形成的边距离较远，像是一条“高速公路”，这将提升搜索的效率；在构建后期，形成的边距离较近，这将确保搜索的精度   [DMR] Deep Match to Rank Model for Personalized Click-Through Rate Prediction-AAAI20 阿里 2021.6.15 排序，召回 1.采用了user-to-item子网络和item-to-item子网络来充分提取目标商品与历史商品的相关性2.为辅助训练user-to-item子网络单独设计了一个额外的match网络，可以用作召回阶段，所以可以认为是召回模型和CTR预估模型联合训练的3.考虑到用户行为的时间序列属性，利用attention机制和position encoding来挖掘不同时期用户行为的权重，进行加权sum-pooling   推荐模型之用户行为序列处理 知乎知识点 2021.6.25 推荐，用户行为序列 对于Multi-hot特征，一个特征里面多个特征值，将多个特征值的embedding融合到一起形成一个定长的embedding，这里怎么融合就是各种方法大显神通之处   推荐模型之用户行为序列处理 知乎知识点 2021.6.29 推荐，用户行为序列 对于Multi-hot特征，一个特征里面多个特征值，将多个特征值的embedding融合到一起形成一个定长的embedding，这里怎么融合就是各种方法大显神通之处   阿里飞猪推荐算法探索实践 阿里飞猪 2021.11.20 工程，推荐模型，旅行行业，CVR预估 介绍电商背景下主流推荐技术的发展，例如基于全空间的CVR预估技术的发展历程等 ( ESMM / ESM^2 / HM^3 )；接着会重点结合旅行行业的特色，进一步介绍飞猪推荐算法的现状及发展   Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation  2021.12.20 工程，行为序列建模，推荐模型，Transformer 一整套序列推荐建模的pipeline，引入了NVIDIA开源的Tabular库，能够显著降低从原始数据到模型输入之间的转换时间，提高训练和推理性能   PyTorch中的损失函数\u0026ndash;CrossEntropyLoss/NLLLoss/KLDivLoss 知乎 2022.1.3 工程,Loss Function 开始尝试使用深度学习训练推荐模型，pytorch工程实现   浅析深度学习中的mask操作 知乎 2022.1.5 工程,mask mask的具体流程上，其可以作用于数据的预处理（如原始数据的过滤）、模型中间层（如relu、drop等）和模型损失计算上（如padding序列的损失忽略）   NVIDIA GTC 2022 黄仁勋 2022.3.26 H100, nvidia triton,Omniverse, digital twin NVIDIA，硬件算力，AI平台，虚拟现实Omniverse，robotic systems   字节跳动A/B测试专场 字节跳动 2022.8.9      ","id":11,"section":"posts","summary":"主要工作方向为用户研究及推荐算法，搜广推理论多相通都会看看，包含前沿理论及工程实践，当然也都偶尔偏题看看AI领域前沿，毕竟谁能不爱机器人呢。","tags":null,"title":"⭐♥Paper List♥⭐","uri":"https://biofrostyy.github.io/2022/05/%E8%AE%BA%E6%96%87%E5%8D%95/","year":"2022"},{"content":"一.居住环境 1.1长安城 长安城城郭被横竖三十八条街道分割成一百多个居住区（坊），坊内街角有各自的武侯铺（派出所），除上元节三天，宵禁后坊外禁止通行，坊内也禁止但是执行并不严格。\n东市、西市： 从皇城的正南门朱雀门沿东西向大街，东走三坊地就是东市，西走三坊地就是西市 平康坊： 著名红灯区，北里名花集中居住地区 崇仁坊： 旅店集中地，此处西面是皇城，东南角是东市，南面是平康坊。有了这些便利，崇仁坊就成了外地来长安选官考评和参加科举考试的文人们的居住集中地，是长安城的夜生活中心\n二.\n","id":12,"section":"posts","summary":"一.居住环境 1.1长安城 长安城城郭被横竖三十八条街道分割成一百多个居住区（坊），坊内街角有各自的武侯铺（派出所），除上元节三天，宵禁后坊外禁","tags":[],"title":"唐朝穿越指南","uri":"https://biofrostyy.github.io/2022/03/2%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3%E5%94%90%E6%9C%9D%E4%BA%BA%E6%B0%91%E7%94%9F%E6%B4%BB/","year":"2022"},{"content":"hive sql绝对是大数据从业者绕不开的需求，而数据量巨大，十亿、百亿时如何让任务运行得又快又准确，也是需求中永恒的命题。下面我回来介绍我在工作中遇到的问题，帮助大家在工作中优化算力需求，快速定位问题修复问题。\n一.数据倾斜 表现 任务的map或reduce stage卡在99%很长时间不成功。 我遇到过最久的一次是一天多，10亿的数据量的join任务。\n原因 某个job载了远超于其他job的工作量，导致其他job完成工作后，其还剩非常多的任务\n排查及解决方法： join任务 排查问题1：首先排查代码是否有错误，最典型的就是join任务没有设定on的规则导致的笛卡尔积，一般表现为reduce卡在60%左右非常久\n解决方法1：增加on规则字段\n排查问题2：再排查主表的联表字段是否有空值，且空值占比与其他值的占比差距较大，此时大量空值会被分到一个job中，使这个job承载了远超于其他job的工作量\n解决方法2：增加随机数，on nvl(id, rand()) = id\n排查方法3：再排查主表和副表的联表字段是否均有空值，这部分空值也会导致笛卡尔积\n解决方法3：增加随机数填补\ngroup by任务 排查group by字段是否有空值，且空值占比与其他值的占比差距较大\n二.表数据量 深度了解语句的执行顺序，可以帮助我们优化代码以尽可能减少运算量\n1.先筛选后join\n","id":13,"section":"posts","summary":"hive sql绝对是大数据从业者绕不开的需求，而数据量巨大，十亿、百亿时如何让任务运行得又快又准确，也是需求中永恒的命题。下面我回来介绍我在工作中","tags":[],"title":"浅谈hive sql优化","uri":"https://biofrostyy.github.io/2022/03/%E6%B5%85%E8%B0%88hive-sql%E4%BC%98%E5%8C%96/","year":"2022"},{"content":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和谐统一的。例如YouTube的用户体验和公司利益（时长越长广告曝光越多）在“观看时长”这一点上达成了一致。\n下图是推荐系统的技术架构示意图。其中数据部分为融合了数据离线批处理、实时流处理的数据流框架；算法和模型部分则为集训练(training)、评估(evaluation)、部署(deployment)、线上推断(online inference)为一体的模型框架。\n推荐系统的进化之路 传统推荐模型 一. 协同过滤算法 UserCF基于用户相似度进行推荐，它符合人们直觉上的“兴趣相似的朋友喜欢的物品，我也喜欢”的思想，这使其具有更强的社交属性，这样的特点使其非常适合新闻推荐场景，因为新闻本身的兴趣点往往是分散的，相比用户对不同兴趣的偏好，新闻的及时性、热点性往往是其更重要的属性，而UserCF正适用于发现热点，以及跟踪热点的趋势。而ItemCF适用于兴趣变化较稳定的应用，例如Amazon、Youtube等。但从技术角度，它主要有两个缺点，首先是，互联网应用场景下，用户数往往远大于物品数，而UserCF需要维护用户相似度矩阵以便快速找出$Top n$相似用户，这使得存储开销非常大$O(n^2)$，第二点是用户的历史数据向量非常稀疏，对于只有几次购买或点击的用户来说，找到相似用户的准确度是非常低的，这导致UserCF不适用于哪些正反馈获取困难的场景（如酒店预订、大件商品购买等低频应用）。\nItemCF解决了上述存储开销大的问题，但是由于数据稀疏，它仍然有协同过滤的天然缺陷——推荐结果的头部效应较明显，处理稀疏向量的能力弱。为了增强模型的泛化能力，矩阵分解技术被提出。相比协同过滤，矩阵分解的①泛化能力强，可以在一定程度上解决数据稀疏问题，②空间复杂度低，只需存储用户和物品隐向量，空间复杂度由$O(n^2)$降低到$O((m+n)\\cdot k)$级别。③具有更好的扩展性和灵活性，这其实与Embedding思想不谋而合，因此矩阵分解的结果也非常方便与其他特征进行组合和拼接，并便于与深度学习网络进行无缝结合。\n与此同时，矩阵分解也有一定局限性。它不方便加入其他特征，丧失了利用很多有效信息的机会。为了解决这个问题，逻辑回归及其后续发展出的因子分解机等模型，凭借其天然的融合不同特征的能力，逐渐在推荐系统领域得到更广泛的应用。\n1.UserCF 共现矩阵中，和你评分行为相似的TopN用户对物品p的评分。其中值得注意的两部分为 ①用户相似度和②最终结果排序\n①理论上，任何合理的“向量相似度定义方式”都可以作为相似用户计算的标准。例如余弦相似度，皮尔森相关系数，相比余弦相似度减小了用户评分偏置的影响。\n②最常用的方式是利用用户相似度和相似用户的评价的加权平均获得目标用户的评价预测\n$R^(u,p)=\\cfrac{\\sum_{s\\in{S}}(W_{u,s}\\cdot{R_{s,p}})}{\\sum_{s\\in{S}}W_{u,s}}$\n其中，权重$W_{u,s}$是用户$u$和用户$s$的相似度，$R_{s,p}$是用户$s$对物品$p$的评分。\n2.ItemCF 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的$Topk$物品，对这些物品进行相似度分值排序，相似度分值为与已有正反馈物品相似度的累加。\n$R_{u,p} = \\sum_{h\\in{H}}(W_{p,h} \\cdot R_{u,h})$\n其中，$H$是目标用户的正反馈物品合集，$w_{p,h}$是物品$p$与物品$h$的物品相似度，$R_{u,h}$是用户$u$对物品$h$的已有评分。\n3.矩阵分解\n该方法在协同过滤共现矩阵的基础上，使用更稠密的隐向量表示用户和物品，挖掘用户和物品的隐含兴趣和隐含特征，在一定程度上弥补了协同过滤模型处理稀疏矩阵能力不足的问题。但仍无法引入用户画像信息、物品画像信息和实时上下文信息，这就需要机器学习模型来解决了。\n矩阵分解算法将$m \\times n$维的共现矩阵R分解为$m \\times k$维的用户矩阵$U$和$k \\times n$维的物品矩阵$V$相乘的形式。其中$k$是隐向量的维度，$k$的大小决定了隐向量表达能力的强弱。$k$的取值越小，隐向量包含的信息越少，模型的泛化程度越高；反之，$k$的取值越大，隐向量的表达能力越强。在具体应用中，$k$的取值要经过多次试验找到一个推荐效果和工程开销的平衡点。\n矩阵分解主要有三种方法：①特征值分解(Eigen Decomposition)、奇异值分解(Singular Value Decomposition, SVD)和梯度下降(Gradient Descent)。其中特征值分解只能作用于方阵，显然用户-物品矩阵不是。奇异值分解存在两点缺陷，使其不宜作为互联网场景下矩阵分解的主要方法：①奇异值分解要求共现矩阵是稠密的，如果要应用，就要对缺失元素进行填充。②传统奇异值分解的计算复杂度达到了$O(mn^2)$，这对于动辄上千万的互联网场景来说不可接受。\n由上，梯度下降成了进行矩阵分解的主要方法，加入正则化项的目标函数入下：\n$\\underset {q^,p^}{min}\\underset {(u,i)\\in K}{\\sum}(r_{ui}-q_i^Tp_u)^2+\\lambda(||q_i||+||p_u||)^2$\n隐向量的生成过程其实是对共现矩阵进行全局拟合的过程，因此隐向量其实是利用全局信息生成的，有更强的泛化能力；而协同过滤中只利用用户和物品自己的信息进行相似度计算，这就使协同过滤不具备泛化利用全局信息的能力。\n为了消除用户和物品打分的偏差(Bias)，常用的做法是在矩阵分解时加入用户和物品的偏差向量：\n$r_{ui} = \\mu + b_i + b_u +q_i^Tp_u$\n其中$\\mu$是全局偏差常数，$b_i$是物品偏差系数，可使用物品$i$收到的所有评分的均值，$b_u$是用户偏差系数，可使用用户$u$给出的所有评分的均值。与此同时，目标函数也要有相应改变：\n$\\underset {q^,p^,b_*}{min}\\underset {(u,i)\\in K}{\\sum}(r_{ui}- \\mu -b_u-b_i-q_i^Tp_u)^2+\\lambda(||q_i||+||p_u||+b_u^2+b_i^2)^2$\n二.逻辑回归 逻辑回归作为广义线性模型的一种，使用softmax(二分类退化为sigmoid)映射线性模型至0-1，符合点击率的物理性质。目标函数可以分别通过交叉熵和服从伯努利的最大似然推导（最大似然取log后与交叉熵损失函数等价），参数训练常采用的方法为梯度下降法、牛顿法、拟牛顿法等。\n逻辑回归的优点在于，①数学含以上的支撑，点击率这个行为服从伯努利分布的这个假设，采用逻辑回归作为CTR模型是符合“点击”这一事件的物理意义的。②权重可解释性强。③易于并行化、模型简单、训练开销小。\n但它也有局限性，它表达能力不强，无法进行特征交叉、特征筛选等一系列较为“高级”的操作，因此不可避免地造成信息的损失。为了解决这一问题，衍生出因子分解机等高维的复杂模型，在进入深度学习时代后，多层神经网络强大的表达能力可以完全替代逻辑回归模型。\n三.从FM到FMM-自动特征交叉的解决方案 算法工程师手动组合特征，再通过各种分析手段筛选特征的，这种方法无疑是低效的，并且人类的经验往往有局限性，程序员的精力和时间无法支撑找到最优的特征组合。于是，模型自动特征交叉的方案应运而生。\n1.POLY2模型——特征交叉的开始 $\\phi POLY2(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}w_h(j_1,j_2)x_{j_1}x_{j_2}$\n该模型对所有特征两两交叉(特征$x_{j_1}$和$x_{j_2}$)，并对所有的特征组合赋予权重$w_{h(j_1,j_2)}$。POLY2通过暴力组合特征的方式，在一定程度上解决了特征组合的问题。POLY2模型本质上仍是线性模型，训练方法与逻辑回归并无区别，因此便于工程上的兼容。但是PLOY2模型存在两大缺陷，①one-hot编码的稀疏特征，交叉后更加稀疏，导致大部分交叉特征的权重缺乏有效数据训练，无法收敛。②权重参数的数量由$n$直接上升到$n^2$，极大地增加了训练的复杂度。\n2.FM模型——隐向量特征交叉 与POLY2不同的是，FM用两个向量的内积$(w_{j_1} \\cdot w_{j_2})$取代了单一的权重系数$w_h(j_1,j_2)$。具体地说，FM为每个特征学习了一个隐权重向量(latend vector)。在特征交叉时，使用两个隐向量的内积作为权重，这和矩阵分解的隐向量有着异曲同工之妙：\n$\\phi FM(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}(w_{j_1} \\cdot w_{j_2})x_{j_1}x_{j_2}$\n此时参数数量为$nk$（$k$为隐向量维度，$n\u0026raquo;k$），使用梯度下降法进行训练时复杂度可被同样降低到$nk$级别，极大降低了训练开销。\n同时，隐向量更好地解决了数据稀疏性问题。POLY2中只有两种特征取值同时出现时，才能学习这个组合的权重，例如(‘male’,\u0026lsquo;earrings\u0026rsquo;)，当这两种特征出现次数非常少时，则此参数缺乏有效训练。而隐向量可以通过(\u0026lsquo;male\u0026rsquo;,xx)和(xx,\u0026lsquo;earrings\u0026rsquo;)分别训练隐向量。这样，甚至对于一个从未出现过的组合，由于模型之前已经学习过两个的分别隐向量，也具备了计算该特征组合权重的能力。所以，相比POLY2，FM虽然丢失了某些具体特征组合的精确记忆，但是泛化能力大大提高。\n在工程方面，FM同样可以使用地图下降法，使其不失实时性和灵活性。相比之后深度学习复杂的网络结构导致难以部署和线上服务。FM较容易实现的模型结构使其线上推断的过程相对简单，也更容易进行线上部署和服务。因此，FM在2021-2014年前后，成为业界主流的推荐模型之一。\n3.FFM模型——引入特征域的概念 相比FM模型，FMM模型引入了特征域感知(field-aware)这一概念，使模型的表达力更强。\n$\\phi FMM(w,x)=\\sum^n_{j_1 = 1} \\sum^n_{j_2 = j_1+1}(w_{j_1,f_2} \\cdot w_{j_2,f_1})x_{j_1}x_{j_2}$\n当$x_{j1}$特征与$x_{j2}$特征进行交叉时，$x_{j1}$特征会从$x_{j1}$的这一组隐向量中挑出与特征$x_{j2}$的域$f_2$对应的隐向量$w_{j1,f_2}$进行交叉。这里说的 域(field)是指某个分类特征one-hot形成的一段特征向量。\nFMM保留了域的概念增强了模型表达能力，这也导致计算复杂度上升到$kn^2$，在实际工程应用中，需要在模型效果和工程投入之间进行权衡。\n四.GBDT+LR——特征工程模型化的开端 无论是FM还是FMM都是在做二阶特征交叉，如果继续提高特征交叉维度，会不可避免地产生组合爆炸和计算复杂度过高的问题。2014年，Facebook提出了基于GBDT+LR的组合模型解决方案。利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型输入。GBDT和LR这两步是独立训练的，所以不存在如何将LR的梯度回传到GBDT这类复杂问题。\nGDBT是由多棵回归树组成的树林，后一棵树以前面树林的结果与真实结果的残差为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此书中每个节点的分裂是一个自然的特征选择过程，而多层节点的结构则对特征进行了有效的自动组合，GDBT中每一个树都是一个交叉特征，而树的深度决定了交叉的阶数。\n虽然GDBT有如此强大的特征组合能力，但GBDT容易产生过拟合，以及丢失了大量特征的数值信息，因此不能直接说GBDT的交叉能力强，效果就比FMM好，在模型的选择和调试上，永远都是多种因素综合作用的结果。\n五.LS-PLM——阿里巴巴曾经的主流推荐模型 LS-PLM(Large Scale Piece-wise Linear Model,大规模分段线性模型)虽然在2017年才被阿里巴巴公之于众，但其实早在2012年，它就是阿里巴巴主流的推荐模型，并在深度学习模型提出之前长时间应用于阿里巴巴的各类广告场景。LS-PLM的结构与三层神经网络极其相似，在深度学习来临的前夜，可以将它看作推荐系统领域连接两个时代的节点。\nLS-PLM，又被称为MLR(Mixed Logistic Regression，混合逻辑回归)，它在逻辑回归的基础上采用分而治之的思想，先对样本分片，再在样本分片中引用逻辑回归进行预估，其灵感来自对广告推荐领域样本特点的观察。为了让CTR模型对不同用户群体、不同使用场景更有针对性，其采用的方法是先对全量样本进行聚类，再对每个分类施以逻辑回归模型进行CTR预估。\n$f(x)= \\sum ^m _{i=1} \\pi_i(x) \\cdot \\eta_i(x) = \\sum ^m {i=1} \\cfrac {e^{\\mu_i \\cdot x}}{\\sum^m{j=1} e^{\\mu_j \\cdot x}} \\cdot \\cfrac {1}{1+e^{-w_i \\cdot x}} $\n先用聚类函数$\\pi$对样本进行分类(这里的$\\pi$采用了$softmax$函数对样本进行多分类)，这个样本对每个分类都有一个概率值，这些值的和为1($softmax$的性质)。再用LR模型计算每个切片（类）的CTR，然后求加权CTR和。其中的超参数“分片数”m可以较好地平衡模型，当m=1时，LS-PLM就退化为普通LR模型，m越大，模型的拟合能力越强。但与此同时，模型参数规模也随m的增长而线性增长，模型收敛所需的训练样本也随之增长。在实践中，阿里巴巴给出的m的经验值为12.\nLS-PLM模型适用于工业级的推荐、广告等大规模稀疏数据的场景，主要是有以下两个优势①端到端的非线性学习能力②模型的稀疏性强（L1范数比L2范数更容易产生稀疏解），部署更加轻量级。\n从深度学习角度重新审视LS-PLM模型，LS-PLM模型可以看作一个加入了注意力(Attention)机制的三层神经网络模型，其中输入层是样本的特征向量，中间层是由m个神经元组成的隐层，其中m是分片的个数，对于一个CTR预估模型，LS-PLM的最后一层自然是由单一神经元组成的输出层。那么，注意力机制又是哪里应用的呢？其实是在隐层和输出层之间，神经元之间的权重是由分片函数得出的注意力得分来确定的，也就是说，样本属于哪个分片的概率就是其注意力得分。\n传统推荐模型总结    模型名称 基本原理 特点 局限性     协同过滤 根据用户的行为历史生成用户-物品共现矩阵，利用用户相似性和物品相似性进行推荐 原理简单、直接，应用广泛 泛化能力差，处理稀疏矩阵的能力差，推荐结果的头部效应明显   矩阵分解 将协同过滤算法中的共现矩阵分解为用户矩阵和物品矩阵，利用用户隐向量和物品隐向量的内积进行排序并推荐 相较协同过滤，泛化能力有所增强，对稀疏矩阵的处理能力有所增强 除了用户历史行为数据，难以利用 其他用户、物品特征及上下文特征   逻辑回归 将推荐问题转换成类似CTR预估的二分类问题，将用户、物品、上下文等不同特征转换成特征向量，再按照预估CTR进行排序并推荐 能够融合多种类型的不同特征 模型不具备特征组合能力，表达能力较差   FM 再逻辑回归的基础上，再模型中假如二阶特征交叉部分，为每一维特征训练得到相应特征隐向量，通过隐向量的内积运算得到交叉特征权重 相比逻辑回归，具备了二阶特征交叉能力，模型的表达能力有所增强 由于组合爆炸问题的限制，模型不易扩展到三阶特征交叉阶段   FFM 在FM模型的基础上，加入“特征域”的概念，使每个特征在与不同域的特征交叉时采用不同的隐向量 相比FM，进一步加强了特征交叉能力 模型的训练开销达到了O(n2)的量级，训练开销较大   GBDT+LR 利用GBDT进行“自动化”的特征组合，将原始特征向量转换成离散型特征向量，并输入逻辑回归模型，进行最终的CTR预估 特征工程模型化，使模型具备了更高阶特征组合的能力 无法进行完全的并行训练，模型更新所需的训练时长较长   LS-PLM 首先对样本进行“分片”，在每个“分片”内部构建逻辑回归模型，将每个样本的各个“分片”概率与逻辑回归的得分进行加权平均，得到最终的预估值 模型结构类似三层神经网络，具备了较强的表达能力 模型结构相比深度学习模型仍比较简单，有进一步提高的空间    2006年，矩阵分解的技术成功应用在推荐系统领域，其隐向量的思想与深度学习中Embedding技术的思路一脉相承；2010年，FM被提出，特征交叉的概念被引入推荐模型，其核心思想——特征交叉的思路也将在深度学习模型中被发扬光大；2012年，LS-PLM在阿里巴巴大规模应用，其结构已经非常接近三层神经网络；2014年，Facebook用GBDT自动化处理特征，揭开了特征工程模型化的篇章。\n另外，Alex Krizhevsky站在Geoffrey Hinton、Yann LeCun、Yoshua Bengio等大师的肩膀上，于2012年提出了引爆整个深度学习浪潮的AlexNet，将深度学习的大幕正式拉开，其应用快速地从图像扩展到语音，再到自然语言处理领域，推荐系统领域也必然紧随其后，投入深度学习的大潮之中。\n从2016年开始，随着FNN、Wide\u0026amp;Deep、Deep Crossing等一大批优秀的推荐模型架构的提出，深度学习模型逐渐席卷推荐和广告领域，成为新一代推荐模型当之无愧的主流。\n深度学习在推荐系统中的应用 随着微软的Deep Crossing，谷歌的Wide\u0026amp;Deep，以及FNN、PNN等一大批优秀的深度学习推荐模型在2016年被提出，推荐系统和计算广告领域全面进入深度学习时代。深度学习时代主要在以下两方面取得重大进展：\n①深度学习模型的表达能力更强，能够挖掘出更多数据中潜藏的模式\n②深度学习的模型结构非常灵活，能够根据业务场景和数据特点，灵活调整模型结构，使模型与应用场景完美契合\n沿着特征工程自动化的思路，深度学习模型从PNN一路走来，经过了Wide\u0026amp;Deep、Deep\u0026amp;Cross、FNN、DeepFM、NFM等模型，进行了大量的、基于不同特征互操作思路的尝试。但特征工程的思路走到这里已经穷尽了可能的尝试，模型进一步提升的空间很小，这也是这类模型的局限性所在。从这之后，越来越多的深度学习推荐模型开始探索更多”结构“上的尝试，诸如注意力机制、序列模型、强化学习等在其他领域大放异彩的模型结构也逐渐进入推荐系统领域，并且在推荐模型的效果提升上成果显著。\n一.AutoRec——单隐层神经网络推荐模型 AutoRec在2015年由澳大利亚国立大学提出。它将自编码器（AutoEncoder）的思想和协同过滤结合，提出了一种单隐层神经网络推荐模型。\n自编码器的原理类似于协同过滤中的共现矩阵，主成分分析等，相当于在重建函数$h(r； \\theta)$中存储了所有数据向量的“精华”。\n如上图，AutoRec是一个非常标准的三层神经网络，紫色单隐层的数量k远小于输入/输出评分向量的维度m，所以可以达到“泛化”的效果。\n重建函数的具体形式：\n$h(r; \\theta) = f(W \\cdot g(Vr+\\mu) + b)$\n其中，$f(\\cdot)$，$g(\\cdot)$分别为输出层神经元和隐层神经元的激活函数。\n为防止重构函数的过拟合，在加入L2正则化项后，AutoRec目标函数的具体形式：\n$\\underset {\\theta}{min} \\sum^m_{j=1}||r^{(i)}-h(r^{(i)} ; \\theta)||^2_O + \\cfrac{\\lambda}{2} \\cdot(||W||^2_F+||V||^2_F)$\n由于AutoRec是一个非常标准的三层神经网络，模型的训练利用梯度反向传播即可完成。\nAutoRec与协同过滤一样，有基于Item的I-AutoRec（Item based AutoRec），当输入物品$i$的评分向量$r^{(i)}$时，模型的输出向量$h(r^{(i)}; \\theta)$就是所有用户对$i$的评分预测。通过遍历，就可以得到一个用户$u$对所有物品的评分预测，进而根据评分预测排序得到推荐列表。U-AutoRec（User based AutoRec）相比I-AutoRec的优势在于仅需输入一次目标用户的用户向量，就可以重建用户对所有物品的评分向量，劣势是用户向量的稀疏性可能会影响模型效果。\n总体来说，AutoRec使用一个单隐层的AutoEncoder泛化用户或物品评分，有泛化和表达能力但是并不足。在模型结构上，AutoRec模型和后来的词向量模型(Word2vec)完全一致，但优化目标和训练方法有所不同。\n二.Deep Crossing——经典的深度学习框架 Deep Crossing由微软在2016年提出，应用在搜索引擎Bing的搜索广告推荐场景。广告点击率则作为Deep Crossing模型的优化目标，即CTR模型。\nDeep Crossing模型特征可以分为三类：一类是可以被处理成one-hot或multi-hot的类别型特征，一类是数值型特征，一类是需要进一步处理的特征，包括广告计划（campaign）、曝光样例（impression）、点击样例（click）等。\n为了完成端到端的训练，Deep Crossing解决了以下三个问题：\n①稀疏特征稠密化——Embedding层以经典的全连接层（Fully Connected Layer）结构为主，另有衍生出的Word2vec、Graph Embedding等。一般来说，Embedding向量的维度应远小于原始的稀疏特征向量，大多几十到上百维。数值型特征不需要Embedding，直接进入Stacking层。\n②自动交叉组合——Multiple Residual Units，相比标准的以感知机为基本单元的神经网络，Deep Crossing采用了多层残差网络（Multi-Layer Residual Network）作为MLP的具体实现。\n③输出层达成CTR预测的目标——Scoring层采用sigmoid（图像分类等多分类问题多采用softmax）\nStacking层比较简单，是把不同的Embedding特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量，该层通常也成为连接层（concatenate）。\n残差神经网络\n最著名的残差网络是在ImageNet大赛中由微软研究员何凯明提出的152层残差网络。推荐模型中的应用也是残差网络首次在图像识别领域之外的成功推广。残差神经网络就是由残差单元（Residual Unit）组成的神经网络，\n上面的残差单元与传统感知机的区别主要有两个不同：\n①输入经过两层以ReLU为激活函数的全连接层后，生成输出向量。\n②输入可以通过一个短路（shortcut）通路直接与输出向量进行元素加（element-wise plus）操作，生成最终的输出向量。\n此时，残差单元中的两层ReLU网络其实拟合的是输出和输入之间的残差（$x^o-x^i$），这就是残差神经网络名称的由来。\n残差神经网络的诞生主要为了解决两个问题：\n①神经网络加深后，容易产生过拟合。残差网络中，由于有输入向量短路的存在，很多时候可以越过两层ReLU网络，减少过拟合的发生。\n②残差单元使用ReLU激活函数取代sigmoid，越靠近0梯度越大。并且输入向量短路相当于直接把梯度毫无变化地传递到下一层，这也使残差网络收敛速度更快。\n三.NeuralCF——CF与深度学习的结合 Embedding层的主要作用是将稀疏向量转换成稠密向量，那么矩阵分解层的用户隐向量和物品隐向量完全可以看作一种Embedding方法。而用户隐向量和物品隐向量的内积操作则可以看作Scoring层。在实际使用矩阵分解来训练和评估模型的过程中，往往会发现模型容易处于欠拟合状态。究其原因是因为矩阵分解的模型结构相对比较简单，特别是Scoring层，无法对优化目标进行有效的拟合。这就要求模型有更强的表达能力，在此动机的启发下，新加坡国立大学的研究人员提出了NeuralCF模型。\nNeuralCF用“多层神经网络+输出层”的结构替代了矩阵分解中简单的内积操作。这样做的收益是直观的，一是让用户向量和物品向量做更充分的交叉，得到更多有价值的特征组合信息；二是引入更多的非线性特征，让模型的表达能力更强。\n以此类推，事实上，用户和物品向量的互操作层可以被任意的互操作形式所代替，这就是所谓的“广义矩阵分解”模型（Generalized Matrix Factorization）。例如，Scoring元素积+输出层逻辑回归。再进一步，可以把不同互操作网络得到的特征向量拼接起来，交由输出层进行拟合。NeuralCF的论文中给出了整合两个网络的例子：\nNeuralCF模型实际上提出了一个模型框架，它基于 用户向量和物品向量这两个Embedding层，利用不同的互操作层进行特征的交叉组合，并且可以灵活地进行不同互操作层的拼接。从这里可以看出深度学习构建推荐模型的优势——利用神经网络理论上能够拟合任意函数的能力，灵活地组合不同的特征，按需增加或减少模型的复杂度。\n在实践中要注意：并不是模型越复杂、特征越多越好。一是要防止过拟合的风险，而是往往需要更多数据和更长的训练时间才能使复杂的模型收敛，这需要算法工程师在模型的实用性、实时性和效果之间进行权衡。\nNeuralCF模型也存在局限性。由于是基于协同过滤的思想进行构造的，所以NeuralCF模型并没有引入更多其他类型的特征，这在实际应用中无疑浪费了其他有价值的信息。此外，对于模型中互操作的种类并没有做进一步的探究和说明。这就需要后来者进行更深入的探索。\n四.PNN——加强特征交叉能力 NeuralCF只提到了用户向量和物品向量两组特征向量，如果加入多组特征向量又该如何设计特征交互的方法呢？2016年，上海交通大学提出的PNN模型，给出了特征交互方式的几种设计思路。\n相比Deep Crossing，PNN模型在输入、Embedding层、多层神经网络，以及最终的输出层部分并没有结构上的不同，唯一的区别在于PNN模型用乘积层（Product Layer）代替了Deep Crossing模型中的Stacking层。也就是说，不同特征的Embedding向量不再是简单的拼接，而是用Product操作进行两两相交，更有针对性地获取特征之间的交叉信息。\nPNN的Product层的多种特征交叉方式 PNN模型对于深度学习的创新主要在于乘积层的引入。具体地说，PNN模型的乘积层由线性操作部分（上图z部分，对各特征向量进行线性拼接）和乘积操作部分（上图p部分）。其中，乘积特征交叉部分又分为内积操作和外积操作，其中内积操作的PNN模型被称为IPNN（Inner Product-based Neural Network），使用外积操作的PNN模型被称为OPNN（Outer Product-based Neural Network）。\n其中外积操作，$g_{outer}(f_i,f_j) = f_if_j^T$，外积互操作生成的是特征向量$f_i,f_j$各维度两两交叉而成的一个$M \\times M$的方形矩阵（其中$M$是输入向量的维度）。这样的外积操作无疑会将问题的复杂度从$M$提升到$M^2$，为了一定程度上减少训练负担，PNN模型的论文中介绍了一种降维的方法，就是把所有两两特征Embedding向量外积互操作结果叠加（Superposition），形成一个叠加外积操作矩阵$p$：\n$p=\\sum^N_{i=1} \\sum^N_{j=1}g_{outer}(f_i,f_j) = \\sum^N_{i=1} \\sum^N_{j=1} f_if_j^T=f_ \\sum f_\\sum^T,f_\\sum=\\sum^N_{i=1}f_i$\n从公式看，叠加矩阵$p$的最终形式类似于让所有特征Embedding向量通过一个平均池化层（Average Pooling）后，再进行外积互操作。在实际应用中，还应对平均池化操作谨慎对待。因为把不同特征对应维度进行平均，实际上是假设不同特征的对应维度有类似含义。但显然，年龄和地域两个特征在经过各自的Embedding后，两者的Embedding向量不在一个向量空间中，显然不具备任何可比性。这是做平均池化，会模糊很多有价值的信息。平均池化的操作经常发生在同类Embedding上，例如，将用户浏览过的多个物品的Embedding进行平均。因此，PNN模型的外积池化操作也需要谨慎，在训练效率和模型效果上进行权衡。\n事实上，PNN模型在对特征的线性和乘积操作后，并没有把结果直接送入上层的$L_1$全连接层，而是在乘积层内部又进行了局部全连接的转换，分别将线性部分$z$，乘积部分$p$映射成了$D_1$维的输入向量$l_z$和$l_p$（$D_1$为$L_1$隐层的神经元数量），再将$l_z$和$l_p$叠加，输入$L_2$隐层。这部分操作不具备创新性，并且可以被其他转换操作完全替代，因此不再详细介绍。\nPNN的结构特点在于强调了特征Embedding向量之间的交叉方式是多样化的，相比于简单的交由全连接层进行无差别化的处理，PNN模型定义的内积和外积操作显然更有针对性地强调了不同特征之间的交互，从而让模型更容易捕获特征交叉信息。\n但PNN模型同样存在局限性，例如在外积操作时，为了优化$M \\times M$的训练效率，对所有特征进行无差别交叉（平均池化），这一定程度上忽略了原始特征向量中包含的有价值信息。如何综合原始特征及交叉特征，让特征交叉的方式更加高效，后续的Wide\u0026amp;Deep模型和基于FM的各类深度学习模型将给出他们的解决方案。\n五.Wide\u0026amp;Deep——记忆能力和泛化能力的综合 谷歌于2016年提出Wide\u0026amp;Deep模型，由单层的Wide部分和多层的Deep部分组成的混合模型。其中，Wide部分的作用是让模型具有较强的“记忆能力”（memorization）；Deep部分的主要作用是让模型具有“泛化能力”（generalization），使模型兼具了逻辑回归（简单模型的记忆能力强）和深度神经网络（深度学习网络不断进行的交叉处理，会减弱记忆能力，但会拥有泛化能力）的优点——能够快速处理并记忆大量历史特征，并且具有强大的表达能力，不仅在当时迅速成为业界争相应用的主流模型，而且衍生出了大量以Wide\u0026amp;Deep模型为基础结构的混合模型，影响力一直延续至今。\nWide\u0026amp;Deep模型把单输入层的Wide部分与由Embedding层和多隐层组成的Deep部分连接起来，一起输入最终的输出层（逻辑回归）。其中Wide部分善于处理大量稀疏特征，而Deep部分善于挖掘特征背后的数据模式。这种把不同特征使用不同处理方法的组合模型，就需要对业务场景的深刻理解。从下图可以详细地了解到Google Play的推荐团队到底将哪些特征作为Deep输入，哪些作为Wide部分输入。\nDeep部分输入全量的特征向量，拼接成1200维的Embedding向量，再经过3层ReLU全连接层，最终输入LogLoss输出层。\nWide部分输入仅仅是已安装应用和曝光应用两类特征，其中已安装应用代表用户的历史行为，而曝光应用代表当前的待推荐应用。选择这两类特征的原因是充分发挥Wide部分的记忆能力，使用简单模型善于记忆用户行为特征中的信息，并根据此类信息直接影响推荐结果。\nWide部分组合“已安装应用”和”曝光应用“两个特征的函数被称为交叉积变换（Cross Product Transformation）函数，其形式化定义如：\n$\\phi_k(X)= \\prod_{i=1}^{d}x_i^{c_ki}$ $c_{ki} \\in {0,1}$\n$c_{ki}$是一个布尔变量，当第$i$个特征属于第$k$个组合特征时，$c_{ki}$的值为1，否则为0；$x_i$是第$i$​个特征的值。例如，对于”AND(user_installed_app=netflix, impression_app=pandora)“这个组合特征来说，只有当\u0026quot;user_installed_app=netflix\u0026quot;和”impression_app=pandora“这两个特征同时为1时，其对应的交叉积变换层的结果才为1，否则为0。\n在通过交叉积变换层操作完成特征组合之后，Wide部分将组合特征输入最终的LogLoss输出层，与Deep部分的输出一同参与最后的目标拟合，完成Wide与Deep的融合部分。\nWide\u0026amp;Deep开启了不同网络融合的新思路，日后有比较经典的2017年由斯坦福大学和谷歌的研究人员提出的Deep\u0026amp;Crossing模型。其主要思路是使用Cross网络替代原来的Wide部分。\n使用Cross网络的目的是增加特征之间的交互力度，使用多交叉层（Cross layer）对输入向量进行特征交叉。假设第$l$层交叉层的输出向量为$x_l$，那么第$l+1$层的输出向量：\n$x_{l+1}=x_0x_l^TW_l + b_l + x_l$\n可以看到，交叉层操作的二阶部分类似于PNN模型中的外积操作，在此基础上增加了外积操作的权重向量$w_l$，以及原输入向量$x_l$和偏置向量$b_l$。\n可以看出，Cross层在参数方面是比较”克制“的，每一层仅增加了一个$n$维的权重向量$w_l$（n维输入向量维度），并且在每一层均保留了输入向量，因此输入与输出之间变化不会非常明显。由多层交叉层组成的Cross网络在Wide\u0026amp;Deep模型中的Wide部分的基础上进行特征的自动化交叉，避免了很多基于业务理解的人工特征组合。同Wide\u0026amp;Deep模型一样，Deep\u0026amp;Cross模型的Deep部分相比Cross部分表达能力更强，使模型具备更强的非线性学习能力。\nWide\u0026amp;Deep模型的影响力无疑是巨大的，不仅是其本身成功应用于多家一线互联网公司，而且其后续的改进创新工作也延续至今。事实上，DeepFM、NFM等模型都可以看成Wide\u0026amp;Deep模型的延伸：\nWide\u0026amp;Deep模型能够取得成功的关键在于：\n①抓住了业务问题的本质特点，能够融合传统模型记忆能力和深度学习模型泛化能力的优势\n②模型的结构并不复杂，易于工程实现、训练和上线，这加速了业界推广\n也正是从Wide\u0026amp;Deep模型之后，越来越多的模型结构被加入推荐模型中，深度学习模型的结构开始朝着多样化、复杂化的方向发展。\n六.FM与深度学习的结合 FNN——用FM的隐向量完成Embedding层初始化 FNN由伦敦大学学院的研究人员于2016年提出，以FM改进Embedding层的Deep crossing模型，用FM模型训练好的各特征向量初始化Embedding层的参数代替随机初始化，相当于在初始化神经网络参数时，已经引入了有价值的先验信息。也就是说，神经网络训练的起点更接近目标最优点，自然加速了整个神经网络的收敛过程。\n一般情况下，模型的收敛速度往往受限于Embedding层。主要有两个原因：①Embedding层的参数量巨大。假设输入层维度维100,000，Embedding层输出维度为32，上层再加5层32维的全连接层，最后输出层维度为10，那么输入层到Embedding层的参数数量是$32 \\times100,000=3,200,000$，其余所有层的参数总数是$(32 \\times 32) \\times4+32\\times 10 = 4416$​。此时Embedding层参数占比99.86%。这就导致大部分的训练时间和计算开销都被Embedding层占据。②由于输入向量过于稀疏，在随机梯度下降时，只有与非零特征相连的Embedding层权重会被更新，这进一步降低了Embedding层的收敛速度。\n需要说明的是，在训练FM的过程中，并没有对特征域进行区分，但在FNN模型中，特征被分成了不同特征域，因此每个特征域具有对应的Embedding层，并且每个特征域Embedding的维度都应与FM隐向量维度保持一致。\nDeepFM——用FM代替Wide部分 FNN把FM的结果作为初始化权重，并没有调整模型结构。而2017年由哈尔滨工业大学和华为公司联合提出的DeepFM则将FM模型与Wide\u0026amp;Deep模型整合：\nFM部分与深度神经网络部分共享相同的Embedding层。左侧FM部分对不同的特征域的Embedding进行两两交叉，也就是将Embedding向量当作原FM中的特征隐向量。最后将FM的输出与Deep部分的输出一同输入最后的输出层，参与最后的目标拟合。\nDeepFM与Deep\u0026amp;Cross模型完全一致，唯一的不同在于Deep\u0026amp;Cross利用多层Cross网络进行特征组合，而DeepFM模型利用FM进行特征组合。当然，具体的应用效果还需要通过实验进行比较。\nNFM——FM的神经网络化尝试 无论是FM还是FFM，归根结底是一个二阶特征交叉的模型，受组合爆炸问题的困扰，FM几乎不可能扩展到三阶以上，这就不可避免地限制了FM模型的表达能力。2017年，新加坡国立大学的研究人员进行了这方面的尝试，提出了NFM模型。\nNFM模型的主要思路是用一个表达能力更强的函数替代原FM中二阶隐向量内积的部分：\n$\\hat{y}{NFM}(x)=w_0 + \\sum^N{i=1}w_ix_i+f(x)$\n传统机器学习可以用来拟合$f(x)$一个表达能力更强的函数，但是进入深度学习时代后，由于深度学习网络理论上有拟合任何复杂函数的能力，$f(x)$的构造工作可以交由某个深度学习网络来完成，并通过梯度反向传播来学习。\nNFM网络架构的特征非常明显，就是在Embedding和神经网络之间加入特征交叉池化层（Bi-Interaction Pooling Layer）：\n$f_{BI}(V_x)=\\sum^n_{i=1} \\sum^n_{j=i+1}(x_iv_i)\\bigodot(x_jv_j)$\n其中，$\\bigodot$代表元素积操作，其中第k维的操作：\n$(v_i\\bigodot v_j)k=v{ik}v_{jk}$\n在进行两两元素积操作后，对交叉特征向量取和，得到池化层的输出向量。再把该向量输入上层的多层全连接神经网络，进行进一步的交叉。\n上图的NFM省略了一阶部分，如果把一阶部分视为一个线性模型，那么NFM的架构也可以视为Wide\u0026amp;Deep模型的进化。相比原始的Wide\u0026amp;Deep模型，NFM模型对其Deep部分加入了特征交叉池化层，加强了特征交叉。这是理解NFM模型的另一个角度。\n七.注意力机制在推荐模型中的应用 “注意力机制”来源于人类最自然的选择性注意的习惯，从2017年开始，推荐领域也开始尝试将注意力机制引入模型之中，这其中影响力较大的工作是由浙江大学提出的AFM和由阿里巴巴提出的DIN。这一机制堆深度学习推荐系统的启发是重大的，使得其更接近用户真实的思考过程。\nAFM——引入注意力机制的NFM 在NFM模型中，不同域的特征Embedding向量经过特征交叉池化层的交叉，将各交叉特征向量进行“加和”，输入最后由多层神经网络组成的输出层。AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络（Attention Net）实现的。AFM的模型结构图：\n注意力网络的作用是为每一个交叉特征提供权重，也就是注意力得分。\n同NFM一样，AFM的特征交叉过程同样采用了元素积操作：\n$f_{PI}(\\varepsilon){(v_i\\bigodot v_j)x_ix_j}_{(i,j)\\in R_x}$\n引入注意力得分后的池化过程：\n$f_{Att}(f_{PI}(\\varepsilon)) = \\sum_{(i,j) \\in R_x} a_{ij}(v_i\\bigodot v_j)x_ix_j$​\n对注意力的分$a_{ij}$来说，最简单的方法就是用一个权重参数来表示，但为了防止交叉特征数据稀疏问题带来的权重参数难以收敛，AFM模型使用了一个在两两特征交叉层（Pair-wise Interaction Layer）和池化层之间的注意力网络来生成注意力得分。 该注意力网络的结构是一个简单的单全连接层加softmax输出层的结构：\n$a^`_{ij}=h^TReLU(W(v_i\\bigodot v_j)x_ix_j+b)$\n$a_{ij}=\\cfrac{exp(a^_{ij})}{\\sum_{(i,j)\\in R_x} exp(a^_{ij})}$\n其中需要学习的参数是特征交叉层到注意力网络全连接层的权重矩阵$W$，偏置向量$b$，以及全连接层到softmax输出层的权重向量$h$。注意力网络将与整个模型一起参与梯度反向传播的学习过程，得到最终的权重参数。\nAFM是研究人员从改进模型角度进行的一次尝试。而阿里巴巴引入注意力机制是基于其对业务观察的一次模型改进，下面介绍阿里巴巴在业界非常知名的推荐模型DIN。\nDIN——引入注意力机制的神经学习网络 它的应用场景是阿里巴巴的电商广告推荐，在计算一个用户$u$是否点击一个广告$a$时，模型的输入特征自然分为两大部分：一部分是用户$u$的特征组，另一部分是候选广告$a$的特征组。无论是用户还是广告，都含有两个非常重要的特征——商品id(good_id)和商铺id(shop_id)。用户特征里的商品特征是一个序列，代表用户曾点击过的商品合集，商铺id同理；而广告特征里的商品id和商铺id就是广告对应的商品id和商铺id（阿里巴巴平台上的广告大部分是参与推广计划的商品）。\n在原来的基础模型中，这些特征进行简单的平均池化操作就后就进入上层神经网络进行下一步训练，序列中的商品既没有区分重要程度，也和广告特征中的商品id没有关系。\n然而事实上，广告特征和用户特征的关联程度是非常强的，假设广告中的商品是键盘，那么用户点击商品序列中的不同商品id：鼠标、T恤和洗面奶。从常识出发，鼠标这个历史商品对预测键盘广告的点击率的重要程度远大于后两者。从模型角度，基于不同特征的注意力理应不同，而且“注意力得分”的计算理应与广告特征有相关性。\n模型中的注意力的强弱，利用候选商品和历史行为商品之间的相关性计算出一个权重，这个权重就代表了“注意力”强弱：\n$V_u=f(V_a)=\\sum ^N_{i=1}w_i \\cdot V_i=\\sum ^N_{i=1}g(V_i,V_a) \\cdot V_i$\n其中$V_u$是用户的Embedding向量，$V_a$是候选广告商品的Embedding向量，$V_i$是用户$u$的第$i$次行为的Embedding向量。这里用户的行为就是浏览商店或店铺，因此行为的Embedding向量就是那次浏览的商品或店铺的Embedding向量。$g(V_i,V_a)$即注意力得分函数采用一个注意力激活单元（activation unit）。其本质上也是一个小的神经网络，其具体结构如上图右上角。可以看出，激活单元的输入层是两个Embedding向量，经过元素减（element-wise minus）操作后，与原Embedding向量一同连接后形成全连接层的输入，最后通过单神经元输出层生成注意力得分。\nDIEN——序列模型与推荐系统的结合 从“注意力机制”开始，越来越多对深度学习模型结构的改进是基于对用户行为的深刻观察而得出。DIEN基于DIN，创新在于用序列模型模拟了用户兴趣的进化过程。序列信息的重要性在于：①加强了最近行为对下次行为预测的影响。②能够学习到购买趋势的信息，如果某个转移概率在全局统计意义上足够高——购买过篮球鞋后购买机械键盘的概率，那么在用户购买篮球鞋时，推荐机械键盘也会成为一个不错的选择。直观上，两者的用户群体很有可能是一致的。\n如果失去序列信息，推荐模型则是基于用户购买历史的综合推荐，而不是针对“下一次购买”的推荐，显然，从业务角度看，后者才是推荐系统正确的推荐目标。\n其中兴趣进化网络分为三层，从下至上依次是：\n①行为序列层（Behavior Layer，浅绿色部分）：其主要作用是把原始的id类行为序列转换成Embedding行为序列。\n②兴趣抽取层（Interest Extractor Layer，米黄色部分）：其主要作用是通过模拟用户兴趣迁移过程，抽取用户兴趣。\n③兴趣进化层（Interest Evolving Layer，浅红色部分）：其主要作用是通过在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程。\n在兴趣进化网络中，行为序列层的结构与普通的Embedding层是一致的，模拟用户兴趣进化的关键在于“兴趣抽取层”和“兴趣进化层”。\n兴趣抽取层的基本结构是GRU（Gated Recurrent Unit）\n本章介绍了以下模型，但深度学习推荐模型从没停下他前进的脚步。从阿里巴巴的多模态、多目标的深度学习模型，到Youtube基于session的推荐系统，再到Airbnb使用Embedding技术构建的搜索推荐模型，深度学习推荐模型不仅进化速度越来越快，而且应用场景也越来越广。在之后的章节中，笔者会从不同的角度出发，介绍深度学习模型再推荐系统中的应用，也希望读者可以在本章的知识结构上，跟踪最新的深度学习推荐模型进展。\n   模型名称 基本原理 特点 局限性     AutoRec 基于自编码器，对用户或者物品进行编码，利用自编码器的泛化能力进行推荐 单隐层神经网络结构简单，可实现快速训练和部署 表达能力较差   Deep Crossing 利用“Embedding层+多隐层+输出层”的经典深度学习框架，预完成特征的自动深度交叉 经典的深度学习推荐模型框架 利用全连接隐层进行特征交叉，针对性不强   NeuralCF 将传统的矩阵分解中用户向量和物品向量的点积操作，换成由神经网络代替的互操作 表达能力加强版的矩阵分解模型 只使用了用户和物品的id特征，没有加入更多其它特征   PNN 针对不同特征域之间的交叉操作，定义“内积”“外积”等多种积操作 在经典深度学习框架上提高特征交叉能力 “外积”操作进行了近似化，一定程度上影响了其表达能力   Wide\u0026amp;Deep 利用Wide部分加强模型的“记忆能力”，利用Deep部分加强模型的“泛化能力” 开创了组合模型的构造方法，对深度学习推荐模型的后续发真产生重大影响 Wide部分需要人工进行特征组合的筛选   Deep\u0026amp;Cross 用Cross网络替代Wide\u0026amp;Deep模型中的Wide部分 解决了Wide\u0026amp;Deep模型人工组合特征的问题 Cross网络的复杂度较高   FNN 利用FM的参数来初始化深度神经网络的Embedding层参数 利用FM初始化参数，加快整个网络的收敛速度 模型的主结构比较简单，没有针对性的特征交叉层   DeepFM 在Wide\u0026amp;Deep模型的基础上，用FM替代原来的线性Wide部分 加强了Wide部分的特征交叉能力 与经典的Wide\u0026amp;Deep模型相比，结构差别不明显   NFM 用神经网络代替FM中二阶隐向量交叉的操作 相比FM,NFM的表达能力和特征交叉能力更强 与PNN模型的结构非常相似   AFM 在FM的基础上，在二阶隐向量交叉的基础上对每个交叉结果加入了注意力得分，并使用注意力网络学习注意力得分 不同交叉特征的重要性不同 注意力网络的训练过程比较复杂   DIN 在传统深度学习推荐模型的基础上引入注意力机制，并利用用户行为历史物品和目标广告物品的相关性计算注意力得分 根据广告物品的不同，进行更有针对性的推荐 并没有充分利用除“历史行为”以外的其他特征   DIEN 将序列模型与深度学习推荐模型结合，使用序列模型模拟用户的兴趣进化过程 序列模型增强了系统对用户兴趣变迁的表达能力，使推荐系统开始考虑时间相关的行为序列中包含的有价值信息 序列模型的训练复杂，线上服务的延迟较长，需要进行工程上的优化   DRN 将强化学习的思路应用于推荐系统，进行推荐模型的线上实时学习和更新 模型对数据实时性的利用能力大大加强 线上部分较复杂，工程实现难度较大    \\未完待续。。。:)\n异常值指通常明显地不同于数据集中其他数据。有些异常值由于自然本身存在特殊情况，一些异常值由于统计失误，无论如何异常点的存在都会扭曲数据集的数据分布，提高数据的不连贯性或使观测产生错误。 为了使训练模型在进行测试时有更好的泛用性，识别并处理异常点非常重要，识别异常值的方法有简单统计分析、3δ原则、z-score及箱型图等。 识别后处理这些异常值的理想方法就是找出引起这些异常值的原因。 处理它们的方法将取决于它们发生的原因，异常值的原因可以分为两大类：人为错误和自然错误。 根据不同原因： 1.删除含有异常值的记录 2.将异常值视为缺失值，交给缺失值处理方法来处理 3.用平均值来修正 4.对于自然异常值且判断可以为模型提供正向训练信息的异常值可以不处理\n","id":14,"section":"posts","summary":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和","tags":[],"title":"1小时读懂《深度学习推荐系统》","uri":"https://biofrostyy.github.io/2021/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","year":"2021"},{"content":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和谐统一的。例如YouTube的用户体验和公司利益（时长越长广告曝光越多）在“观看时长”这一点上达成了一致。\n下图是推荐系统的技术架构示意图。其中数据部分为融合了数据离线批处理、实时流处理的数据流框架；算法和模型部分则为集训练(training)、评估(evaluation)、部署(deployment)、线上推断(online inference)为一体的模型框架。\n推荐系统的进化之路 幸运的是，我们开始做这项工作时，作为用户画像组，我们有着丰富的用户画像。\n我们使用线上的实时特征包括拖寄物、上下文、流向等。\n第二版，我们增加了redis的历史购买。\n再后来，因为画像数据太多，我们分析了之后，对用户画像（交叉）进行了客群分析，使用新客群作为分类特征。\n再后来，增加了再次购买率特征，使点击率增加了1%\n再后来，为了解决长尾，我们将高频与长尾分开，max(高频)低于0.5时，进入长尾规则判断，类似冷启动阶段，使用基于矩阵分解的协同过滤。\n再后来，加入组合推荐，apriori分析，概率模型\n再后来，缺失走兜底规则过多，包装服务过多\n再后来，准时保（流向/拖寄物符合）推荐过多，调整准时保位置，降低判断位置\n","id":15,"section":"posts","summary":"推荐系统的终极优化目标应包括两个维度：一个维度是用户体验的优化，另一个维度是满足公司的商业利益。对一个健康的商业模式来说，这两个维度应该是和","tags":[],"title":"深度学习推荐系统","uri":"https://biofrostyy.github.io/2021/07/%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/","year":"2021"},{"content":"本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，没有选择自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些相关应用。当然，这本书是一本科普读物，至少在仅有的我熟悉的几章中，书中的介绍还是比较基础的模型（当然这正是现在复杂模型的数学基础）。另外，这是一本让你构建宏观体系的书，它介绍了人工智能运用在工业界的方方面面，让人们对这些问题的解决有一个“道”的思想理解，有趣的是，因为这种宏观的描绘，在读书中会有很多背后蕴藏的深刻理论会吸引你停下读书的脚步，对其进行更深的探索，这正是本书的乐趣所在。\n在此记录读书中的收获和积累，也希望给无暇读书的同行一个十分钟读完本书的可能。\n①统计语言模型 当前词的概率只取决于前面N-1个词，这种假设被称为N-1阶马尔可夫假设，对应的语言模型称为N元模型(N-Gram Model)。N = 2的二元模型就只取决于前一个词，而N = 1的一元模型实际上是一个上下文无关的模型，也就是假定当前词出现的概率与前面的词无关。而在实际中，应用最多的是N=3的三元模型，更高阶的模型就很少使用。\n为什么N取值一般都这么小呢？这里主要有两个原因。首先，N元模型的大小（或者说空间复杂度）几乎是N的指数函数，即O(|V|^N )，这里|V|是一种语言词典的词汇量，一般在几万到几十万个。而使用N元模型的速度（或者说时间复杂度）也几乎是一个指数函数，即O(|V|^(N−1) )。因此，N不能很大。而且当模型从3到4时，效果的提升就不是很显著了，但资源的耗费却增加的非常快，所以，除非是为了做到极致不惜资源，很少有人使用四元以上的模型。Google的罗塞塔翻译系统和语音搜索系统，使用的就是四元模型，该模型存储于500台以上的Google服务器中。\n另外，因为上下文之间的相关性可能跨度非常大，甚至可以从一个段落到另一个段落，所以N无论多大都不能覆盖所有的语言现象。这就需要一些长程的依赖性(Long Distance Dependency)来解决问题了。\n而重新估算概率的估计使用古德-图灵估计(Good-Turing Estimate)，模型的零概率及较小统计值的平滑问题卡茨退避法(Katz Backoff)，一般参数T在8-10之间，频数在T以上的词不用进行古德-图灵估计，同时为了保证总概率为1，所有下调频率总和平均分给未出现的词，内伊(Herman Ney)等人对卡茨退避法进行了一次优化，原理大同小异，参考。PS.二元组的相对频率比三元组更接近概率分布，低阶模型比高阶模型零概率问题轻微，因此用低阶语言模型和高阶语言模型进行线性插值来达到平滑目的，这种方法称为删除差值(Deleted Interpolation)，此方法比卡茨退避法略差，现在已经很少用了。\n需要训练数据和应用数据一致且训练量足够大，所以在语料少的情况下，片面追求高阶的大模型没什么意义。另外，训练语料的噪音高低也会产生影响，因此，一般情况下，对于能找到模式(Pattern)的、量比较大的噪音还是有必要过滤的，比如网页文本中存在的大量制表符。\n②谈谈分词 分词问题属于已经解决的问题，在工业界，只要采用基本的统计语言模型，加上一些业界熟知的技巧就能得到很好的分词结果，提高的空间微乎其微（人工分词也有不同的差异）。另外，在手写体识别中，罗马体系的拼音语言也需要分词方法。\n梁南元教授提出的“查字典”这种最简单的方法可以解决七八成以上的分词问题，1990年前后，郭进博士用统计语言模型成功解决了分词二义性问题，即各种不同的分词方式中，出现概率最高的分词方法。但是，如果穷举所有可能的分词方法并计算出每种可能性下句子的概率，那么计算量是相当大的。因此，可以把它看成是一个（Dynamic Programming）的问题，并利用维特比(Viterbi)算法快速地找到最佳分词。接下来，孙茂松教授解决了没有字典时的分词问题，吴德凯教授最早将中文分词方法用于英文词组的分割，并且将英文词组和中文词组在机器翻译时对应起来。\n词的颗粒度问题，在机器翻译中，颗粒度大效果好，而在搜索中，颗粒度小效果好，因为当用户查询清华时，我们时希望能找到清华大学的。对于上述大小粒度的需求，我们可以构造一个分词器同时支持不同层次的分词\u0026mdash;基于基本词表与复合词表，根据基本词表和复合词表分别建立语言模型\u0026ndash;L1和L2。这就是分词的层次概念。\n分词的不一致性可以分为错误和颗粒度不一致两种，错误又分成两类，一类是越界错误，比如把“北京大学生”分成“北京大学-生”。另一类是覆盖型错误，比如把“贾里克尼”拆成四个字。这些是明显的错误，是改进分词器时要尽可能消除的。接下来是颗粒度的不一致性，人工分词的不一致性大多属于此类。这一类不一致性在衡量分词器的好坏时，可以不作为错误。对于某些应用，需要尽可能地找到各种复合词，而不是将其切分。总之，要继续做数据挖掘，不断完善复合词的词典（它的增长速度较快），这也是近年来中文分词工作的重点。\n③信息的度量和作用 变量的不确定性越大，熵也就越大，所需信息量就越大。一本50万字的中文书平均有多少信息量，常用的前10%的汉字占常用文本的95%以上，那么每个汉字的信息熵约8-9bit，如果考虑上下文，每个汉字的信息熵就只有5bit左右。所以一本50万字的中文书，信息量大约是250万比特，采用较好的算法进行压缩，整本书可以存成一个320kb的文件，而如果直接用两字节的国标编码存储这本书，大约需要1MB大小，是压缩文件的3倍，这两个数量的差距，在信息论中被称作“冗余度”(Redundancy)，需要指出的是，这里的250万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复内容很多，它的信息量就小，冗余度就大。而在不同语言中，汉语是冗余度相对小的。\n一个事物的不确定性U，需要引入信息I来消除，而需要引入的信息量取决于这个不确定性的大小，即I\u0026gt;U才行。在某些时候，不引入新的信息，而在已有的信息上玩数字和公式的游戏，本质上和蒙没有区别。\n说回自然语言的统计模型，其中一元模型就是通过某个词本身的概率分布来消除不确定性，而二元及更高阶的语言模型则还使用了上下文的信息，这些“相关的”信息可以消除不确定性，就像ID3树模型中使用的节点选择依据\u0026ndash;条件熵，也是根据条件熵体现某个特征对目标变量取值的判断是否有消除不确定性的作用。所以说，自然语言处理的大量问题就是寻找相关信息。\n还是使用决策树节点选择作为例子，为了度量一个特征（信息）对目标特征（目标信息）的相关性，或者说消除目标信息不确定性的能力，我们使用互信息，而互信息就是目标特征的不确定性(熵)-条件熵（在已知某特征时目标特征的不确定性），当X和Y完全相关时，它的取值时H（X），同时H（X）=H（Y），当两者完全无关时，它的取值是0。\n信息论中的另一个重要概念，相对熵，又叫交叉熵，是机器学习分类模型中经常使用的损失函数。相对熵也用来度量相关性，但和变量的互信息不同，它用来衡量两个取值为正数的函数的相似性。交叉熵是不对称的，詹森和向明提出了一种新的相对熵的计算方法，将其左右取平均，在Google的自动问答系统中，我们采用了上面的詹森-香农度量来衡量两个答案的相似性。相对熵还可以用来衡量两个常用词（在语义和语法上）在不同文本中的概率分布，看它们是否同义。另外，利用相对熵，还可以得到信息检索中最重要的一个概念：词频率-逆向文档频率(TF-IDF)。\n总的来说，熵、条件熵和相对熵这三个概念与语言模型的关系非常密切。贾里尼克从条件熵和相对熵出发，定义了一个称为语言模型复杂度(Perplexity)的概念来直接衡量语言模型的好坏。复杂度有很清晰的物理含义，它是在给定上下文的条件下，句子中每个位置平均可以选择的单词数量。一个模型的复杂度越小，每个位置的词就越确定，模型越好。\n李开复博士介绍他发明的Sphinx语音识别系统的论文里谈到，如果不用任何语言模型（即零元语言模型），（模型的）复杂度为997，也就是说句子中每个位置有997个可能的单词可以填入。如果（二元）语言模型只考虑前后词的搭配，不考虑搭配的概率，复杂度为60。虽然它比不用语言模型好很多，但与考虑搭配概率的二元语言模型相比要差很多，因为后者的复杂度只有20。\n④贾里尼克和现代语言处理 20世纪70年代的IBM，在其他科学家把语音识别问题当作人工智能和模式匹配问题时，贾里尼克等人在IBM把它当作通信问题，并用两个隐马尔可夫模型（声学模型和语言模型）把语音识别概括得清清楚楚。\n贾里尼克和波尔、库克以及拉维夫的另一大贡献是BCJR算法，这是今天数字通信中应用最广的两个算法之一（另一个是维特比算法）。\n⑤简单之美-布尔代数和搜索引擎 Truth is ever to be found in simplicity, and not in the multiplicity and confusion of things.\n布尔代数对于数学的意义等同于量子力学对于物理学的意义。他们将我们对世界的认知从连续状态扩展到离散状态。\n搜索引擎通过建立索引来在零点零几秒内就找到成千上万甚至上亿的结果。早期的文献检索查询系统，严格要求查询语句复合布尔运算。相比之下，今天的搜索引擎会聪明的自动把用户的查询语句转换成布尔运算的算式，但是基本的原理没有什么不同。\n就像我们理解的ES倒排索引进行搜索的原理一样，如果要找到同时包含“原子能”和“应用”的文章，只要将代表是否有这两个字的二进制数组进行布尔运算AND，就可以得到结果。例如“原子能”的布尔数组为10001，表示第一篇和第五篇文章包含原子能关键字。“应用”的布尔数组为01001，那么同时包含“原子能”和“应用”的文章就是00001，即第五篇文章。\n对于互联网的搜索引擎来说，每一个网页就是一个文献。这个索引是巨大的，在万亿字节这个量级。早期的搜索引擎（比如AltaVista以前的所有搜索引擎），由于受计算机速度和容量的限制，只能对重要、关键的主题词建立索引，但是这样不常见的词和太常见的虚词就找不到了。现在，为了保证对任何搜索都能提供相关的网页，常见的搜索引擎都会对所有的词进行索引。但是，这在工程上却极具挑战性。\n加入互联网上有10^10^个有意义的网页，而词汇表的大小是30万，那么这个索引的大小至少是(10^10^)*30=3000万亿。考虑到大多数词只出现在一部分文本中，压缩为30万亿的量级。为了网页排名方便，索引中还需存有大量附加信息，如这个词出现的位置、次数等。因此，整个索引会通过分布式的方式根据网页的序号将索引分成很多份(Shards)，分别存储在不同服务器中。同时，为了提高效率，需要根据网页的重要性、质量和访问的频率建立常用的非常用等不同级别的索引。常用的索引需要访问速度快，附加信息多，更新也要快；而非常用的要求就低多了。但是不论搜索引擎的索引在工程上如何复杂，原理上依然非常简单，即等价于布尔运算。\n⑥图论和网络爬虫 上文提到搜索引擎，主要由下载、索引和排序组成，布尔代数支撑了索引的逻辑，那么图论中的遍历算法支撑了搜索引擎的下载。\n对于图论相关算法有很多，图的遍历、最短路径、MST、Floyd算法等。而基于图论，还有许多Np-Hard的有趣问题如TSP、VRP问题等。\n假定从一家门户网站的首页出发，先下载这个网站，然后通过分析这个网站，可以找到页面里的所有超链接，也就等于知道了这家门户网站首页所直接链接的全部网页。让计算机不停搜索下去，就能下载整个互联网。在网络爬虫中，人们使用Hash Table来记载哪些网站已经被访问过。现在的互联网非常庞大，一个商业的网络爬虫需要有成千上万个服务器，并且通过高速网络连接起来。如何建立起这样复杂的网络结构，如何协调这些服务器的任务，就是网络设计和程序设计的艺术了。\n网络爬虫对网页遍历的次序不是简单的BFS或DFS，而是有一个相对复杂的下载优先级排序方法，由调度系统(Scheduler)管理。在调度系统里需要存储那些已经发现但尚未下载的URL，它们一般存在一个优先级队列(Priority Queue)，这在工程上与BFS更相似，因此，爬虫中BFS的成分多一些。\n对于页面的分析，过去由HTML书写的网站很容易提取，但是现在很多网页是由一些脚本语言(如JavaScript)生成。打开网页的源代码，URL不是直接可见的文本，而是运行这一段脚本才能得到的结果。因此网络爬虫需要模拟运行这一段脚本。另外，有些脚本写的非常不规范，以至于解析起来非常困难（浏览器可以解析），这就需要解析人员具有非常强大的浏览器内核工程能力。因此，如果一些网页明明存在，但搜索引擎没有收录，一个原因可能是网络爬虫中的解析程序没能成功解析网页中不规范的脚本程序，又或者这个网页搞了反爬（就像GitHub.io的博客）。\n上面提到过，Hash Table被用来记录哪些网站已经被访问过，Hash Table判断一个网页是否在表中只需要1个复杂度，写入某新网站也只需要Hash运算。但在一个成千上万个服务器的爬虫工程中，存储并实时维护一张哈希表就是一个令人头疼的事了。为了各个服务器不做重复的工作，它们需要在下载前和下载后实时访问哈希表，这就造成了通信瓶颈。对于这个问题有两个技术：首先明确各个服务器的分工，也就是一看到某个URL就确定要某一台服务器去做（例如结尾数字为1的URL都要服务器1去下载），这样就可以避免重复下载，在明确分工的基础上，判断URL就可以批处理了，比如每次向Hash Table发送一大批询问，或者每次更新一大批Hash Table的内容。这样通信的次数就大大减少了。\n书中没有提到具体的Hash Table存储的解决方案，个人理解可能参考参数服务器的分布式存储构架进行存储。\n⑦搜索引擎的排序-网页质量度量与相关性度量 搜索引擎中，根据索引返回的网页数有成千上万条，那么该如何排序，把用户最想看到的结果排在前面呢。总的来讲，排名取决于两个信息：网页的质量信息(Quality)，以及这个查询与每个网页的相关性信息(Relevance)。\n网页质量的衡量 最初，一些衡量网络质量的方法或多或少地用到了指向某个网页的链接以及链接上的文本（在搜索技术中成为锚文本，Anchor Text）。真正找到计算网页自身质量的完美的数学模型的是Google创始人拉里·佩奇和谢尔盖·布林。PageRank的高明之处在于把整个互联网当作一个整体来对待，这无意中符合了系统论的观点。核心思想是，如果一个网页被很多其他网页链接，说明它受到普遍的承认和信赖，那么它的排名就高。另外，PageRank也考虑了网页排名高的网站贡献的链接权重大这个因素。现在就出现了一个问题，计算搜索结果的网页排名过程中需要用到网页本身的排名，这不就是\u0026quot;先有鸡还是先有蛋\u0026quot;吗？破解这个怪圈的是布林，他把这个问题变成了一个 二维矩阵相乘的问题，并用迭代的方法解决了这个问题，他们先假定所有网页的排名是相同的，然后根据这个初始值，算出各个网页的第一次迭代排名，然后再根据第一次迭代排名算出第二次迭代排名。而且他们从理论上证明了，不论初始值如何选取，这种算法都能保证网页排名的估计值能收敛到排名的真实值，值得一提的是，这种算法不需要任何人工干预。理论的问题解决了，数量庞大的网页有带来了实际问题，十亿个网页时，这个二维矩阵就是一百亿亿个元素。这么大的矩阵相乘，计算量是非常大的。佩奇和布林利用稀疏矩阵的计算技巧，大大简化了计算量，并实现了这个网页排名算法。\nGoogle早期，并行化是半自动的，这样更新一遍所有网页的周期很长。2003年，Google的工程师迪恩(Jeffrey Dean)和格麦瓦特(Sanjay Ghemawat)发明了二MapReduce（矩阵相乘很容易分解成许多小任务），PageRank的并行计算完全自动化了。\nPageRank的计算方法，结果向量B，为一个$1 \\times N$的向量，初始时为$B_0=(\\cfrac{1}{N},\\cfrac{1}{N},…,\\cfrac{1}{N})$，A为$N \\times N$的矩阵，其中值代表第n个网页指向第m个网页的链接数。通过公式$B_i = A \\cdot B_{i-1}$进行迭代，可以证明$B_i$最终会收敛至$B=B \\cdot A$，一般来讲，只要10次左右的迭代基本上就收敛了，即两次迭代的结果$B_i$和$B_{i-1}$之间的差异非常小。\n由于网页之间链接的数量相比互联网的规模非常稀疏，因此需要对零概率或者小概率事件进行平滑处理。\n$B_i = [\\cfrac{α}{N} \\cdot I + (1-α)A] \\cdot B_{i-1}$\n其中N是互联网网页的数量，α是一个（较小的）常数，I是单位矩阵\n网页与查询的相关性度量 2007年，技术和算法的重要性依然高于数据，因此确定相关性主要依靠算法。但是今天，由于商业搜索引擎已经有了大量的用户点击数据，因此，对搜索相关性贡献最大的是根据用户对常见搜索点击网页的结果得到的概率模型。如今，影响搜索引擎质量的诸多因素，除了用户的点击数据之外，都可以归纳成下面四大因素：\n1.完备的索引。俗话说，巧妇难为无米之炊，如果一个网页不在索引中，那么再好的算法也找不到。\n2.对网页质量的度量。当然现在看来，PageRank的作用比10年前已经小了很多。今天，对网页质量的衡量是全方位的，比如对网页内容权威性的衡量，一些八卦网站的PageRank可能很高，但是他们的内容权威性则很低。\n3.用户偏好。一个好的搜索引擎会针对不同用户，对相同的搜索给出不同的排名。\n4.确定一个网页和某个查询的相关性的方法。\nTF-IDF TF（Term Frequency 单文本词频），词频，关键词的出现次数/总字数。其中停止词，如“的”，“是”，“中”，的权重为0。对于一组关键词，可以得到$TF_1$,$TF_2$,\u0026hellip;,$TF_n$\nIDF（Inverse Document Frequency 逆文本频率指数），对于预测主题能力更强的词，权重越大，这个权重的衡量就是IDF，$log(\\cfrac{D}{D_w})$，其中D为全部文本数，$D_w$为关键词w在$D_w$个文本中出现。即如果一个关键词只在很少的网页中出现，通过它就容易锁定搜索目标，它的权重也就应该大。其实，所谓IDF的概念就是一个特定条件下关键词的概率分布的交叉熵(KL散度)，这样，关于信息检索相关性的度量，又回到了信息论。\n现在，各家搜索引擎对关键词重要性的度量，都在TF-IDF的基础上做了一定的改进和微调，但是，原理上与TF-IDF相差不远。\n⑧搜索引擎反作弊问题和搜索结果的权威性问题 搜索引擎反作弊 搜索引擎中排名靠前的网页不一定就是高质量的、相关的网页，而是商业味儿非常浓的作弊网页，它们采用不正当的手段(SPAM)提高自己网页的排名。\n早期的作弊方法是重复关键词，有时为了不让读者看到过多讨厌的关键词，聪明一点的作弊者常用很小的字体或与背景相同的颜色来掩盖这些关键词，其实这种方法很容易被搜索引擎发现并纠正。有了PageRank后，就有了专门买卖链接的生意，有人会创建成百上千的网站用于链接客户的网页。但是大量地卖链接很容易露出马脚（流通量大后很容易找到源头）。当然，还会有各种各样其他的作弊手段，抓作弊是一种长期的“猫捉老鼠”的游戏。\n作弊者所做的事情，就像是在手机信号中加入噪声，这种人为加入的噪声并不难消除，因为这些作弊方法不是随机的（否则就无法提高排名了），因此，可以在搜集一段时间的作弊信息后，将作弊者抓出来。即针对这些商业相关的搜索，采用一套“刚干扰”强的搜索算法。\n首先，那些卖链接的网站，都有大量的出链(Out Links)，而这些出链的余弦距离如果接近1，则说明这些网站间并无关系！那么很有可能这是一个卖链接的网站。其次，反作弊用到的另一个工具是图论。在图中，如果有几个节点两两互相都连接在一起，它们被称为一个Clique。作弊网站一般需要互相链接以提高自己的排名，图论中有专门发现Clique的方法，可以直接用于找到这些作弊网站的Clique。另外，至于术的方面，方法也很多，例如针对作弊的JavaScript跳转页面，通过解析相应JavaScript内容即可。\n作弊的本质是在网页排名信号中加入了噪声，因此反作弊的关键是去噪声。沿着这个思路可以从根本上提高搜索算法抗作弊的能力，事半功倍，而如果只是根据作弊的具体特征头痛医头，脚痛医脚，则很容易被作弊者牵着鼻子走。\n搜索结果的权威性 当用户问的是一些需要专业人士认真作答的的问题，比如医疗方面的问题，那么如何才能从众多信息源中找到最权威的信息，就成了近年来搜索引擎公司面对的难题。这些网页虽然权威性不高，但是文章常常写的很好看，名气也很大，PageRank也很高，但是它们的内容未必权威。其次，互联网上对同一个问题给出的答案常常互相矛盾。比如奥巴马的出生地，竟然有近百个答案，他的一些政敌说他生于肯尼亚，而官方给出的是夏威夷，虽然大家都知道政敌说的话未必可信，但是互联网又怎么知道谁是政敌呢？\n这就会引出“权威性的度量”，为了引入这一点，我们引入一个概念“提及”(Mention)，如果在各种新闻、学术论文或者其他网络信息页中，讨论到“吸烟危害”这一主题时，国际卫生组织和约翰·霍普金斯大学这两个组织作为信息源被多次提及，那么我们就有理由相信这两个组织是谈论“吸烟危害”这个主题的权威机构。需要指出的是，“提及”不像是超链接那样一目了然，它隐含在文章的自然语句中，需要通过自然语言处理的方式分析出来，即使有了好的算法，计算量也是非常大的。并且权威性与搜索主题是相关的，毕竟每个组织都只在自己的领域有权威性，使得存储量非常大，比如M个网页，N个搜索关键词时，我们要计算和存储$O(M \\cdot N)$个结果，而一般的网页只需要M个结果。因此，只有在今天有了云计算和大数据技术的情况下，计算权威性才成为可能。\n在计算权威度时，我们采用了句法分析、互信息和短语（词组）聚类这三种方法。\n计算权威度的步骤：\n1.对网站的文字进行句法分析，找到涉及主题的短语，以及对信息源的描述，这样大量的运算得益于皮耶尔领导开发的Google句法分析器足够快，而且有大量的服务器可供使用。\n2.利用互信息，找到主题短语和信息源的相关性，即出现主题短语时更容易出现某些信息源。\n3.对主题短语进行聚合，聚合那些字面上不同，但意义相同的主题短语，如“吸烟的危害”、“吸烟是否致癌”等等。这样我们就得到了一些搜索的主题。至于聚类的方法，可以采用前面提过的矩阵运算的方法。\n4.对一个网站中的网页聚合，比如把一个网站下面的网页按照子域(Subdomain)或者子目录(Subdirectory)进行聚类。这一步的目的是，即使一个权威的网站，它下面的一些子域却未必具有权威性。比如约翰·霍普金斯大学的网站，它下面可能有很多子域的内容与医学无关。因此，权威性的度量只能建立在子域或者子目录这一级。\n完成上述四个步骤后，我们就可以得到一个针对不同主题，哪些信息源(网站)具有权威性的关联矩阵。当然，在计算这个关联矩阵时，也可以像计算PageRank那样，对权威度高的网站给出“提及”关系更高的权重，并且通过迭代算法，得到收敛后的权威度关联矩阵。这样便可以在搜索结果中提升那些权威度高的信息源的结果，使得用户对搜索结果更放心。\n⑨有限状态机和动态规划-地图与本地搜索的核心技术 2008年9月23日，Google、T-mobile和HTC宣布了第一款基于开源操作系统Android的3G智能手机，它可以利用全球卫星定位系统实现的全球导航功能。此时这个系统可以媲美任何一个卫星导航仪，加上他的地址识别技术（采用有限状态机）比卫星导航仪严格的地址匹配技术（不能输错一个字母）要好得多。智能手机的定位和导航功能，其实只有三项关键技术：第一，利用卫星定位，这一点传统的导航仪都能做到；第二，地址的识别；第三，根据用户输入的起点和终点，在地图上规划最短线路。\n地址分析和有限状态机 地址的写法各种各样，且有比较复杂的上下有关的文法，例如 上海市北京东路XX号，南京市北京东路XX号，当识别器扫描到“北京东路”时，它和后面的门牌号能否构成一个正确的地址，取决于上下文，即城市名。在统计语言模型中有介绍，上下文有关的文法分析非常复杂又耗时，如果没有好的模型，这个分析器写出来很难看不说，很多情况无法覆盖。所幸的是，地址的文法是上下文有关文法中相对简单的一种，因此有许多识别和分析的方法，其中最有效的是有限状态机。有限状态机还能帮助google对用户输入的查询进行分析，挑出其中描述地址的部分，当然，剩下的关键词就是用户要找的内容。\n如果一个地址能从状态机的开始状态进过状态机的若干中间状态，走到终止状态，那么这条地址有效，否则无效。\n使用有限状态机识别地址，关键要解决两个问题，首先，要通过给定地址建立状态机，然后，给定这个有限状态机后，地址字串的匹配算法。好在这两个问题都有现成的算法。\n在实际情况中，用户输入地址不标准或有错别字时，有限状态机因为只能进行严格匹配，所以无法识别。其实，有限状态机在计算机科学早期的成功主要用于程序语言编译器的设计中。为了实现可以模糊匹配，基于概率的有限状态机被提出，它可以给出一个字串为地址的可能性，原理与离散的马尔科夫链基本等效。\n值得一提的是，有限状态机的用途远不止于对地址这样的状态序列进行分析，在Google新一代的产品中，有限状态机被应用在Google Now，一个在智能手机上的基于个人信息的服务软件。他会根据个人的地理位置信息、日历和一些其他信息（对应于有限状态机里面的状态），以及用户当前语音或者文字输入，回答个人的问题，提供用户查找的信息，或者提供相应服务（如打开地图导航、拨打电话等）。Google Now的引擎和AT\u0026amp;T的有限状态机工具库从功能上讲完全等价。\n有一些领域使用一种特殊的有限状态机——加权的有限状态机(Weighted Finite State Transducer, WFST)，有限状态传感器的特殊性在于，每一个状态由输入和输出符号定义，任何一个词的二元组，都可以对应到WFST的一个状态，WFST是天然的自然语言处理的分析和解码工具。在语音识别中，每个句子都可以用一个WFST表示，WFST中的每一条路径就是一个备选句子，其中概率最大的那条路径就是这个句子的识别结果。而这个算法的原理是动态规划。\n全球导航和动态规划 所有的导航系统都采用了动态规划(Dynamic Programming, DP)的办法，关于动态规划的思想及实现相信在数据结构及算法中已经讲解的很深刻，我在这里根据我在路径规划的研究和工作实践，补充一些其他经验。\n路径规划问题的精确解，都会采用动态规划。而在一些节点数庞大，又对完全精确解需求不高的情况下，解决这种NP-hard问题运筹学中的启发式算法，近年来有很多强化学习被应用在路径规划中，但是由于强化学习的加入带来了时间成本的增加，算法整体效率并没有太大提升。\n⑩余弦定理和新闻分类 所谓新闻的分类，或者更广义地讲文本的分类，无非是要把相似的新闻归入同一类中。这就要求我们先把文字的新闻变成一组可计算的数字，然后再设计一个算法来算出任意两篇新闻的相似性。\n一个新闻可以用一个向量表示，根据TF-IDF，对其中的每一个词在词典中进行标识，例如一个65535个词的词典，那么就会形成一个65535维的向量，称为新闻的特征向量(Feature Vector)。和计算搜索相关性一样，出现在文本不同位置的词再分类时的重要性也不相同，一般，标题\u0026gt;正文开头结尾\u0026gt;正文中间。可以对标题和重要位置的词进行额外加权，以提高文本分类的准确性。\n现在我们有了向量来代表一个新闻的内容，可以发现，同一类新闻用词一定是相似的。因此，可以通过计算两个向量的夹角来判断对应的新闻主题的接近程度，即余弦相似度。\n$cosθ = \\cfrac{\u0026lt;b,c\u0026gt;}{|b| \\cdot |c|} = \\cfrac{x_1y_1+x_2y_2+\u0026hellip;+x_{65535}y_{65535}}{\\sqrt{x_1^2+x_2^2+\u0026hellip;+x_{65535}^2} \\cdot \\sqrt{y_1^2+y_2^2+\u0026hellip;+y_{65535}^2}}$\n在工程中，为了简化余弦相似度的计算复杂度$O(N^2·|a|)$，有以下三种方向，首先，分母部分（向量的长度）不需要重复计算，计算向量a和向量b的余弦时，可以先将它们的长度存起来，等计算向量a和向量c的余弦时，直接取用a的长度即可；其次，在计算分子，即两个向量内积时，只需考虑向量中的非零元素，如果一篇新闻有2000个词，那么非零元素一般只有1000个，这样计算的复杂度可以下降1000/词典总词数，计算时间直接从“天”下降到十几分钟这个量级；第三，可以删除虚词，进一步减少非零元素个数，另外删除虚词，不仅可以提高计算速度，对新闻分类的准确性也大有好处，因为虚词的权重其实是一种噪声。经过这些操作，10万篇新闻两两比较，计算时间也就几分钟而已，如果做几十次迭代，可以在一天内计算完。\n具体的分类算法分为两种情形，第一种，假定我们已经有了已知每一类的特征向量，新的新闻特征向量就可以通过计算它与各类新闻特征向量的余弦相似度，来决定将它划分到哪一类中去。第二种，当我们是类似层次聚类，从第向上，一步一步合并直至类别越累越少，而每个类越来越大，当某一类太大时，这一类里一些新闻之间的相似性就很小了，这是就要停止迭代，这就是自动分类的方法。\n本章介绍的这种新闻归类的方法，准确性好，适用于被分类的文本集合再百万数量级。如果大到亿这个数量级，那么计算时间还是比较长的。对于更大规模的文本处理，将在下一章介绍一种更快速但相对粗糙的方法。\n⑪矩阵运算和文本处理中的两个分类问题 自然语言处理中，最常见的两个分类问题分别是，将文本按主题归类（比如新闻分类）和将词汇表中的字词按意思归类（比如将各种运动的项目名称都归到体育类）。这两个分类问题都可以通过矩阵运算来圆满地、一次性地解决。\n上一章中的文本分类其实是一个聚类问题，关键是计算两篇新闻（向量）的相似度。运用余弦相似度时需要两两计算并且多次迭代，虽然算法漂亮但耗时长。我们希望有一个办法，一次就能把所有新闻相关性计算出来。这个一步到位的方法利用的是矩阵运算中的奇异值分解(Singular Value Decomposition, SVD)。\n$M = UΣV$ $$ M=\\left[ \\begin{matrix} a_{11} \u0026amp; \\cdots \u0026amp; a_{1j} \u0026amp; \\cdots \u0026amp; a_{1N} \\ \\cdots\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\cdots \\a_{i1} \u0026amp; \\cdots\u0026amp;a_{ij} \u0026amp; \\cdots \u0026amp; a_{iN} \\ \\cdots\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\cdots\\ a_{M1} \u0026amp; \\cdots\u0026amp;a_{Mj} \u0026amp; \\cdots \u0026amp; a_{MN} \\ \\end{matrix} \\right] $$ 其中，每一行对应一篇文章，每一列对应一个词，如果有N个词，M篇文章，则得到一个$M \\times N$的矩阵，第i行、第j列的元素$a_{ij}$，是字典中第j个词在第i篇文章中出现的加权词频(比如TF-IDF值)。\n奇异值分解，就是把M，分解成三个小矩阵相乘。这三个矩阵有非常清晰的物理含义。\n由于对角矩阵B对角线上的元素的很多值相对其他的值非常小，或者干脆为0，故可以省略。\n$U$的每一行表示一个词，每一列表示一个语义相近的词类，或者称为语义类，数值越大越相关。\n$V$是对文本的分类结果，它的每一列对应一篇文本，每一行对应一个主题。这一列中的每个元素表示这篇文本在不同主题中的相关性。\n$Σ$表示词的类和文章的类之间的相关性。\n因此只要完成对关联矩阵$M$进行一次奇异值分解，就可以同时完成近义词分类和文章的分类。另外，还能得到每个主题和每个词的语义类之间的相关性。这个结果非常漂亮！\n奇异值分解一般分为两步。首先，将矩阵A变成一个双对角矩阵，这个过程的计算量是$O(MN^2)$，当然这里假设M\u0026gt;N。我们仍然可以利用矩阵$A$的稀疏性大大缩短计算时间。第二步是将双对角矩阵变成奇异值分解的三个矩阵，这一步的计算量只是第一步的零头，可以忽略不计。在文本分类中，$M$对应文本的数量，$N$对应词典大小。奇异值分解的计算复杂度与余弦定理一次迭代的时间复杂度处于一个量级，但是它不需要多次迭代。奇异值分解的另一个大问题是存储量较大，因为整个矩阵都需要存在内存里，而利用余弦定理的聚类则不需要。实际应用中，可以先进行奇异值分解，得到粗分类结果，再利用计算余弦向量的方法，在粗分类的基础上，进行几次迭代，得到比较精确的结果。\n理论的问题解决了，下面就是工程的实际问题，怎么利用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值，以及数值分析的各种算法等就统统派上用场了。对于不大的矩阵，比如几万乘几万的矩阵，用计算机上的数学工具MATLAB就可以计算。但是更大的矩阵，比如上百万乘上百万，奇异值分解的计算量非常大。虽然Google早就有了MapReduce等并行计算工具，但是由于奇异值分解很难拆成不相关的子运算，即使在Google内部以前也无法利用并行计算的优势来进行矩阵分解。直到2007年，Google中国（谷歌）的张智威博士带领几个中国的工程师及实习生实现了奇异值分解的并行算法，这是Google中国对世界的一个贡献。\n⑫信息指纹及其应用 所谓信息指纹，可以简单理解为一段信息（文字、图片、音频、视频等）随机地映射到一个多维二进制空间中的一个点（一个二进制数字）。只要这个随机函数做得足够好，那么不同信息对应的这些点就不会重合，因此，这些二进制的数字就成了原来的信息所具有的独一无二的指纹。\n字符串的信息指纹的计算方法一般分为两步，首先，把这个字符串看成是一个特殊、很长的整数，这一步非常容易，因为在计算机里本来就是这样存储的。接下来就需要用到一个产生信息指纹的关键算法：伪随机数产生器算法（Pseudo-Random Number Generator, PRNG），通过它把任意很长的整数转换成定长的伪随机数，现在常用的算法是梅森旋转算法(Mersenne Twister)。\n另外，信息指纹的不可逆性为网络加密传输提供了很好的支持。一些网站会将用户访问的cookie（本来已经加密，但是如果某一访问所有网站的cookie都一样还是会暴漏行为）通过HTTPS加密。加密的可靠性，取决于是否很难人为地找到具有某一指纹的信息，即解密。从加密的角度来讲，梅森旋转算法还不够好，因为它产生的随机数还有一定的相关性，破解一个就等于破解了一大批。\n在互联网上加密要使用基于加密的伪随机数产生器(Cryptographically Secure Pseudo-Random Number Generator, GSPRNG)。常用的算法有MD5或SHA-1等标准，它们可以将不定长的信息变成定长的128位或160位的二进制随机数。\n除了上述信息指纹可以帮助快速且高效得判断文本相同和互联网加密外，信息指纹在互联网和自然语言处理中还有很多应用。\n集合相同的判定 普通的先将两个集合排序，再进行比较的算法时间复杂度为$O(NlogN)$。或者将第一个表放在哈希表里，再用第二个表比对，这个方法可以达到最佳的$O(N)$时间复杂度，但是额外使用了$O(N)$空间，而且代码复杂。完美的办法是计算这两个集合中元素的指纹和，然后进行比较，加法的交换率，保证了集合的指纹不因元素次序变化而变化。类似如检测网络上某首歌曲是否盗版别人，只需要计算这两个音频的信息指纹即可。\n判定基本相同 判定两个邮箱群发的接收电子邮件地址清单(Email Address List)是否相同，可以用来判断是否群发垃圾邮件。但是如果其中有一点小小的不同，上述信息指纹甚至会产生很大的不同。这个时候可以按照规则，挑选几组电子邮箱地址集，例如尾数为24的地址集与尾数为34的地址集，由于挑选的地址集数量一般为个位数，因此可以得到80%，90%重复这样的结果。又或者对网页中的，刨除常见词如“的”，“是”与只出现一次的噪声词，只需要找出剩下的词中IDF值最大的几个词，并且算出它们的信息指纹即可，为了允许有一定的容错能力，Google采用了一种特定的信息指纹——相似哈希(SimHash)。\n假定一个网页中有若干词$t_1,t_2,\u0026hellip;,t_k$，它们的权重（如TF-IDF）为$w_1,w_2,\u0026hellip;,w_k$。先计算出这些词的信息指纹（假定有8位）。第一步扩展，如$t_1$的信息指纹为10100110，那么对$t_1$的权重$w_1$，逢1相加，逢0相减。这样在k次加减$w$之后，可以获得8个实数值。第二步收缩，这些实数值正数变1，负数变0，就可以重新获得一个8位的二进制数，这个数就是这个网站的SimHash。相似哈希的特点是，相同网页或只有小权重词不同网页的相似哈希必定相同，如果两个网页的相似哈希不同但相差很小，则对应的网页也非常相似。工程上使用的64位相似哈希，如果只差一两位，那么对应网页内容重复的可能性大于80%。\n⑬密码学的数学原理 加密函数不应该通过几个自变量和函数值就能推出函数本身，这样一来，破译一篇密文就可能破译以后全部的密文。\n香农提出的信息论为密码学的发展提供了理论基础，根据信息论，密码的最高境界是敌方在解惑密码后，信息量没有增加。一般来讲，当密码之间均匀分布且统计独立时，提供的信息量最少。均匀分布使地方无从统计，而统计独立可保证敌人即使知道了加密算法，并且看到了一段密码和明码后，无法破解另一段密码。\nDiffie和Hellman在1976年的开创性论文“密码学的新方向”($\\it{New \\ Directions \\ in \\ Cryptography}$)，介绍了公钥和电子签名的方法，这是今天大多数互联网安全协议的基础。\n虽然公开密钥下面有许多不同的具体加密方法，比如早期的RSA算法、Rabin算法、和后来的ElGamal算法、椭圆曲线算法(Elliptic curve)，它们的原理基本一致，并不复杂：\n1.它们都有完全不同的密钥，一个用于加密，一个用于解密\n2.这两个看上去无关的密钥，在数学上是关联的\n以下是一个公开密钥的原理：\n公开密钥的好处：\n1.简单，就是一些乘除而已。\n2.可靠，公开密钥方法保证产生的密文是统计独立而分布均匀的。也就是说，不论给出多少份明文与密文的对应，也无法破解下一份密文。更重要的是N、E可以公开给任何人进行加密，但是只有掌握私钥D的人才可以解密，即使加密者也是无法破解的。\n3.灵活，可以产生很多的公开密钥E和私钥D的组合给不同的加密者。\n最后让我们看看破解这种密码的难度。至今的研究结果表明最彻底的方法还是对大数N进行因数分解，即通过N反过来找到P和Q，这样密码就被破解了。而找到P和Q目前只有一个方法：用计算机把所有可能的数字试一遍，这实际上实在拼计算机的速度，这也就是为什么P和Q都需要非常大。一种加密方法只要保证50年内计算机破解不了，也就算是令人满意了。\n遗憾的是，虽然在原理上非常可靠，但是很多加密系统在工程实现上却留下了不少漏洞。因此，很多攻击者从攻击算法转而攻击实现方法。\n⑭数学模型的重要性 1.一个正确的数学模型应当在形式上是简单的\n2.一个正确的模型一开始可能还不如一个精雕细琢过的错误模型来的准确，但是，如果我们认定大方向是对的，就应该坚持下去\n3.大量准确的数据对研发很重要\n4.正确的模型也可能受噪声干扰，而显得不准确；这时不应该用一种凑合的修正方法加以弥补，而是要找到噪声的根源，这也许能通往重大的发现\n⑮最大熵模型 当需要综合几十甚至上百种不同的信息，最大熵模型可以保留全部的不确定性（除了已知的信息，不做主管假设），将风险降到最小。最初，计算能力还不足时，只能处理计算量相对不太大的自然语言处理问题，拉纳帕体成功将上下文信息、词性（名词、动词和形容词）以及主谓宾等句子成分，通过最大熵模型结合起来，做出了当时世界上最好的词性标识系统和句法分析器。且至今仍然是使用单一方法的系统中效果最好的。在2000年前后，计算机速度的提升以及训练算法的改进，很多复杂的问题，包括句法分析、语言模型和机器翻译都可以采用最大熵模型了。最大熵模型和一些简单组合了特征的模型相比，效果可以提升几个百分点，对于投资这种很少的提升也能带来巨大的利润的行业来说非常有效，所以很多对冲基金开始使用最大熵模型。\n$P(d|x_1,x_2,\u0026hellip;,x_{20}) = \\cfrac{1}{Z(x_1,x_2,\u0026hellip;,x_{20})} e^{λ_1(x_1,d)+λ_2(x_2,d)+\u0026hellip;+λ_{20}(x_{20},d)}$\n其中，归一化因子为：\n$Z(x_1,x_2,\u0026hellip;,x_{20}) = \\sum\\limits_{d}e^{λ_1(x_1,d)+λ_2(x_2,d)+\u0026hellip;+λ_{20}(x_{20},d)}$\n在实现方法上，从GIS(Generalized Iterative Scaling)到IIS(Improved Iterative Scaling)缩短了一到两个数量级，再到吴军找到的数学变换在IIS的基础上减少两个数量级，再加上现代的MapReduce并行计算，现在，再1000台计算机上并行计算一天就可以完成了。\n逻辑回归损失函数的基于伯努利分布的最大似然法，与交叉熵损失函数本质相同（经过log变换后公式相同），可见信息论的思想，变换无穷但又不离本质。\n⑯拼音输入法的数学原理 输入汉字的快慢取决于汉字编码的平均长度诚意寻找这个键所需的时间。单纯地编短编码长度未必能提高输入速度，因为寻找一个键的时间可能变得很长。\n对汉字的编码分为两部分：对拼音的编码和消除歧义性的编码。早期的输入法常常只注重第一部分而忽视第二部分，中国最早的汉字输入微机中华学习机和长城0520采用的都是双拼，即每个字母代表一个或多个声母或韵母，这种输入方法看似节省了编码长度，但是输入一点也不快，一是因为增加了歧义性（很多韵母不得不共享一个字母键），二是双拼比按照拼音的全拼增加了拆字的过程，三是容错性不高，比如an,ang的编码完全没有相似性，一个好的输入法不能要求用户读准每个字的发音。早期的输入法都是这样，一味的追求更少的输入的编码（当然它们都声称自己的编码比其他的输入法更合理，但从信息论的角度上看都是一个水平线上的），而忽视了找到每个键所用的时间，要求普通用户背下这些输入方法里所有汉字的编码是不现实的。\n最终，用户还是选择了拼音输入法。它不需要专门学习，并且输入自然，即找每个键的时间很短，另外因为编码长，有信息冗余，容错性好。于是，拼音输入法要解决的问题只剩下排除一音多字的歧义性了，这也是目前各种拼音输入法的主要工作。接下来我们就分析平均输入一个汉字可以做到最少几次击键。\n按照香农第一定理，对于一个信息，任何编码的长度都不小于它的信息熵。以GB2312，6700个常见字集计算，汉字的信息熵在10个比特以内，如果假定输入法只能用26个字母，那么每个字母可以代表log26≈4.7bit（$2^{4.7}≈26$），也就是输入一个汉字平均需要敲10/4.7≈2.1次键盘。而如果把汉字组成词，再以词为单位统计信息熵，不考虑上下文相关性，那么每个汉字的平均信息熵就会减少为8bit，这也是现在所有输入法都基于词输入的根本原因。如果再考虑上下文相关，可以将每个汉字的信息熵降到6bit，这是输入一个汉字只需要敲6/4.7≈1.3次键盘。但事实上没有一种输入法能接近这个效率，有两个原因，首先，需要根据词频进行特殊编码，而上一段说过特殊编码会增加找到每个键的思考时间，其次上，在个人电脑上，很难安装非常大的语言模型。\n事实上，现在汉语全拼的平均长度为2.98，如果能利用上下文解决一音多字的问题，全拼输入法的平均击键次数应该小于3次。而解决上下文问题，10年前的拼音输入法（以紫光为代表）解决这个问题的办法是建立大词库，但是这种方法多少根据经验和直觉，一直是在打补丁。而利用上下文最好的方法是借助语言模型，目前，各家输入法（Google、腾讯和搜狗）基本处在同一个量级，将来技术上进一步提升的关键在于看谁能准确而有效地建立语言模型。\n拼音转汉字的算法和在导航中寻找最短路径的算法相同，都是动态规划。\n每一条路，都是一个可选结果，找到概率最高的一条就是最优的句子：\n$w_1,w_2,\u0026hellip;,w_N = \\mathop{ArgMax}\\limits_{w∈W}P(W_1,W_2,\u0026hellip;,w_n|y_1,y_2,\u0026hellip;,y_N)$\n上述公式的简化方法：\n$w_1,w_2,\u0026hellip;,w_N = \\mathop{ArgMax}\\limits_{w∈W}P(y_1,y_2,\u0026hellip;,y_N|W_1,W_2,\u0026hellip;,w_n)\\ \\cdot \\ P(W_1,W_2,\u0026hellip;,w_n)≈ \\mathop{ArgMax}\\limits_{w∈W}\\prod\\limits_{i=1}^{N}P(w_i|w_{i-1})·P(y_i|w_i)$\n如果对上述简化公式中的概率取对数同时取反，即定义$d(W_{i-1},W_i)=-logP(w_i|w_{i-1})·P(y_i|w_i)$，上面的连乘关系，就变成了加法关系，寻找最大概率的问题，就变成了最短路径问题。\n当输入速度超过一定阈值后，用户的体验可能更重要，客户端上虽然不能放置太大的语言模型，但是可以建立个性化的语言模型。那么就有两个问题：如何训练一个个性化的语言模型，其次是怎样处理好它和通用语言模型的关系。为某个人训练一个特定的词汇量在几万的二元语言模型需要几千万的语料，这对一个人来说可能一辈子也达不到，没有足够多的训练数据，高阶的语言模型根本没用，一些输入法就找到了一种经验做法：用户词典，这实际上是一个小规模的一元模型加上非常小量的元组（比如一个用户定义的词ABC，实际是一个三元组）。\n更好的办法是找到大量符合用户经常输入的内容和用语习惯的语料，训练一个用户特定的语言模型。这就要用到余弦定理和文本分类的技术了，即找到某个人输入文本的特征向量Y和已有文本$X_1,X_2,\u0026hellip;,X_{1000}$，选择前K个和Y距离最近的类对应的文本，作为这个特定用户语言模型的训练数据。\n大部分情况下，$M_1$对这个特定用户的输入比通用模型$M_0$好，但是对于相对冷僻的内容，$M_1$的效果就远不如$M_0$了，所以我们需要综合两个模型。把各种特征综合在一起的最好方法是采用最大熵模型，当然，如果为每个人都建立这样一个模型，成本较高。因此可以采用一个简化的模型：线性插值模型，能得到最大熵模型约80%的收益，Google拼音输入法的个性化语言模型就是这么实现的。\n⑰布隆过滤器 判断一个元素是否在一个集合中，按12章信息指纹所描述的，可以通过8个字节的信息指纹哈希表，但由于哈希表的存储效率一般只有50%，因此一个元素需要占用16个字节，一亿个地址大约要1.6GB，即16亿字节的内容，那么存储几十亿个邮件地址需要上百GB，除非是超级计算机，一般服务器是存不下的。\n而我们现在介绍的一种称作布隆过滤器的数学工具，它只需要哈希表$\\cfrac{1}{8}$到$\\cfrac{1}{4}$的大小就能解决同样的问题。布隆过滤器(Bloom Filter)是由伯顿·布隆(Burton Bloom)于1970年提出的，它实际上是一个很长的二进制向量和一系列随机映射函数。\n假定存储一个亿个电子邮件，那么我们使用16亿个比特位，每个比特位初始值为0。对于每个电子邮件地址X，用8个不同的随机数产生器（$F_1,F_2,\u0026hellip;,F_8$）产生8个信息指纹，再用一个随机数产生器G把这8个信息指纹映射到1-16亿中的8个自然数$g_1,g_2,\u0026hellip;,g_8$。然后把这8个位置的比特位全部设置为1。在把所有邮件都处理后，一个针对这些电子邮件的布隆过滤器就建成了。当有一个新的电子邮件地址Y时，就将Y重复上述的处理结果，然后检查这8个位置是否都是1就行了。当然根据这个逻辑，布隆过滤器决不会漏掉任何一个可以地址，但是，当某个好的电子邮件对应的8个位置都恰好被设置成1时，就会被误判为黑地址。好在这种可能性很小，对于一个元素的集合，这种误识别率在万分之一以下。常见的补救方法是建立一个小的白名单，存储那些可能被误判的地址。\n误判率的计算，先计算某个比特被设为1个概率，根据伯努利分布，$1-(1-\\cfrac{1}{m})^{kn}$。而对于一个新的不在集合中的元素，它的k个bit每个都命中的概率为$(1-(1-\\cfrac{1}{m})^{kn})^{k}$，化简后为\n$P = (1-e^{-\\cfrac{ln{(\\cfrac{m}{n}ln2)n}}{m}})^{ln(\\cfrac{m}{n}ln2)}$\n如果n比较大，可以近似为\n$(1-e^{-k(n+0.5)/(m-1)})^k ≈ (1-e^{\\cfrac{kn}{m}})^k$\n假定一个元素用16bit，即k=8，那么假阳性的概率为万分之五。当然这根据m(布隆过滤器的总bit)，n(集合中元素数)，k(判定一个元素使用的bit数)的不同有所差别。根据直觉，m越大误判率越小，n越大误判率越大，而k从1开始误判率会经过一个先减小再增加的过程。\n⑱马尔可夫\u0026quot;网络\u0026quot;——贝叶斯网络 马尔可夫链描述了一种状态序列，其每一个状态值取决于前面有限个状态，但实际问题中，很多事物相互的关系并不能用一条链串起来，很可能是交叉的、错综复杂的，如图：\n每一个edge都表示一个量化的可信度(belief)，马尔可夫假设保证了贝叶斯网络便于计算，在网络中，每个节点的概率，都可以用贝叶斯公式来计算，贝叶斯网络因此得名，又因为每个edge都有一个可信度，贝叶斯网络也被称作信念网络(Belief Networks)。\n使用贝叶斯网络必须先确定这个网络的拓扑结构，然后还要知道各个状态之间相关的概率，得到拓扑结构和这些参数的过程分别叫做结构训练和参数训练，统称训练。和训练马尔可夫模型一样，训练贝叶斯网络要用一些已知的数据，并且贝叶斯网络的训练比较复杂，从理论上讲，它是一个NP-hard问题。但是对于某些应用，这个训练过程可以简化，并在计算机上实现。\n优化的贝叶斯网络要保证它产生的序列从头走到尾的可能性最大，即后验概率最大。当然产生一个序列可以有多条路径，从理论上讲，需要完备的穷举搜索（Exhaustive Search），但这就是Np-Hard的了，因此一般采用贪心算法（Greedy Algorithm），也就是在每一步时，沿着箭头的方法寻找有限步，为了防止陷入局部最优，可以采用蒙特卡洛（Monte Carlo），用许多随机数在贝叶斯网络中试一试，看看是否陷入局部最优，这个方法计算量比较大。最近，新的方法是利用信息论，计算节点之间两两的互信息，只保留互信息较大的节点直接的连接，然后再对简化了的网络进行完备的搜索，找到全局优化的结构。\n找到结构后就要对参数进行训练了，假定这些权重用条件概率来度量，我们需要优化贝叶斯网络的参数，使得观察到的这些数据的概率（即后验概率）$P(D|θ)$达到最大，这个过程就是EM过程。在计算后验概率时，计算的是条件X和结果Y之间的联合概率$P(X,Y)$，我们的训练数据会提供一些$P(X,Y)$之间的限制条件，而训练出来的模型要满足这些条件。并且这个模型应该是满足最大熵模型的，因此涉及最大熵模型的训练方法在这里都能使用。\n值得一提的是，结构的训练和参数的额训练通常是交替进行的。IBM华生实验室的茨威格博士(Geoffrey Zweig)和西雅图华盛顿大学的比尔默教授（Jeff Bilmer）完成了一个通用的贝叶斯网络的工具包，提供给贝叶斯网络有兴趣的研究者免费使用。\n贝叶斯网络再图像处理、文字处理、支持决策等方面有很多应用。在文字处理方面，语义相近的词之间的关系可以用一个贝叶斯网络来描述，我们利用贝叶斯网络，可以找出近义词和相关的词，因而在Google搜索和Google广告中都有直接的应用。\n贝叶斯网络可用于分析文本，抽取概念，分析主题，这是一种主题模型（Topic Model）。前面提到的余弦相似性和奇异值分解方法都是主题模型的一种。这里介绍通过贝叶斯网络建立的另一种模型——Google的Rephil。在主题模型中，一个主题可以包含多个词，一个词也可以属于多个主题，那么就可以用贝叶斯网络建立一个文章、概念和关键词之间的联系：\n其中，文章和关键词本身有直接的关联，它们两者都还和概念有直接关联，同时它们通过主题还有间接的关联。\n其他参考资料：\n比尔默于茨威格共同发表的论文：http://people.ece.uw.edu/~bilmes/pgs/sort_Date.html\n斯坦福大学科勒（Daphne Koller）教授的巨著：$\\it{Probabilistic \\ Graphical \\ Models \\ :Principles \\ and \\ Techniques}$\n⑲条件随机场、文法分析与其他 句法分析（Sentence Parsing），指根据文法对一个句子进行分析，建立这个句子的语法树，即句法分析（Syntactic Parsing）。20世纪80年代前，人们受形式语言学的影响，采用基于规则的方法，这种方法规则非常多，并且一旦某一步走岔了，需要回溯很多步，计算复杂度大的不得了。20世纪80年代后，布朗大学计算机系的计算语言学家尤金·查尼阿克（Eugene Charniack）统计出文法规则的概率，在选择文法规则时坚持一个原则——让被分析的句子的语法树概率达到最大。马库斯的学生拉纳帕提把文法分析看成是一个括括号的过程，每次从左到右扫描句子的每个词（或句子成分）时，只需要判断是否属于以下三个操作之一：\nA1.是否开始一个新的左括号，比如在“美联储”是新括号的开始。\nA2.是否继续留在这个括号中，比如在“保险公司”的位置，是否继续留在括号中。\nA3.是否结束一个括号，即标上右括号，比如“资金”的位置是一个括号的结束。\n为了判断采用哪个操作，拉纳帕提建立了一个统计模型$P(A|prefix)$，其中A表示行动，句子前缀$prefix$是指句子从开头到目前为止所有的词和语法成分。拉纳帕提用最大熵模型实现了这个模型，当然，拉纳帕提还用了一个统计模型来预测句子成分的种类。但是这种方法对于非常“规矩“的语句，分析正确率在80%以上，但是2000年后，随着互联网的普及，非严谨的句子中这个模型的正确率连50%也达不到，所幸在很多自然语言处理的应用中，并不需要对语句做深入的分析，只要做浅层的分析（Shallow Parsing），比如找出句子中主要的词语以及它们之间的关系即可。\n到了20世纪90年代以后，随着计算机计算能力的增强，科学家们采用了一种新的数学模型工具——条件随机场，大大提高了句子浅层分析的正确率，可达到95%，使文法分析得以应用到很多产品，比如机器翻译上。\n⑳维特比和他的维特比算法 维特比算法是一个特殊但应用最广的动态规划算法，维特比算法是针对一个特殊的图——篱笆网络（Lattice）的有向图最短路径问题而提出的。它之所以重要，是因为凡是使用隐马尔可夫模型描述的问题都可以用它来解码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词等。\n以输入法拼音转汉字为例，输入可见的序列$y_1,y_2,\u0026hellip;,y_N$，而产生他们的隐含序列使$x_1,x_2,\u0026hellip;,x_N$，$P(x_i|x_{i-1})$是状态之间的转移概率，$P(y_i|x_i)$是每个状态的产生概率。这个马尔可夫链的每个状态的输出是固定的，但是每个状态的值可以变化，比如输出读音”zhong“的字可以是”中“”钟“等多个字。\n用符号$X_{ij}$表示状态$x_i$的第$j$个可能的值，如果把每个状态按照不同的值展开，就得到下图的篱笆网络\n每一条路径都能产生我们观察到的输出序列Y，我们要找到的就是最可能（P最大）的一条路径。但是这样的路径组合数非常多，假定句长为10个字，那么这个组合数为$13^5~5 \\times10^{14}$。假定计算每条路径需要20次乘法，就是$10^{16}$次计算，而今天的处理器每秒计算$10^{11}$次的话，也需要一天时间。因此需要一个最好能和状态数目成正比的算法，这就是维特比算法。\n维特比算法的基础可以概括成以下三点：\n1.如果概率最大的路径$P$（或者说最短路径）经过某个点，那么这条路径上从起始点$S$到这个点的子路径$Q$一定是$S$到这个点的最短距离，否则，用$S$到这个点的最短路径$R$替代$Q$，便构成了一条比P更短的路径，这显然是矛盾的\n2.从$S$到$E$路径必定经过第$i$时刻的某个状态，即对于每个$x$，都要经过它的一个状态，不能跳过其中一个时刻。假定第$i$时刻有$k$个状态，那么如果记录了从$S$到第$i$个状态的所有$k$个节点的最短路径，那么最终的最短路径一定经过其中一条。这样，在任何时刻，只要考虑非常有限条候选路径即可\n3.结合上述两点，假定当我们从状态$i$进入状态$i+1$时，从$S$到状态$i$上各个节点的最短路径已经找到，那么在计算从起点$S$到第$i+1$状态的某个节点的最短路径时，只要考虑从S到钱一个状态$i$所有的$k$个节点的最短路径，以及从这$k$个节点到$x_{i+1},j$的距离即可\n基于上述三点基础，维特比算法过程如下：\nstep1.从S出发，对于第一个状态$x_1$的各个节点，不妨假定有$n_1$个，计算出$S$到它们的距离$d(S,x_{1i})$，其中$x_{1i}$代表任意状态1的节点。因为只有一步，所以这些距离都是S到它们各自的最短距离\nstep2.对于第二个状态$x_2$的所有节点，要计算出从S到它们的最短距离。路径长度$d(S,x_{2i}) = d(S,x_{1j})+d(x_{ij},x_{2i})$，由于$j$有$n_1$种可能，我们要一一计算，然后找到最小值，即\n$d(S,x_{2i})=min_{I=1,n_1}d(S,x_{1j})+d(x_{1j},x_{2i})$\n此时，第二个状态有多少个个节点（假设每个节点只有一条最短路径），就有多少个可能路径，与第一个状态的节点数就无关了，这样一直走到最后一个状态，就得到了整个网格从头到尾的最短路径，每一步计算的复杂度都和相邻两个状态$S_I$和$S_{i+1}$各自的节点数$n_i,n_{i+1}$成正比，即$O(n_i \\cdot n_{i+1})$。如果假定在这个隐马尔可夫链中节点最多的状态有D个节点，也就是说整个网格的宽度为D，那么任何一步的复杂度不超过$O(D^2)$，由于网格长度是$N$，所以整个维特比算法的复杂度是$O(N \\cdot D^2)$。回到上述那个输入法问题，计算量基本上是$13 \\times 13 \\times 10 = 1690≈10^3$，这和原来的$10^{16}$有天壤之别。更重要的是，维特比算法与长度$N$成正比，无论是在通信中，还是在语音识别、打字中，输入都是按照流（Stream）的方式进行的，制药处理每个状态的时间比讲话或者打字速度快（这点很容易做到），那么无论输入有多长，解码过程永远是实时的。\n不知道有没有同学对这个算法的原理有非常似曾相识的感觉，其实这个算法与最短路径算法中的Dijkstra算法很相似——它们的基础理论是相似的，但是具体实现及应用场景有些许不同。首先，维特比只能用于有向无环图求最长最短路径，因为有环的时候 维特比的动态规划的递推公式会相互依赖形成类似于死锁的结构，没法解出来。其次，Dijkstra不适用于有负权值的场景。再次，Dijkstra是找一个节点到其他所有节点到最短路径，viterbi则是在篱笆网络里找一条从起点到终点（可能有多个起点，多个终点）的最短路径。HMM，CRF等算法中使用Viterbi算法，是因为Viterbi算法的效率比Dijsktra算法高。\n㉑CDMA技术——3G移动通信的基础 最早，海蒂·拉玛尔（Hedy Lammarr）与她的邻居乔治·安泰尔（George Antheil）一道发明了一种称为”保密通信系统“的调频通信技术。这种传输方式是在一个较宽的扩展频带上进行的，因此它称为扩频传输(Spread-Spectrum Transmission)。和固定频率的传输相比，它有三点明显的好处：\n1.它的抗干扰能力强。当有人想用噪音干扰固定的广播频率时，对于扩频传输来说基本不可能，因为不能把所有的频带都干扰了，否则整个国家的通信就中断了。\n2.扩频传输的信号很难被截获。以极低的功率在很宽的频带上发送加密信号，对于试图截获者来讲，这些信号能量非常低，很难获取。\n3.扩频传输利用带宽更充分。固定频率的通信由于邻近的频率相互干扰，载波频率的频点不能分布得太密集，两个频点之间的频带就浪费了。扩频通信由于抗干扰能力强，浪费的频带较少。\n虽然这种扩频技术和调频技术早在20世纪60年代就应用于军事，但是转为民用则是20世纪80年代以后的事情，在CDMA以前，移动通信使用过两种技术：频分多址(FDMA)和时分多址(TDMA)。\n频分多址，是对频率进行切分，每一路通信使用一个不同的频率，对讲机采用的就是这个原理。由于相邻频率会互相干扰，因此每个信道要有足够的带宽。如果用户数量增加，总带宽就必须增加。我们知道空中的频带资源是有限的，因此要么必须限制通信人数，要么降低话音质量。\n时分多址是将同一频带按时间分成很多份。每个人的（语音）通信数量在压缩后只占用这个频带传输的$\\cfrac{1}{N}$时间，这样同一个频带可以被多个人同时使用了。第二代移动通信的标准都是基于TMDA的。\n前面讲了，扩频传输对频带的利用率比固定频率传输高，因此，如果把很多细分的频带合在一起，很多路信息同时传输，那么应该可以提高带宽的利用率。\n由于每个发送者有不同的密码，接收者在接到不同信号时，通过密码过滤掉自己无法解码的信号，就可以避免相邻频率干扰的问题。由于这种方法是根据不同的密码区分发送的，因此称为码分多址(CDMA)。\n㉒上帝的算法——期望最大化算法 在一般性问题中，如果有非常多的观测数据，定义一个最大化函数，经过若干次迭代，我们需要的模型就训练好了。这实在是太美妙了。\n前面介绍过的很多算法，其实都是EM算法，比如隐马尔可夫模型的训练方法Baum-Welch算法，以及最大熵模型的训练方法GIS算法。在Baum-Welch算法中，E过程就是根据现有的模型计算每个状态之间转移的次数（可以是分数值）以及每个状态产生它们输出的次数，M过程就是根据这些次数重新估计隐马尔可夫的参数。这里最大化的目标函数就是观测值的概率。在最大熵模型的通用迭代算法GIS中，E过程就是跟着现有的模型计算每一个特征的数学期望值，M过程就是根据这些特征的数学期望值和实际观测值的比值，调整模型参数。这里，最大化的目标函数是熵函数。\n值得一提的是，在凸函数中，EM算法一定能达到最优解，但是在非凸函数中，EM算法可能陷于局部最优解。\n㉓逻辑回归和搜索广告 搜索广告基本上走过了三个阶段。第一个阶段以早期的Overture和百度的广告系统为代表，按广告主出价高低来排名。第二个阶段，Google按照预测哪个广告可能被点击，综合出价和点击率(Click Through Rate, CTR)等因素决定广告的投放。第三个阶段是进一步的全局优化。这章主要介绍第二阶段。\n预估点击率有以下几点问题，首先，对于新广告的冷启动问题，第二，点击数据少，统计数据不足的问题，第三，广告的点击量显然与展示位置有关，因此，在预估点击率时，必须消除这些噪音。最后还要指出，影响点击率的因素非常多，这些都是在预估点击率时要考虑的。\n需要整合这些特征，工业界普遍采用了逻辑回归模型（Logistic Regression Model）。逻辑回归作为最基础的广义线性模型，这里就不多介绍了。一个广告系统中，点击率预估机制的好坏决定了能否城北提高单位搜索的广告收入。而目前Google和腾讯的广告系统在预估点击率都采用了逻辑回归函数。\n另外提一句，CTR在推荐中的实现，现在工业上会使用DeepFM、Wide\u0026amp;Deep等深度学习模型。\n㉔分布式和Google云计算的基础 MapReduce实Google云计算的基础，将一个大任务拆分成小的子任务，并且完成了子任务的计算，这个过程叫做Map，将中间结果合并成最终结果，这个过程叫做Reduce。\n㉕Google大脑和人工神经网络 无论是在计算机科学、通信、生物统计和医学，还是在金融和经济学（包括股市预测）中，大多数与“智能”有点关系的问题，都可以归结为一个在多维空间进行模式分类的问题。模式分类的任务就是要在空间里切一刀，将多个类分开，如语音识别中，就是把韵母a和e分开，有了深度学习，我们就可以实现特征工程自动化了。\n除了神经元之间的线性关系之外，毕竟线性关系上的线性关系还是线性关系，并无意义，所以为了实现复杂的弯曲的世界，我们会增加非线性映射。为了通用性，一般在人工神经网络中，规定神经元函数只能对输入变量线性组合后的结果进行一次非线性变换。请注意是组合后的结果，将每一个输入值都进行非线性变换后再线性组合在一起是不被允许的（这样的计算量太大了）。\n从理论上讲，人工神经网络只要设计得当，就可以实现任何复杂曲线（在高维空间里是曲面）的边界。\n总的来说，人工神经网络是一个分层的有向图，第一层输入节点$X_1,X_2,\u0026hellip;,X_n$接受输入的信息，也成为输入层。来自这些点的数值$x_1,x_2,\u0026hellip;,x_n$按照它们输出的弧的权重($w_0,w_1,w_2,\u0026hellip;,w_n$)进行加权，然后再做一次函数变换$f(G)$，赋给第二层的节点Y。\n第二层的节点照此将数值向后传递，直到最后一层，最后一层又被称为输出层，输出层哪个节点的数值最大，输入的模式就被分在了哪一层。\n在人工神经网络中，需要设计的部分只有两个，一个是它的结构，即网络分几层，每层几个节点，节点之间如何连接等等；第二就是非线性函数$f(·)$的设计，常用的函数有指数函数，sigmoid等，在指数函数时，它的模式分类能力等价于最大熵模型。\n值得指出的是，如果我们把不同输出节点上得到的值看成是一种概率分布，那么实际上人工神经网络就等价于一个概率模型了，比如前面提到的统计语言模型。\n关于人工神经网络的训练，一般情况下，在拥有大量训练数据时，我们定义一个cost function，然后按照梯度下降法找到让成本达到最小的参数，当然除了书中提到的外，还有很多值得我们去深入学习的内容，例如，反向传播导致梯度消失和爆炸，以及衍生的用于不同场景的深度学习模型，例如用于图像的CNN、用于推荐的DeepFM等。\n上述都是在有大量训练数据时的选择，当我们无法获得大量标注好的数据时，就需要借助无监督的训练得到人工神经网络的参数。此时，我们需要定义一种新的成本函数，它能够在不知道正确的输出值的情况下，确定（或者预估）训练出的模型是好是坏。最简单的，我们希望分完类后，同一类样本应该靠的比较近，而不同类比较远，这样就可以把每一个样本点到训练出来的聚类中心（Centroid）的欧氏距离作为成本函数。对于估计语言模型的条件概率，就可以用熵作为成本函数。定义了成本函数后，就可以用梯度下降法进行无监督的参数训练了。对于结构复杂的人工神经网络，它的训练计算量非常大，而且是个Np-Hard问题（梯度下降可能会陷入局部最优），因此有很多机器学习的专家在寻找各种好的近似方法。\n需要指出的是，人工神经网络的规模决定了它能做多大的事情，但是要想要人工神经网络上规模并非易事，因为网络节点之间是连接的，它的复杂度会随着网络规模的扩大呈指数上升。幸运的是，从20世纪90年代到2010年，计算机处理能力的增长和云计算的兴起让人工神经网络的用处大大增加，并且因为并行计算，过去训练人工神经网络的方法就必须改变，以适应云计算的要求。Google大脑就是在这样的前提下诞生的，其创新之处也在于利用了云计算的并行处理技术。\nGoogle大脑采用人工神经网络主要有以下三点原因：首先，人工神经网络理论上可以在多维空间“画出”各种形状的模式分类边界，有很好的通用性。其次，人工神经网络的算法比较稳定且通用，可以一次设计长期并多场景使用。第三，人工神经网络容易并行化，被贝叶斯网络则不然。\nGoogle大脑的实现与MapReduce的分治思想相似，但是更加复杂。它会把一整个神经网络模型切成数千块，与MapReduce不同，每一块的计算并不是完全独立的，而是要考虑上下左右很多块，相互的关联总数大致和块数的平方成正比，虽然会让这一部分关联的计算变得复杂，但是达到了分解一个大任务成多个小任务的目的。\n除了并发训练，Google大脑在减少计算量方面做了两个改进。首先，降低了每次迭代的计算量，Google大脑采用了随机梯度下降（Stochastic Gradient Descent）,这种算法秩序随机抽取少量数据来计算成本函数，可以大大降低计算量，虽然会牺牲一点点准确性。第二个改进是减少迭代次数，Google大脑采用比一般梯度下降法收敛的更快的L-BFGS方法（Limited-memory Broyden Fletcher Goldfard Shanno Method），其原理和随机梯度法相似，但是略微复杂一些。它的好处是可以根据离最后目标的“远近”调整每次迭代的步长，这样经过很少次的迭代就能收敛，但是它每一次迭代的计算量也会增加一点（因为要计算二阶导数）。另外，L-BFGS方法更容易并行化计算。借助这两点，Google大脑才能完成外界认为计算量大的难以承受的人工神经网络的训练任务。\n㉖人工神经网络与贝叶斯网络 共同点：\n1.它们都是有向图，每一个节点的取值只取决于前一级的节点，而与更前面的节点无关，也就是说遵从马尔可夫假设\n2.它们的训练方法相似\n3.对于很多模式分类问题，这两种方法在效果上相似，也就是说很多用人工神经网络解决的问题，也能用贝叶斯网络解决，反之亦然，但是它们的效率可能会不同。如果把人工神经网络和贝叶斯网络都看成事统计模型，那么这两种模型的准确性也是类似的\n4.它们的训练计算量都特别大，大家在使用人工神经网络时要有心理准备\n不过，人工神经网络与贝叶斯网络还是有不少差别的：\n1.人工神经网络在结构上完全是标准化的，而贝叶斯网络更灵活。Google大脑选用人工神经网络，就是因为看中了它的标准化这一特点\n2.虽然神经元函数为非线性函数，但是各个变量智能先进性线性组合，然后对一个变量（即前面组合出来的结果）进行非线性变换，因此用计算机实现起来比较容易。而在贝叶斯网络中，变量可以组合成任意的函数，毫无限制，在获得灵活性的同时，也增加了复杂性\n3.贝叶斯网络更容易考虑（上下文）前后的相关性，因此可以解码一个输入的序列，比如将一段语音识别成问题，或者将一个英语句子翻译成中文。而人工神经网络的输出相对孤立，它可以识别一个个字，但是很难处理一个序列，因此它主要的应用常常是估计一个概率模型的参数，比如语音识别中声学模型参数的训练、机器翻译中语言模型参数的训练等，而不是作为解码器\n㉗区块链——椭圆曲线加密原理 想要保护私有信息，特别是隐私，必须有一套比对称的机制，做到在特定授权的情况下，不需要拥有信息也能使用信息；在不授予访问信息的权限时，也能验证信息。比特币的意义就在于，它证实了利用区块链能够做到上述这两件事。\n㉘量子密钥分发——随机性带来的好处 数据泄露无外乎两种可能：在数据存储的地方被盗取，或者在数据传输的过程中被解惑。要解决这个问题，最好的方法就是对数据进行加密。那么是否存在一种无法破解的密码呢？其实信息论的发明人香农早就指出了，一次性密码从理论上讲永远是安全的。而近年来非常热门的量子通信，便是试图实现加密密钥的安全传输，确保保密通信不被识破。这就是量子密钥分发技术。量子通信并不是像很多媒体曲解的那样——靠量子纠缠实现通信，而是靠光量子的偏振特性承载信息，靠数学和信息论的基本原理保证它的保密性。\n光子既是一种粒子，又是一种波，其传播方向与振动方向垂直，这就是爱因斯坦指出的光的波粒二象性。光子的振动频率和偏振的方向都可以人为控制，激光震动的频率已应用到激光通信中，而量子密钥分发利用了光子的偏振特性。即，我们可以在发送方调整光的振动方向来传递信息，比如把光偏振的方向调成水平的，代表0；调成垂直的，代表1。在接收端，我们放置一个垂直的偏振镜就能检测到所传递来的垂直信息，我们收到信号，就认为发送方送来的信息是1。当然，这么做不是很可靠，因为没有收到信号时，不容易确认是对方没有发送，还是发送过来的是0，因此，更好的办法就是在接收方用一个十字交叉的光栅，让垂直和水平的信号都通过，这样就不会把信息0与没有发送信息这件事混淆了。\n当然，（激光）光子的偏振方向可以有各种角度，未必一定要是水平或者垂直的，而如果偏振的方向是其他角度，经过一个水平的光栅，它是否能通过就是随机的了。例如，如果发送方发射了一个偏振方向是45°的光子，它经过垂直光栅的概率是50%，即被检测为1的概率为50%、这个信息就是随机的了。\n利用这个特性，我们就可以来分发密钥了，具体做法是这样的。首先，发送方和接收方约定好有两组信息编码方式，一组用垂直的偏振光代表1，水平的代表0，另一组则分别用45°和135°代表1和0。其次，发送方采用哪种编码方式完全是随机的，而且是交替进行的，它并不告诉接收方。接收方也随机调整偏振镜（光栅）的方向。此时的期望值如下，有50%的可能接收方和发送方的解调和调制方法一致，另外50%不一致的情况下，又有50%的几率随机蒙对，也就是说，不论接收方如何设置偏振镜解调的方向，最后得到的信息大约有75%是一致的，或者说误码率为25%。\n如果在传输过程中，信息被中间的窃听者截获了。由于光子在经过被错误放置的光栅中，光子的偏振方向就无从得知了，得到的是0还是1是完全随机的（可以被认为是噪声）。如果窃听者再将这些信息转发给原本的接收者，接收者得到的信息只有$75% \\times 75% = 56.25%$。接下来，如果接收方再将自己的信息发还给发送者确认，发送者就会发现只有56.25%的一致性，这时他们就知道信息被截获了。\n解决了信息安全问题后，我们就需要消除信息的不确定性，来确定以下双方通信的密钥。这一步其实非常简单，发送方只要用明码将它调制偏振方向的基传给接收方即可。这样，接收方就知道在哪些信息位它设置对了（其实是蒙对了），然后再用明码把它设置对的信息位告诉发送方即可。此时这个密钥一般有一半传送信息的长度（偏振器设置对的概率为50%），且完全是随机的。上述通信都是明码进行的，但是因为窃听者并不知道原本的信息，所以获知哪些位置应该设置什么样的解调器并没有意义，并没有破坏密钥的安全性。\n上述这种通信协议被称为BB84协议，时查理斯·贝内特(Charles Bennett)和吉勒·布拉萨(Gills Brassard)在1984年发表的。后来，人们又在这个协议的基础上进行改进，有了其他的协议，但是其加密和通信原理并没有本质的变化。\n在使用上述协议通信的过程中，发送方和接收方需要通过几次通信彼此确认密钥，而这个密钥只使用一次。如果需要继续通信，就需要再产生和确认新的密钥。因此，这种做法实际上是用时间换取通信的安全性。\n值得一提的是，量子通信绝不像很多媒体讲的是万能的。加入通信卫星真的被黑客攻击了，或者通信的光纤在半途被破坏了，虽然通信的双方知道有人在偷听，能够中断通信，不丢失保密信息，但是于此同时，它就无法保证正常的信息能送出去了，就如同情报机关虽然抓不到对方的信使，却能把对方围堵在家里，不让消息发出。此外，虽然今天已经实现了上千千米量级的量子密钥分发，但是量子通信从实验到工程，再到商用，还有很长的路要走。\n㉙数学的极限——希尔伯特第十问题和机器智能的极限 就如物理学上无法超越的光速极限或绝对零度的极限一样，人工智能的能力有数学的边界。这一边界与技术无关，仅取决于数学本身的限制。具体到今天大家使用的计算机，它有着两条不可逾越的边界，分别是由图灵和希尔伯特划定的。\n图灵划定计算机可计算问题的边界：机器智能显得极为强大，靠的是人们找到了让及其拥有智能的方法，即大数据、摩尔定律和数学模型这三个支柱。我们在前面各个章节中所介绍的内容，其实依然只是一部分数学模型而已。这些数学模型将各种形形色色的实际问题变成了计算问题，当然，这里面有一个前提，就是那些问题本质上就是数学问题，而且是可以用计算机计算的数学问题。但是，当计算机科学家们揭开了一个又一个这样的问题的数学本质之后，人们自然会贪心地以为这样的进步是没有极限的，一致浪费时间去解决根本解决不了、可能也没有必要解决的问题。图灵思考问题的方式恰恰和常人一步步进步的方式相反，他会先划定计算这件事情的边界，在他眼中边界内的问题都是可以通过计算来解决的，当然在边界外可能还有更多的问题，它们与计算无关，无法通过计算来解决，因此图灵并不打算考虑它们。图灵就划定了这样一条边界，后人就不必再浪费时间纠结没有意义的事情，也就不必试图超越边界或极限做事情。\n计算对应于确定性的机械运动，这保证了在相同条件下计算出来的结果是可重复的，而人的意识则可能来自于测不准原理（量子力学中一个微粒的位置与速度无法同时被准确测量，即越精确的知道速度，就越测不准位置，而人则是由无数的测不准堆积出来的意识）。所以，关于人的意识，图灵认为是不确定的，不属于计算的范畴，如果真是像图灵想的那样，那么宇宙本身就存在着大量数学问题之外的问题。事实上，与图灵同时代的数学家哥德尔在1930年便证明了数学不可能既是完备的，又是一致的，也就是说，一些命题即使是对的，我们也无法用数学证明它们。这被称为哥德尔不完全性定理，它说明数学的方法不是万能的。事实上除了意识上的不确定性，一些数学问题也是无法用数学证明的，今天很多数学问题并没有数学答案，比如不存在一组正整数$x,y,z$，让它们满足$x^3+y^3=z^3$。这个问题实际上是著名的费尔马大定理的一个特例，而这个定理本身已于1994年由英国著名的数学家怀尔斯（Andrew Wiles）证明了，也就是说，它是无解的。当然，不管怎么说，知道一个问题无解也算是有了答案。但是，是否还存在一些问题，我们根本无法判定答案存在与否呢？如果有，那么这类问题显然是无法通过计算来解决的。在这一方面给了图灵启发的是希尔伯特。\n1900年，希尔伯特在国际数学大会上提出了23个（当时还无解的）著名的数学问题，其中第十个问题讲的是：\n“任何一个（多项式）不定方程，能否通过有限步的运算，判定它是否存在整数解”\n所谓不定方程（也被称为丢番图方程，Diophantine equation），就是指有两个或更多未知数的方程，它们的解可能有无穷多个。为了对这个问题有根性认知，我们来看三个特例：\n例一 $x^2+y^2=z^2$\n这个方程有三个未知数，它有很多正整数解，每一组解其实就是一组勾股数，构成直角三角形的三边。\n例二 $x^N+y^N=z^N$\n这些方程都没有正整数解，这就是著名的费尔马大定理。\n例三 $x^3+5y^3=4z^3$\n这个方程是否有正整数解，就不那么直观了。更糟糕的是，我们没有办法一步一步地判定它是否存在整数解。此外，需要指出的是，即使我们能判定它有整数解，也未必找得出来。\n希尔伯特第十问题在1970年被苏联数学家尤里·马蒂亚塞维奇（Yuri Matiyasevich）严格地证明了，除了极少数特例，在一般情况下，无法通过有限步的运算，判定一个不定方程是否存在整数解，那么就说明很多数学问题其实上帝也不知道答案是否存在。对于连答案存在都无法判定的问题，答案自然是找不到的，我们也就不用费心去解决这一类问题了。正是希尔伯特对数学问题边界的思考，让图灵明白了计算的极限所在。\n第十问题向世人宣告了很多问题我们无从得知是否有解。如果连是否有解都不知道，就更不可能通过计算来解决它们了。更重要的是，这种无法判定是否有解的问题，要远比有答案的问题多得多。即，有答案的问题\u0026mdash;\u0026gt;可判定的问题\u0026mdash;\u0026gt;数学问题\u0026mdash;\u0026gt;所有问题。\n那么对于可判定且有答案的数学问题是否都能够用计算机解决吗？那就要看看计算机是怎么设计的了，1963年，图灵提出了一种抽象的计算机的数学模型，这就是后来人们常说的图灵机，图灵机的核心思想就是用机器来模拟人进行数学运算的过程，这个过程其实是在不断重复两个动作：在纸上写或擦掉一些符号，用笔在纸上不断移动书写位置。图灵机这种数学模型在逻辑上非常强大，任何可以通过有限步逻辑和数学运算完成的问题，从理论上讲都可以遵循一个设定的过程，在图灵机上完成。今天的各种计算机也不过是图灵机这种模型的一种具体实现方式。不仅如此，今天那些还没有实现的假想计算机，比如量子计算机，在逻辑上也并没有超出图灵机的范畴，因此，在计算机科学领域，人们就把能够用图灵机计算的问题称为可计算的问题。\n是否所有有答案问题都是可计算问题，这一问题依旧有争议，一方面，人们总可以构建出一些类似悖论的数学问题，显然无法用图灵机来解决；另一方面，在现实世界里是否有这样的问题存在，或者说这些构建出来的我呢提是否有意义，很多人觉得暂时没有必要去考虑。但不管怎么讲，依据丘奇和图灵这两位数学家对可计算问题的描述（也就是所谓的丘奇-图灵论题），有明确算法的任何问题都是可计算的，至于没有明确算法的问题，计算也无从谈起。另外，对于理论上可计算的问题，在工程上未必能实现，比如NP-Hard问题，可能需要算到宇宙的尽头，并且图灵机没有存储内容的限制，这在现实中也是不可能的。\n联系一下上述的子集关系，即所有问题\u0026mdash;\u0026gt;数学问题\u0026mdash;\u0026gt;可判定的问题\u0026mdash;\u0026gt;有答案的问题\u0026mdash;\u0026gt;可计算问题\u0026mdash;\u0026gt;工程可解问题\u0026mdash;\u0026gt;人工智能问题。这一种层层递进的包含关系，就可以清楚的看到人工智能的边界了。可以看到，理想状态的图灵机可以解决的问题，只是有答案问题的一部分，而在今天和未来，在工程上可以解决的问题都不会超出这个可计算的范畴。\n完结撒花✨✨✨\n","id":16,"section":"posts","summary":"本书基本上算是我自然语言处理方向的启蒙读物，虽然之后研究生读了机器学习，没有选择自然语言处理，但是无论是在学习中还是工作中，都还是会接触一些","tags":[],"title":"十分钟读完数学之美","uri":"https://biofrostyy.github.io/2021/06/10%E5%88%86%E9%92%9F%E8%AF%BB%E5%AE%8C%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/","year":"2021"},{"content":" 评前碎碎念\n 翻译的好坏总是能左右我对一本书的兴趣，仿佛我试图入坑克苏鲁神话被译本晦涩仿佛机翻的语言劝退的那天，一直以来我对译本总是抱着怀疑的态度，我觉得翻译过就缺失了原作者的最初想法和语言造诣。尤其是日文译本，包括早些年看过村上春树，我不懂日文，不知道是不是日文本身的原因会造成译本晦涩难懂，还好这本书除了开头的几章有些晦涩，后面也没什么阅读障碍，我得以顺利的读完。另外，本书更多聚焦在社会派的推理，直到中间部分才开始出现紧张而烧脑的案件推进情节，对于本格推理爱好者可能吸引不大，对死刑相关及当下社会的思考才是这本小说最注重且可贵的地方。\n 读后感\n 说回这本书的本身，关于刑法的正义性和是否废除死刑的喧嚣从来没有停止过，\u0026lt;大卫·戈尔的一生\u0026gt;，\u0026lt;一级恐惧\u0026gt;等，都让我印象深刻。我也渐渐对于各种故事（尤其是日式推理）里面所有被害人都是罪有应得的设定，开始审美疲劳起来。这本书带来了思考，我也庆幸作者的立场大体上与我的并没有相悖，不至于看得难受。从理性角度私刑永远不该被选择，从感情角度我只关注受害者的故事，无论犯罪者有多么凄惨的过去，只要他伤害的是无辜者，那么他永远得不到我感情上的同情。如果感情上总是站在加害者的立场上，为他找各种理由得到宽恕，遵纪守法的公民们又怎么能感到安全呢？毕竟我真的有一头牛。\n作为这本书的读后感，我将分别从两个主角-纯一和南乡-的角度观察他们的行为与立场。\n因为纯一是个杀人犯，所以他一直在把自己代入罪犯的立场，他会在翻案时犹豫，会认为他们查案去抓住真正的凶手只不过是把一个人解救出来，又把另一个人送上绞刑台吗？他这种想法这在我看来是无法理解的。但是他确实是我感情上的同情者，在这起案件中，佐村恭介是“罪有应得”的受害人，他对于杀害佐村恭介没有懊悔，他也对于佐村恭介的父亲想要杀死自己没有任何责备\u0026ndash;“就像我杀死了佐村恭介那样，他的父亲也可以杀死我”。他体会到了私刑会引起一次又一次的复仇，为了避免这种情况的发生，就必须有人来代替他们做这件事，他安慰了南乡，至少给不是被冤枉的死刑犯实施死刑确确是正确的事。\n再说到审判官南乡，最让我震撼也是感同身受的就是南乡的想法变化，最开始他认为自己在做正义的替受害者惩罚犯罪者的事，后来他看到了一封家属宽恕犯罪者的信，开始怀疑如果受害者家属都希望宽恕犯罪者，自己忍着人类对\u0026quot;杀人\u0026quot;的厌恶实施死刑是为了谁，但其实这里有两个逻辑问题：一是家属可以代替受害者宽恕吗，如果可以那会不会有人伙同罪犯杀掉家人然后再作为家属为罪犯脱罪？二是文中这个家属所说的宽恕是希望施暴者永生都在监狱中，但事实上，按书中所说，如果不是死刑，那么如果考虑表现优异的减刑政策，无期徒刑的平均服刑时间是十八年。南乡一直在挣扎，这些事情本就像是一万个哈姆雷特，我认为或者纯一认为南乡是正义的都没有用，最重要的是南乡可以走出自己心里的牢，毕竟就算是大慈大悲的佛教也为那些无法挽救的愚昧众生准备了破坏神\u0026ndash;不动明王。\n说到本书表达的社会性问题。首先，对死刑问题持有疑问，其中一个原因人们把死刑与杀人的不快感混同在一起导致的，正如南乡在亲自执行死刑之前，他一直是支持死刑制度的，而为了规避私刑，执行死刑必须有第三者，也就是国家机器行使刑罚。这是很多支持废除死刑的人的论据之一，他们认为这对行刑者是一种折磨。有些工作肯定是大家都不想去做的，例如随时会死的矿工工人，但是当他的社会责任感或是朴素的钱给够了，总是会有一些人愿意去做，请注意，是愿意去做，他们有选择权且他们选择了去做，我们尊敬他们，但为了他们废除一种刑罚则大可不必。再一点，有人认为，人生而平等，人命也是平等的，那么法律剥夺罪犯的生命，不和罪犯剥夺被害者的生命一样吗。这种想法我实在是无法苟同。首先罪犯剥夺被害者的生命是一个自然人对另一个自然人生命权力的剥夺，是主动的，而死刑则是报复式刑法的一种，是第三方对一个自然人生命的剥夺。另外，作为社会中拥有权利的公民，首先他是要遵守社会的秩序的，那么在罪犯剥夺受害人生命的时候，就已经放弃了自身作为合法公民的权利，那么法律刑罚应用在他的身上一点都不令人奇怪。\n第二点令我震撼的是，整个故事中反映出的对受害者的恶意，性侵是亲告罪，纯一与友里不敢声张，害怕受到检方和社会更大的而侮辱；南乡是属于自卫杀人，但是在他杀人的一瞬间，他的south wind面包店和家人就已经成为遥不可及的梦。身体上的伤害尚且可愈，心灵上的伤害对于受害者可能是永远的无期徒刑了。\n最后，对于十三级台阶，就像挑水的三个和尚。虽然着墨不多，但是也几次描写了死刑审判员们，因为怕麻烦或担心支持率，虽然案件存疑，也推动了树原亮的死刑进程。作为替受害者”审判\u0026quot;的第三者，也需要有良好的制度与责任感，才能承担法务国家机器的责任。\nPS.十三级台阶，指对于一个死刑犯的判决，共经过13个人的审核。第一步刑事局3人审核；第二步矫正局3人审核；第三步保护局3人审核；第四步法务大臣事务局秘书科科长审核；第五步法务大臣事务局局长审核；第六步法务大臣的次官审核；最后第七步法务大臣审核。一共13人。\n","id":17,"section":"posts","summary":"评前碎碎念 翻译的好坏总是能左右我对一本书的兴趣，仿佛我试图入坑克苏鲁神话被译本晦涩仿佛机翻的语言劝退的那天，一直以来我对译本总是抱着怀疑的态","tags":[],"title":"消失的第13级台阶","uri":"https://biofrostyy.github.io/2021/05/%E8%AF%BB%E5%90%8E%E6%84%9F-%E6%B6%88%E5%A4%B1%E7%9A%84%E7%AC%AC13%E7%BA%A7%E5%8F%B0%E9%98%B6/","year":"2021"},{"content":"一.问题介绍 工作中遇到一个问题，一个列表中，存在多个以\u0026quot;-\u0026ldquo;连接的范围string，希望可以把规则集压缩，即融合的范围融合\nI：[10-30kg,3-10kg,3-10kg,1-3kg,3-10kg,3-10kg,50-100kg]\nO：[1-30kg,50-100kg]\n注：范围string无序且有重复\n二.python实现 ①去重并排序 def takeFirst(elem): return int(elem.split('-')[0]) input = list(set(input.split(\u0026quot;,\u0026quot;))) # 去重 input.sort(key=takeFirst) # 排序 '''此时input为[3-10kg,10-30kg,50-100kg]'''  ②规则融合 def merge(elem1,elem2): elem1 = elem1.split('-') # 将元素基于-分割，次数为没有后缀的情况，如果像我上面的实例有kg作为后缀，可以先对string将进行切片，如elem1[0:-2] elem2 = elem2.split('-') # 如果前一个元素的最大值等于后一个元素的最小值，则进行融合，返回融合值，否则返回空 if elem1[1] == elem2[0]: return ''.join([elem1[0],'-',elem2[1]]) def mergelist(list): res = [] # 如果此input列表只有一个元素，则返回原列表 if len(list) == 1: return list # 从列表后端遍历所有相邻元素对 a = len(list)-2 while a \u0026gt;= 0: new_elem = merge(list[a],list[a+1]) # 如果可以融合，删除两元素，并在原位置插入新元素 if merge(list[a],list[a+1]) is not None: list[a] = new_elem del list[a+1] a -= 1 return list # 如果存在'\u0026gt;100kg'这样的情况，可以先转换成'100-infikg'进行处理，结束后再统一反转 mergedinput = mergelist(input) # 融合  三.你可能还需要\u0026hellip; ①融合前整合数据 我们拿到手的数据，都是松散的，所以需要我们先整合再进行上述融合操作，pandas库的groupby函数可以帮我们做到这一点\nI：\n   name classify weight     小王 手表 0-1kg   小王 日用品 3-10kg   小李 手机 1-3kg   小王 日用品 1-3kg   小李 手机 \u0026lt;1kg   小李 手机 10-30kg   小李 手机 1-3kg    O：\n   name classify weight     小王 手表 0-1kg   小王 日用品 3-10kg,1-3kg   小李 手机 1-3kg,\u0026lt;1kg,10-30kg,1-3kg    outputDataframe = inputDataframe.groupby(['name','classify'])['weight'].apply(lambda x:x.str.cat(sep=',')).reset_index()  ②融合后，无法融合的部分松散数据 对于无法融合成一条的多条规则，因为dataframe后续处理问题，我们也不能将这些规则继续挤在一行用逗号隔开，我们需要将他们拆成多行，保证每一行只有一个规则\nI：\n   name classify weight     小王 手表 0-1kg   小王 日用品 1-10kg   小李 手机 0-3kg,10-30kg    O：\n   name classify weight     小王 手表 0-1kg   小王 日用品 1-10kg   小李 手机 0-3kg   小李 手机 10-30kg    ''' 本质上，使用numpy来存储累加数据，再重新转为dataframe ''' newvalues=np.dstack((np.repeat(inputDataframe.name.values,list(map(len,inputDataframe.weight.values))),np.repeat(inputDataframe.classify.values,list(map(len,inputDataframe.weight.values))),np.concatenate(inputDataframe.weight.values))) outputDataframe = pd.DataFrame(data=newvalues[0],columns=inputDataframe.columns)  ","id":18,"section":"posts","summary":"一.问题介绍 工作中遇到一个问题，一个列表中，存在多个以\u0026quot;-\u0026ldquo;连接的范围string，希望可以把规则集压缩，即融合的范围","tags":[],"title":"python 实现规则集分类并融合压缩","uri":"https://biofrostyy.github.io/2021/05/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E8%A7%84%E5%88%99%E9%9B%86%E5%88%86%E7%B1%BB%E5%B9%B6%E8%9E%8D%E5%90%88%E5%8E%8B%E7%BC%A9/","year":"2021"},{"content":"一.算法介绍 实践于阿里19年发布的paper，地址 https://arxiv.org/abs/1905.06874?context=cs.AI\n关键字：pytorch, BST, 行为序列建模，推荐模型，Transformer\n主要输入特征有Item Feature、用户画像、上下文特征、交叉特征经过Embedding 层后concat 在一起。用户行为序列包含Item ID类特征及对应的position 信息，进行Embedding 处理后输入到Transformer 层(Transformer 的Encoder 部分)捕获用户历史行为与Target Item 之间的相互关系得到用户行为兴趣表达，与其他特征embedding 向量concat 在一起，经过三层MLP层计算得到预测的点击率CTR\n二.Implementation ①Data from urllib.request import urlretrieve from zipfile import ZipFile urlretrieve(\u0026quot;http://files.grouplens.org/datasets/movielens/ml-1m.zip\u0026quot;, \u0026quot;movielens.zip\u0026quot;) ZipFile(\u0026quot;movielens.zip\u0026quot;, \u0026quot;r\u0026quot;).extractall()  ②规则融合 def merge(elem1,elem2): elem1 = elem1.split('-') # 将元素基于-分割，次数为没有后缀的情况，如果像我上面的实例有kg作为后缀，可以先对string将进行切片，如elem1[0:-2] elem2 = elem2.split('-') # 如果前一个元素的最大值等于后一个元素的最小值，则进行融合，返回融合值，否则返回空 if elem1[1] == elem2[0]: return ''.join([elem1[0],'-',elem2[1]]) def mergelist(list): res = [] # 如果此input列表只有一个元素，则返回原列表 if len(list) == 1: return list # 从列表后端遍历所有相邻元素对 a = len(list)-2 while a \u0026gt;= 0: new_elem = merge(list[a],list[a+1]) # 如果可以融合，删除两元素，并在原位置插入新元素 if merge(list[a],list[a+1]) is not None: list[a] = new_elem del list[a+1] a -= 1 return list # 如果存在'\u0026gt;100kg'这样的情况，可以先转换成'100-infikg'进行处理，结束后再统一反转 mergedinput = mergelist(input) # 融合  三.你可能还需要\u0026hellip; ①融合前整合数据 我们拿到手的数据，都是松散的，所以需要我们先整合再进行上述融合操作，pandas库的groupby函数可以帮我们做到这一点\nI：\n   name classify weight     小王 手表 0-1kg   小王 日用品 3-10kg   小李 手机 1-3kg   小王 日用品 1-3kg   小李 手机 \u0026lt;1kg   小李 手机 10-30kg   小李 手机 1-3kg    O：\n   name classify weight     小王 手表 0-1kg   小王 日用品 3-10kg,1-3kg   小李 手机 1-3kg,\u0026lt;1kg,10-30kg,1-3kg    outputDataframe = inputDataframe.groupby(['name','classify'])['weight'].apply(lambda x:x.str.cat(sep=',')).reset_index()  ②融合后，无法融合的部分松散数据 对于无法融合成一条的多条规则，因为dataframe后续处理问题，我们也不能将这些规则继续挤在一行用逗号隔开，我们需要将他们拆成多行，保证每一行只有一个规则\nI：\n   name classify weight     小王 手表 0-1kg   小王 日用品 1-10kg   小李 手机 0-3kg,10-30kg    O：\n   name classify weight     小王 手表 0-1kg   小王 日用品 1-10kg   小李 手机 0-3kg   小李 手机 10-30kg    ''' 本质上，使用numpy来存储累加数据，再重新转为dataframe ''' newvalues=np.dstack((np.repeat(inputDataframe.name.values,list(map(len,inputDataframe.weight.values))),np.repeat(inputDataframe.classify.values,list(map(len,inputDataframe.weight.values))),np.concatenate(inputDataframe.weight.values))) outputDataframe = pd.DataFrame(data=newvalues[0],columns=inputDataframe.columns)  参考代码\nhttps://github.com/D-Roberts/transformer-recommender\nhttps://github.com/jiwidi/Behavior-Sequence-Transformer-Pytorch\nhttps://github.com/nihalsangeeth/behaviour-seq-transformer\nhttps://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation\n","id":19,"section":"posts","summary":"一.算法介绍 实践于阿里19年发布的paper，地址 https://arxiv.org/abs/1905.06874?context=cs.AI 关键字：pytorch, BST, 行为序列建模，推荐模型，Transformer 主要输入特征有I","tags":[],"title":"阿里BST CTR预测模型复现练习","uri":"https://biofrostyy.github.io/2021/05/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-bst/","year":"2021"},{"content":"几乎所有的算法书中，排序算法都是在最开始介绍的算法，不仅仅是因为排序算法非常简单，而且因为排序算法非常基础，在后续其他算法或者处理其他问题时都有广泛的应用。就像我在刷题时，应用到排序就可以直接按时间复杂度$O(nlogn)$计算。所以此次复习算法和数据结构也从排序算法开始，使用到的工具书有\u0026lt;算法4\u0026gt;、\u0026lt;我的第一本算法书\u0026gt;及其app、\u0026lt;labuladong的算法小抄\u0026gt;。刷题网站为leetcode，理论上会把相关主题的题全部刷到，并且整理出笔记与代码沉淀。\n一.排序算法 ①时间复杂度为$O(n^2)$的排序算法——选择排序、插入排序、希尔排序 选择排序的过程为，首先，找到数组中最小的那个元素，其次，将它和数组的第一个元素交换位置（如果第一个元素就是最小元素那么它就和自己交换）。再次，在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置。如此往复，直到将整个数组排序。这种方法叫做选择排序，因为它在不断地选择剩余元素之中的最小者。\n总的来说，选择排序是一种很容易理解和实现的简单排序算法，它有两个很鲜明的特点。其一：运行时间和输入无关。即一个已经有序的数组或是主键全部相等的数组和一个元素随机排列的数组所用的排序时间一样长。一些其他的排序算法可能会更善于利用输入的初始状态。其二：数据移动是最少的。每次在找到最小值时才会进行一次交换，因此选择排序用了N次交换——交换次数和数组的大小是线性关系。我们将研究的其他任何算法都不具备这个特征（大部分的增长数量级都是线性对数或是平方级别）。\n插入排序则像整理桥牌一样一张一张的来，将每一张牌插入到其他已经有序的牌中的适当位置，与选择排序一样，当前索引左边的所有元素都是有序的，但它们的最终位置还不确定，为了给更小的元素腾出空间，它们可能会被移动。但是当索引到达数组的右端时，数组排序就完成了。\n对于随机排列的长度为$N$且主键不重复的数组，平均情况下插入排序需要$\\cfrac{N^2}{4}$次比较以及$\\cfrac{N^2}{4}$次交换。最坏情况下需要$\\cfrac{N^2}{2}$次比较和$\\cfrac{N^2}{2}$次交换，最好情况下需要$N-1$次比较和$0$次交换。其中的情况好坏取决于输入中元素的初始顺序。\n我们可以发现在比较好的情况下，即对于部分有序的数组，插入排序可以获得线性的时间复杂度！\nPS.衡量一个数组是不是部分有序和有序的程度时，可以用到倒置数量这个指标。倒置指的是数组中的两个顺序颠倒的元素。比如E X A M P L E中有11对倒置：E-A、 X-A、 X-M、 X-P、 X-L、 X-E、 M-L、 M-E、 P-L、 P-E以及L-E。如果数组中倒置的数量小于数组大小的某个倍数，那么我们说这个数组是部分有序的。下面是几种典型的部分有序的数组：❏数组中每个元素距离它的最终位置都不远；❏一个有序的大数组接一个小数组；❏数组中只有几个元素的位置不正确。插入排序对这样的数组很有效，而选择排序则不然。事实上，当倒置的数量很少时，插入排序很可能比本章中的其他任何算法都要快。\n上面我们说到，对于部分有序的数组，插入排序非常有效，那么我们可不可以对数组预处理成部分有序，再用插入排序呢。这就是希尔排序。希尔排序又叫缩小增量排序，它是基于插入排序的增强版。时间复杂度是$O(N*(logN)^2)$，在最坏的请款下比较次数和$N^{\\cfrac{3}{2}}$。人们发明了很多递增序列来渐进式地改进最坏情况下所需的比较次数（N4/3,N5/4, N6/5…），但这些结论大多只有学术意义，因为对于实际应用中的N来说它们的递增序列的生成函数（以及与N乘以一个常数因子）之间的区别并不明显。\n②时间复杂度为$O(nlogn)$的排序算法——归并排序、快速排序 当我们看到$logn$时不难想到\u0026quot;二分\u0026quot;的思想。\n归并排序运行速度比简单排序块，但是它需要的空间是原始数组空间的两倍；通常这是一个严重的缺点。\n二.代码沉淀 ①有序数组中查找左右边界 def helper(tar): #左边界 i, j = 0, len(nums) - 1 while i \u0026lt;= j: m = (i + j) // 2 if nums[m] \u0026lt; tar: i = m + 1 else: j = m - 1 return j def helper(tar): # 右边界 i, j = 0, len(nums) - 1 while i \u0026lt;= j: m = (i + j) // 2 if nums[m] \u0026lt;= tar: i = m + 1 else: j = m - 1 return i  ②双指针——将两个数组合并 # 有些题中，为了不占用新内存空间，可以使用逆向双指针在原数组上进行 class Solution: def merge(self, A: List[int], m: int, B: List[int], n: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Do not return anything, modify A in-place instead. \u0026quot;\u0026quot;\u0026quot; pointera = m-1 pointerb = n-1 while pointera \u0026gt;= 0 and pointerb \u0026gt;= 0: if A[pointera] \u0026gt;= B[pointerb]: A[pointera+pointerb+1] = A[pointera] pointera -= 1 else: A[pointera+pointerb+1] = B[pointerb] pointerb -= 1 while pointerb \u0026gt;= 0: A[pointerb] = B[pointerb] pointerb -= 1 '''题目链接：https://leetcode-cn.com/problems/sorted-merge-lcci/ 此题还可以直接把B数组填入A数组中，对A进行排序''' # 另一些题中，希望两种数字（如奇偶），在特定的位置，可以使用双指针，找到后交换两指针的元素  ③python排序实现 ------自定义排序 在python中有自定义排序函数,list.sort(key=函数) sorted_items = sorted(list,key=函数,reverse=False) 可以根据function对一个list进行排序，函数可以返回一个tuple(x[0],-x[1])，表示先根据x[0]排序，如果相同再根据x[1]排序，负号可以达到根据x[1]降序排列的目的 引申的，如果想根据一个元素在list中出现的次数排序，sorted(list,key=lambda x:(list.count(x),-x))，collections.Counter(s)方法可以返回一个字典，表示每个元素的出现次数。对于上述方法的提升---return ''.join([i*j for i, j in sorted([[i, j] for i, j in dict_al.items()], key=lambda x: -x[1])]) ------计数排序，当数据种类较少时可以使用 参考例题：https://leetcode-cn.com/problems/relative-sort-array/  ④python排列组合实现 ------全排列combinations和permutations函数 combinations方法重点在组合，permutations方法重在排列。返回的都是一个iterator。 ------回溯递归 def permutation(self, s: str) -\u0026gt; List[str]: if len(s) \u0026lt;= 1: #当只剩一个的时候返回 return [s] return list(set(s[i] + perm for i in range(len(s)) for perm in self.permutation(s[:i] + s[i+1:]))) # 此时当每个元素都不同时，不会有相同的返回值，当有基本元素相同时(['a','a','b']三个基本元素就会出现两个\u0026quot;aab\u0026quot;)，set()，可以帮助去除重复 def permutation(self, s: str) -\u0026gt; List[str]: # 递归普通写法 result = [] def permutation(ans, s): if not s: return result.append(ans) for i in set(s): new_ans = i new_s = s.copy() new_s.remove(i) permutation(ans + new_ans, new_s) permutation('', list(s)) return result -------回溯非递归 class Solution: def permutation(self, s: str) -\u0026gt; List[str]: n = len(s) curr = list(sorted(s)) end = list(reversed(curr)) ans = [] # 生成下一个排列 while curr != end: ans.append(''.join(curr)) i = n - 2 # 29631 -\u0026gt; 31269 while i \u0026gt; 0 and curr[i] \u0026gt;= curr[i+1]: i -= 1 j = n - 1 while j \u0026gt; i-1 and curr[j] \u0026lt;= curr[i]: j -= 1 curr[i], curr[j] = curr[j], curr[i] curr = curr[:i+1] + sorted(curr[i+1:]) ans.append(''.join(end)) return ans  ⑤python bisect二分查找 \u0026quot;\u0026quot;\u0026quot; bisect 为可排序序列提供二分查找算法 \u0026quot;\u0026quot;\u0026quot; import bisect #使用bisect函数前需要对列表进行排序，否则虽然可以输出数值，但没有意义 a = [1, 5, 6, 10, 9] a.sort() print(\u0026quot;最初的列表：\u0026quot;, a) #bisect.bisect 返回某个数在列表中可以插入的位置，但不会插入该数。 #如果这个数与列表中的元素相同，则返回元素后面的位置 print(\u0026quot;6在列表中可以插入的位置：\u0026quot;, bisect.bisect(a, 6)) #bisect.insort 将某个数插入列表 bisect.insort(a, 7) print(\u0026quot;在列表中插入7：\u0026quot;, a) #处理插入数值与列表元素相同的情况，返回位置，但不会插入该数 #bisect.bisect_left 插入元素左侧位置；bisect.bisect_right 插入元素右侧位置 print(\u0026quot;9在列表中可以插入的位置：\u0026quot;, bisect.bisect_left(a, 9)) print(\u0026quot;9在列表中可以插入的位置：\u0026quot;, bisect.bisect_right(a, 9)) #处理插入数值与列表元素相同的情况，插入该数 #bisect.insort_left 插入元素左侧位置；bisect.insort_right 插入元素右侧位置 bisect.insort_left(a, 9) print(\u0026quot;在列表中插入10：\u0026quot;, a) bisect.insort_right(a, 10) print(\u0026quot;在列表中插入10：\u0026quot;, a)  ","id":20,"section":"posts","summary":"几乎所有的算法书中，排序算法都是在最开始介绍的算法，不仅仅是因为排序算法非常简单，而且因为排序算法非常基础，在后续其他算法或者处理其他问题时","tags":[],"title":"数据结构复习-排序算法","uri":"https://biofrostyy.github.io/2020/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%8E%92%E5%BA%8F/","year":"2020"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等\n二.刷题沉淀 ①两个栈实现队列 # 类似于负负得正的思想，append时直接放入栈1，delete时如果栈2为空，那么把栈1移入栈2（负负得正，栈1最先放进去的最后弹出来，在栈2中最后放进去的最先弹出来），再pop栈2；如果原来栈2不为空，那么表示上次放入的还有剩（上次放入的一定比这次早），直接pop；如果移入后栈2仍为空，返回-1 class CQueue: def __init__(self): self.A,self.B = [],[] def appendTail(self, value: int) -\u0026gt; None: self.A.append(value) def deleteHead(self) -\u0026gt; int: if not self.B: if not self.A: return -1 else: while self.A: self.B.append(self.A.pop()) return self.B.pop() # Your CQueue object will be instantiated and called as such: # obj = CQueue() # obj.appendTail(value) # param_2 = obj.deleteHead()  ②队列实现栈 # 双队列实现栈，等于把一个队列B存放之前的结果，把新的元素放入A后，再把B中之前的元素放入A 执行用时：32 ms, 在所有 Python3 提交中击败了94.87%的用户 内存消耗：15 MB, 在所有 Python3 提交中击败了35.37%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A,self.B = deque(),deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) while self.B: self.A.append(self.B.popleft()) self.A,self.B = self.B,self.A self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.B.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.B[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0 # 单队列实现栈，存储原有元素数量n，把新元素放入队列中后，执行n次pop-append操作，把之前的元素重新放入队列中 执行用时：40 ms, 在所有 Python3 提交中击败了61.83%的用户 内存消耗：14.8 MB, 在所有 Python3 提交中击败了93.24%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A = deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) n = self.size while n \u0026gt; 0: self.A.append(self.A.popleft()) n -= 1 self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.A.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.A[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0  ③字符串匹配——KMP https://leetcode-cn.com/problems/implement-strstr/solution/zhe-ke-neng-shi-quan-wang-zui-xi-de-kmp-8zl57/ class Solution: def strStr(self, haystack: str, needle: str) -\u0026gt; int: a=len(needle) b=len(haystack) if a==0: return 0 next=self.getnext(a,needle) p=-1 for j in range(b): while p\u0026gt;=0 and needle[p+1]!=haystack[j]: p=next[p] if needle[p+1]==haystack[j]: p+=1 if p==a-1: return j-a+1 return -1 def getnext(self,a,needle): next=['' for i in range(a)] k=-1 next[0]=k for i in range(1,len(needle)): while (k\u0026gt;-1 and needle[k+1]!=needle[i]): k=next[k] if needle[k+1]==needle[i]: k+=1 next[i]=k return next  ④摩尔投票——选出数组中超过半数的值 class Solution: def majorityElement(self, nums: List[int]) -\u0026gt; int: # n = len(nums) # dic_nums = Counter(nums) # for key in dic_nums.keys(): # if dic_nums[key] \u0026gt; n/2: # return key # return -1 # 狼人杀归票算法 # 第一轮找到最可能出局的那个人 n = len(nums) ans = -1 count = 0 for num in nums: # 没有票数，暂时认为是当前的人 if not count: ans = num # 有相同的人上票，票数加一；否则票数减一 if num == ans: count += 1 else: count -= 1 # 第二轮确定这个人的票数确实过半 return ans if count and nums.count(ans) \u0026gt; n // 2 else -1  ","id":21,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等 二.刷题沉淀 ①两个栈实现队列 # 类似于负负得正的思想，ap","tags":[],"title":"数据结构-基础结构","uri":"https://biofrostyy.github.io/2020/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84/","year":"2020"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等$n^2$\n二.刷题沉淀 ①花费最少爬楼梯 class Solution: def minCostClimbingStairs(self, cost: List[int]) -\u0026gt; int: # cost.append(0) # def min_cost(i): # # 递归 -- 超时 # if i in (0,1): # return cost[i] # return cost[i] + min(min_cost(i-1),min_cost(i-2)) # return min_cost(len(cost)-1) # # 带记忆的递归 # cost.append(0) # cost_sum = [-1]*len(cost) # def min_cost(i): # if i in (0,1): return cost[i] # elif cost_sum[i] == -1: # cost_sum[i] = cost[i] + min(min_cost(i-1),min_cost(i-2)) # return cost_sum[i] # return min_cost(len(cost)-1) # DP cost.append(0) i, n, cost_num = cost[0],cost[1],0 for k in range(2,len(cost)): cost_num = min(i,n)+cost[k] i,n = n,cost_num return cost_num  ②队列实现栈 # 双队列实现栈，等于把一个队列B存放之前的结果，把新的元素放入A后，再把B中之前的元素放入A 执行用时：32 ms, 在所有 Python3 提交中击败了94.87%的用户 内存消耗：15 MB, 在所有 Python3 提交中击败了35.37%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A,self.B = deque(),deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) while self.B: self.A.append(self.B.popleft()) self.A,self.B = self.B,self.A self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.B.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.B[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0 # 单队列实现栈，存储原有元素数量n，把新元素放入队列中后，执行n次pop-append操作，把之前的元素重新放入队列中 执行用时：40 ms, 在所有 Python3 提交中击败了61.83%的用户 内存消耗：14.8 MB, 在所有 Python3 提交中击败了93.24%的用户 from collections import deque class MyStack: def __init__(self): \u0026quot;\u0026quot;\u0026quot; Initialize your data structure here. \u0026quot;\u0026quot;\u0026quot; self.A = deque() self.size = 0 def push(self, x: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; Push element x onto stack. \u0026quot;\u0026quot;\u0026quot; self.A.append(x) n = self.size while n \u0026gt; 0: self.A.append(self.A.popleft()) n -= 1 self.size += 1 def pop(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Removes the element on top of the stack and returns that element. \u0026quot;\u0026quot;\u0026quot; self.size -= 1 return self.A.popleft() def top(self) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot; Get the top element. \u0026quot;\u0026quot;\u0026quot; return self.A[0] def empty(self) -\u0026gt; bool: \u0026quot;\u0026quot;\u0026quot; Returns whether the stack is empty. \u0026quot;\u0026quot;\u0026quot; return self.size == 0  ","id":22,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等$n^2$ 二.刷题沉淀 ①花费最少爬楼梯 class Solution: def minCostClimbingStairs(self, cost: List[int]) -\u0026gt; int: #","tags":[],"title":"数据结构-基础结构","uri":"https://biofrostyy.github.io/2020/07/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","year":"2020"},{"content":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等\n二.刷题沉淀 ①ListNode题 # 可以使用一个空起始点来做开头，这样就不需要制作list保存了，规避了第一个的下一个的悖论  ②str # 实现str倒序，-1表示步长，即每次向前一步 str(x)[::-1]  ③差分数组 差分数组--把区间统一修改转嫁为前后临界点的修改 1109.航班预定统计  ","id":23,"section":"posts","summary":"一.问题介绍 在排序数组中查找左右边界，可用于在排序数组中查找某个值出现的位置与次数等 二.刷题沉淀 ①ListNode题 # 可以使用一个空起始点来","tags":[],"title":"数据结构-刷题tips","uri":"https://biofrostyy.github.io/2020/06/%E6%A8%A1%E5%9D%97%E6%B2%89%E6%B7%80-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%88%B7%E9%A2%98tips/","year":"2020"}],"tags":[{"title":"“网球”","uri":"https://biofrostyy.github.io/tags/%E7%BD%91%E7%90%83/"}]}